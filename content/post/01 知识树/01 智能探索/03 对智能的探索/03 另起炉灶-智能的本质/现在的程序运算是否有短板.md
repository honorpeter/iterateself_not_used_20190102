---
title: 现在的程序运算是否有短板
toc: true
date: 2018-08-18 20:29:43
---


作者：龙蜒草
链接：https://www.zhihu.com/question/51863955/answer/128173876
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

举例，像在视觉上分辨猫和狗的图片，很难得到大数据反馈，每到分辨的一次验证的时候需要结果判定，像搜寻引擎，金融投资，博弈类游戏可以马上获得结果反馈给程序作出判断，视觉上没有及时的反馈的环境所以在这方面相表现的进展慢，这也是都感觉视觉放缓的现象一部分原因。关于深度学习的这个词汇现在用的比较多，但是这个词汇有点被滥用的感觉，程序还是围绕着暴力算法加大存储量高运算，阿尔法狗就是一个明显的例子，那个根本应该不叫深度学习。假设一个视觉细胞经过10层细胞最后连接到256个不同记忆细胞，如果靠只有几百亿次计算能力的计算机无穷大时间，但是如果把cpu芯片改为几十个晶体管组成一个细胞结构体，数据结构在另外一个结构体共存在处理器内，程序可以改变他的结构，他自己根据反馈也可以改变细胞体结构跟数据的链接结构，这样一个波长时钟内可以同时运算大量的数据，像大脑一样的工作，处理器芯片可以人工编程改变结构也可以自己根据运算结果改变自己的结构。美国在2013年就提出了脑计划，主要看中的是这方面的前景。将来很可能美国在这方面先创造出这种智能芯片将会获得人工智能方面的大的发展。现阶段计算机视觉主要是因为信息的反馈无法向其它行业那样有大数据及时反馈，计算机单核线程的运算速度还是相对慢，而只是都纠结于程序员的算法上，和信息反馈上问题。在未来的5到10年内应该会有新的专门为人工智能的方面研制新的处理器会解决一大块问题，不过要搭建一个模拟视觉环境让认识万物比较难，个人电脑服务器，跑的到处都是程序，可能让一个人工智能在这样的环境里成为黑客比让他通过视频能分辨出各种水果植物动物要容易的多。感觉计算机视觉连启蒙期都算不上。




看到这篇文章，我又想起了：养数据。

比如说，是否可以有更好的方法来实现整个计算过程？比如说，直接对内存中的东西进行修改来达到目的？而不是靠计算来达到目的？现阶段的各种数学运算，一定是必须的吗？有没有另外一种形式？虽然不是数学运算，但是本质与数学运算的本事是有相同之处的？

比如，也许他并没有形成神经网络的脉冲激发形式，但是，也许有别的形式，我们现在的对神经的模拟，实际上是在模拟什么呢？



## 相关资料

- [计算机视觉是否进入了瓶颈期？](https://www.zhihu.com/question/51863955)
)
