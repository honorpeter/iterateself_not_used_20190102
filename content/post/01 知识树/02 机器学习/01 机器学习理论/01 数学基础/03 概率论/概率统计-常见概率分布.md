---
title: 概率统计-常见概率分布
toc: true
date: 2018-08-21 18:16:23
---
---
author: evo
comments: true
date: 2018-04-05 10:59:45+00:00
layout: post
link: http://106.15.37.116/2018/04/05/normal-distribution/
slug: normal-distribution
title: 概率统计-常见概率分布
wordpress_id: 3395
categories:
- 随想与反思
tags:
- '@todo'
- '@want_to_know'
---

<!-- more -->

[mathjax]


## 相关资料






  * 七月在线 机器学习


  * 《机器学习》周志华




## 需要补充的






  * **讲《机器学习》中的附录的概率分布总结进来**


  * **并且将原来的图片都替换为文字**




# MOTIVE






  * 因为总是涉及到二项分布，而且后面如果学各种似然估计的话，这个分布是必须的。





* * *






# 分布


复习各种常见分布本身的统计量，统计量是属于数学统计的。

在复习各种分布的同时，重温积分、Taylor展式等前序知识

常见分布是可以完美统一为一类分布。**怎么统一？**


# 两点分布


一个硬币投掷1次


![mark](http://images.iterate.site/blog/image/180727/FD1A7jBjAa.png?imageslim)




# 二项分布：


**这两种求法都要掌握。**

一个硬币投掷n次


![mark](http://images.iterate.site/blog/image/180727/Hkli1mD7ab.png?imageslim)

也可以用二项展开式去做：


![mark](http://images.iterate.site/blog/image/180727/HJIE70HCE1.png?imageslim)



![mark](http://images.iterate.site/blog/image/180727/Ag1iI4JA9H.png?imageslim)

**上面这两种方法都要清楚，没有仔细看**


# 泰勒展开式 与 泊松分布




![mark](http://images.iterate.site/blog/image/180727/BH18Fb77ce.png?imageslim)

某一项长的就是这个样子。

最后的一个东西就是泊松分布。可见泰勒展开式可以推出泊松分布。也可见，如果我们知道泊松分布，那么可以反推出\(e^x\)的taylor展开式是对的。


![mark](http://images.iterate.site/blog/image/180727/beL5EL4AHm.png?imageslim)

离散的情况叫做分布率。


![mark](http://images.iterate.site/blog/image/180727/Kj5eiIE4K0.png?imageslim)

可见，泊松分布的期望和方差都是λ。

那么怎么去理解泊松分布呢？


在实际事例中，当一个随机事件，以固定的平均瞬时速率λ(或称密度)随机且独立地出现时，那么这个事件在单位时间(面积或体积)内出现的次数或个数就近似地服从泊松分布P(λ)。




* 某一服务设施在一定时间内到达的人数
* 电话交换机接到呼叫的次数
* 汽车站台的候客人数
* 机器出现的故障数
* 自然灾害发生的次数
* 一块产品上的缺陷数
* 显微镜下单位分区内的细菌分布数
* 某放射性物质单位时间发射出的粒子数


刚才的二项分布里面，如果n比较大，p比较小的时候，n*p是等于\(\lambda\)的，可以近似成泊松分布。

泊松分布是离散分布的一个非常重要的东西。

**理解的不够，不知道应用的时候是什么样子的？**



上面全是离散情况，下面将连续情况。


# **均匀分布**




![mark](http://images.iterate.site/blog/image/180727/KiECF2d3CK.png?imageslim)




# 指数分布





![mark](http://images.iterate.site/blog/image/180727/7HD6K009h5.png?imageslim)

指数分布怎么去理解？事件间隔就是指数分布，这样的话，就与泊松分布的例子感觉比较像，**泊松分布与指数分布到底有什么关系？**


![mark](http://images.iterate.site/blog/image/180727/baaaaaimmF.png?imageslim)

等车是服从指数分布的，平均下来是等20分钟车就来了，但是今天我等10分钟，车没来，那么我在等20分钟等到车的概率跟之前是一样的。所以是无记忆性的。


![mark](http://images.iterate.site/blog/image/180727/A8ihJlFhdD.png?imageslim)




# **正态分布**


**很奇怪的一个分布。这个在线性回归里面会讲。正态分布最后会推出线性回归，二项分布会推出logistic回归。**

**正态分布是一个连续变化的，是一个回归问题。而二项分布是0-1变化的，因此解决的是分类问题。因此线性回归解决的是连续变化的，logistic回归解决的是分类问题。**

**这一段还是没有很理解。**


![mark](http://images.iterate.site/blog/image/180727/fK9Ij4IDBh.png?imageslim)



![mark](http://images.iterate.site/blog/image/180727/58HjcGf7EK.png?imageslim)



![mark](http://images.iterate.site/blog/image/180727/AgdF1h8kmI.png?imageslim)




## 二元正态分布


左边的是标准的，如果方差变小，就变成中间上的，方差变大，就变成3-1的

均值做一个旋转，得到下面两个


![mark](http://images.iterate.site/blog/image/180727/L2mle6097l.png?imageslim)




# 一个问题： 集合Hash问题：


某Hash函数将任一字符串非均匀映射到正整数k，概率为\(2^{-k}\)，如下所示，现在有字符串集合S，其元素经映射后，得到的最大整数为10，试估计S的元素个数。

\[P\{Hash(<string>)=k\}=2^{-k},\, k\in Z^+\]

由于Hash映射成整数是指数级衰减的，“最大整数为10”这一条件可近似考虑成“整数10曾经出现”，继续近似成“整数10出现过一次”。**应该说：整数10曾经出现过1次，整数10以上的从来没有出现过吧？嗯，应该是整数10曾经出现过1次，整数10以上有出现的可能性，但是没有出现。还不是很确定**

字符串被映射成10的概率为\(p=2^{-10}=\frac{1}{1024}\)，从而，一次映射即两点分布：**这里没明白？为什么0的概率是1023？比10大的数不是没法被映射到吗？感觉应该是1022吧？而且没明白为什么一次映射是两点分布？嗯如果是说整数10以上有出现的可能性，但是没有出现，那么感觉亮点分布是合理的，1023也是合理的。**

\[\left\{\begin{matrix}P(X=1)=\frac{1}{1024}\\P(X=0)=\frac{1023}{1024}\end{matrix}\right.\]


从而n个字符串的映射，即二项分布：


\[P\{X=k\}=C_n^kp^k(1-p)^{n-k},\; 其中p=\frac{1}{1024}

二项分布的期望为：

\[E(P\{X=k\})=np,\; 其中p=\frac{1}{1024}\]

而期望表示n次事件发生的次数，当前问题中发生了1次，从而：

\[np=1\Rightarrow n=\frac{1}{p}\Rightarrow n=1024\]




# 关于分布的总结：




![mark](http://images.iterate.site/blog/image/180727/CLekHHcla0.png?imageslim)




# 指数族分布


刚才的内容都可以归成一个指数族分布。


### \(\eta\)是自然常数，不用管。但是\(T(y)\)叫做充分统计量




![mark](http://images.iterate.site/blog/image/180727/2jJ268CFA4.png?imageslim)




## 如Bernoulli分布和高斯分布




![mark](http://images.iterate.site/blog/image/180727/Lhb4Gf42Gc.png?imageslim)




## Bernoulli 分布属于指数族


\(\Phi\)比如是投一次硬币，朝上的概率，y是投掷的次数，


![mark](http://images.iterate.site/blog/image/180727/mFL5f0B0HE.png?imageslim)




### 考察参数Φ


注意在推导过程中，出现了Logistic方程。下面这个f(x)就是Logistic方程，或者叫Sigmoid方程，也叫做S曲线。

\[\Phi =\frac{1}{1+e^{-\eta} }\]

\[f(x)=\frac{1}{1+e^{-x} }\]


这个f(x)就是我们通过二项分布和充分统计量这个概念得到的。曲线是如下的样子：




![mark](http://images.iterate.site/blog/image/180727/Hj4hm4G8jk.png?imageslim)

对它求导数，是下面这个样子：

\[\begin{align*}f'(x)&=(\frac{1}{1+e^{-x} })'\\&=\frac{e^{-x} }{(1+e^{-x})^2}\\&=\frac{1}{1+e^{-x} }\cdot \frac{e^{-x} }{1+e^{-x} }\\&=\frac{1}{1+e^{-x} }\cdot (1-\frac{1}{1+e^{-x} })\\&=f(x)\cdot (1-f(x))\end{align*}\]

这个结论在后面做Logistic回归的时候会用到，因为logistic要求梯度下降的时候要求导数。


## Gaussian分布也属于指数族分布




![mark](http://images.iterate.site/blog/image/180727/CcE07JCG73.png?imageslim)




# Gamma分布


\[p(x;\alpha,\beta)=\frac{\beta^\alpha x^{\alpha-1}e^{-\beta x} }{\Gamma (\alpha)},\; x\geq 0(常系数\alpha,\beta >0)\]

其中的Gamma函数\(\Gamma (\alpha)\)就是一个常数，就是一个规划因子：

\[\Gamma (\alpha)=\int_{0}^{\infty}t^{\alpha-1}e^{-t}dt\]

而且\(\Gamma(n)=(n-1)!\)

这个Gamma分布怎么理解呢？就把泊松分布的\(\lambda\)替换为成 \(\beta^\alpha x^{\alpha-1}\)。**未证实。要再看下。**

Gamma分布的期望为：

\[E(x)=\frac{\alpha}{\beta}\]

这个期望在后面谈主题模型的时候会遇到。当然，那里面不是Gamma分布了，不只涉及\(\alpha\)、\(\beta\)两个参数，而是涉及k个参数。那么这个时候就从Gamma分布变成Dirichlet分布。也就是LDA的D对应的。**什么是主题模型？什么是Dirichlet分布？什么是LDA？**




# 由基本分类器得到高级分类器：


**为什么这个放在这里？因为后面会谈到MCMC的一些东西，带拒绝的采样？这个后续要补充下，不然这个放在这里感觉不是很恰当。**

给定一个分类器p，它有0.5的概率输出1，0.5的概率输出0。

如何生成一个分类器使该分类器输出1的概率为0.25，输出0的概率为0.75？做两回即可。**即怎么由一个基本的分类器得到一个高级的分类器呢？**

如何生成一个分类器使该分类器输出1的概率为0.3，输出0的概率为0.7？这个到后面会谈到MCMC的东西，马尔科夫链的...的模拟。




# 一定接受率的采样


已知有个 rand7() 的函数，返回 1 到 7 随机自然数，让利用这个 rand7() 构造 rand10() 等概率返回 1~10 。

解：因为rand7仅能返回1~7的数字，少于rand10的数目。因此，多调用一次，从而得到49种组合。

伪代码如下：

```
int rand10()
{
    int a1,a2,r;
    do{
        a1=rand7()-1;
        a2=rand7()-1;
        r=a1*7+a2;
    }while (r>=40)
    return r/4+1
}
```

如果是rand7()+rand7()+...+rand7()这样7个加起来的话来代替a1*7，这样是不对的，因为任何的分布这样加起来在求平均，最后会退化为高斯分布。**为什么会退化为高斯分布？什么是高斯分布？上面讲的分布里面好像没有高斯分布？**


# COMMENT：


**很多还是不清楚**
