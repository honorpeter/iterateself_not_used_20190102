---
title: 梯度下降和拟牛顿
toc: true
date: 2018-08-01 18:30:48
---
# 梯度下降和拟牛顿

## 相关资料：






  1. 七月在线 机器学习


********************************************************************************


# 缘由：


对梯度下降进行总结

\(\nabla\) 的符号是 nabla


#




# 预备题目


已知二次函数的一个点函数值和导数值，以及另外一个点的函数值，如果确定该函数的
解析式？即：二次函数f(x)，已知\(f(a)\)，\(f’(a)\)，\(f(b)\)，求f(x)

特殊的，若a=0，题目变成：对于二次函数f(x)，已知\(f(0)\)，\(f’(0)\)，\(f(a)\)，求f(x)

实际上是可以求的，求出来是这样的：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/ICE7jag41K.png?imageslim)






# 从线性回归引起两个问题




经过回归部分的学习，基本上对线性回归有认识了。那么下面就有几个问题了：


学习率的问题：




  * 学习率α如何确定？


  * 使用固定学习率还是变化学习率


  * 学习率设置多大比较好？


下降方向的问题：


  * 处理梯度方向，其他方向是否可以？


  * 如果可以的话，可行的方向和梯度方向有何关系？




# 进行梯度下降的学习率的实验：




## 实验1：固定学习率的梯度下降


\(y=x^2\) ，初值取x=1.5，学习率使用0.01


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/h1G2i3dCe4.png?imageslim)



* 效果还不错
* 经过200次迭代，x=0.0258543；
* 经过1000次迭代，x=2.52445×10^-9




## 实验2：固定学习率的梯度下降


\(y=x^4\) ，初值取x=1.5，学习率使用0.01


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/heC63Kidga.png?imageslim)






  * 分析：效果不理想


  * 经过200次迭代，x=0.24436；


  * 经过1000次迭代，x=0.111275


为什么次数变高之后下降就不理想了呢？因为对于\(y=x^4\) 来说，降到一定程度的时候，虽然x还是很大，但是y的值已经很小了，比如x=0.1，但是y已经是0.0001 ，这个时候 y 很难往下降了。

**所以如果使用固定学习率做梯度下降的时候，有些时候很难下降了，达不到要求。**


## 固定学习率实验的代码




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6ddJHDG7a9.png?imageslim)



# 那么怎么调整学习率好呢？


分析“学习率α”在f(x)中的意义

调整学习率：




  * 在斜率(方向导数)大的地方，使用小学习率


  * 在斜率(方向导数)小的地方，使用大学习率 比如在\(y=x^4\) 中，x比较小的时候，y已经很小了，这个时候能不能用更大的学习率，使它降得更快呢？


如何构造学习率α


### 梯度下降的运行过程阐述


从点 \(x_k=a\)，沿着负梯度方向，移动到 \(x_{k+1} =b\)，这个时候有：

\[b=a-\alpha \bigtriangledown F(a)\Rightarrow f(a)\geq g(b)\]

也就是说从 \(x_0\) 为出发点，每次沿着当前函数梯度反方向移动一定距离\(\alpha k\)，可以得到序列：

\[x_0,x_1,\cdots ,x_n\]

此时对应的各点函数值序列之间的关系为：

\[f(x_0)\geq f(x_1)\geq \cdots \geq f(x_n)\]

当n达到一定值时，函数f(x)收敛到局部最小值




## 视角转换一下


记当前点为\(x_k\) ，当前搜索方向为\(d_k\) (如：负梯度方向)，因为学习率\(\alpha \)是待考察的对象，因此，将下列函数 \(f(x_k+\alpha d_k)\)看做是关于\(\alpha \)的函数\(h(\alpha)\)。**是的，可以的。**

\[h(\alpha)=f(x_k+\alpha d_k),\;\alpha >0\]

那么，当\(\alpha=0\)时，\(h(0)=f(x_k)\)。而对\(\alpha\)求偏导可以得到：\(\bigtriangledown h(\alpha)=\bigtriangledown f(x_k+\alpha d_k)^Td_k\)


## 看作是关于\(\alpha \)的函数之后，就可以求导了


因为梯度下降是寻找f(x)的最小值，那么，在\(x_k\) 和\(d_k\) 给定的前提下，即寻找函数 \(f(x_k +\alpha d_k )\)的最小值。即：

\[\alpha =arg \,\underset{\alpha >0}{min}h(\alpha)=arg\,\underset{\alpha>0}{min}f(x_k+\alpha d_k)\]

进一步，如果 \(h(\alpha)\)可导，局部最小值处的\(\alpha\)满足：

\[h'(\alpha)=\bigtriangledown f(x_k+\alpha d_k)^Td_k=0\]

注意这个\(\alpha\)的函数并不一定是凸函数，我们也不需要找到一个最好的\(\alpha\)，只需要找到一个还不错的\(\alpha\)就行 。


## 分析这个学习率函数的导数




将 \(\alpha =0\) 带入：**还有些没明白**


\[h'(0)=\bigtriangledown f(x_k+0*d_k)^Td_k=\bigtriangledown f(x_k)^Td_k\]

下降方向\(d_k\)可以选负梯度方向 \(d_k=-\bigtriangledown f(x_k)\)，或者选中与负梯度夹角小于90°的某个方向（谋面会继续阐述搜索方向问题）。因此：\(h'(0)<0\)。而我们是想找到一个等于0的。

因此，如果能够找到足够大的 \(\alpha\) 使得 \(h'(\hat{\alpha})>0\) 。则必存在某个 \(\alpha^*\) ，使得\(h'(\alpha^*)=0\)。而这个 \(\alpha^*\) 即为要寻找的学习率。

那么怎么找这个使\(h'(\alpha^*)=0\)的点呢？


## 使用线性搜索的方法 Line Search




### 二分线性搜索(Bisection Line Search)


这个使最简单的处理方法

不断将区间 \([\alpha_1,\alpha_2\]) 分成两半，选择端点异号的一侧，直到区间足够小或者找到当前最优学习率。


### 回溯线性搜索(Backing Line Search)


基于Armijo准则计算搜素方向上的最大步长，其基本思想是沿着搜索方向移动一个较大的步长估计值，然后以迭代形式不断缩减步长，直到该步长使得函数值 \(f(x_k+\alpha d_k)\) 相对与当前函数值\(f(x_k )\)的减小程度大于预设的期望值(即满足Armijo准则)为止。

\[f(x_k+\alpha d_k)\leq f(x_k)+c_1\alpha \bigtriangledown f(x_k)^Td_k\; c_1\in (0,1)\]


### 回溯与二分线性搜索的异同


二分线性搜索的目标是求得满足 \(h'(\alpha)\approx 0\) 的最优步长近似值，而回溯线性搜索放松了对步长的约束，只要步长能使函数值有足够大的变化即可。

二分线性搜索可以减少下降次数，但在计算最优步长上花费了不少代价。而回溯线性搜索找到一个差不多的步长即可。




# 使用回溯线性搜索




## 回溯线性搜索代码






  * x为当前值


  * d为x处的导数


  * a为输入学习率


  * 返回调整后的学习率




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/19afcdC8lg.png?imageslim)




## 实验：回溯线性搜索寻找学习率


y=x^4 ，初值取x=1.5，回溯线性方法


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1cmHh5CfAm.png?imageslim)

效果还不错




  * 经过12次迭代，x=0.00010872；


  * 经过100次迭代，x=3.64905×10^-7


试比较固定学习率时:


  * 经过200次迭代，x=0.24436；


  * 经过1000次迭代，x=0.111275




## 回溯线性搜索的思考：插值法


采用多项式插值法(Interpolation) 拟合简单函数，然后根据该简单函数估计函数的极值点，这样选择合适步长的效率会高很多。

现在拥有的数据为：\(x_k\) 处的函数值\(f(x_k )\)及其导数\(f’(x_k )\) ，再加上第一次尝试的步长\(\alpha_0\) 。如果\(\alpha_0\)满足条件，显然算法退出；若\(\alpha_0\)不满足条件，则根据上述信息可以构造一个二次近似函数：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1DcL5e7dee.png?imageslim)

### 二次插值法求极值


显然，导数为0的最优值为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/54f62H6KJF.png?imageslim)

若 \(\alpha_1\) 满足Armijo准则，则输出该学习率；否则，继续迭代。


# 二次插值法




## 代码如下：






  * x为当前值


  * d为x处的导数


  * a为输入学习率


  * 返回调整后的学习率




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/f5C14CGBHi.png?imageslim)




## 实验：二次插值线性搜索寻找学习率


y=x^4 ，初值取x=1.5，二次插值线性搜索方法


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/50kJ9c1El7.png?imageslim)

效果还不错




  *  经过12次迭代，x=0.0000282229 ；


  *  经过100次迭代，x=3.61217×10^-7


试比较回溯线性搜索时:


  *  经过12次迭代，x=0.00010872 ；


  *  经过1000次迭代，x=3.649×10^-7




## 总结与思考


通过使用线性搜索的方式，能够比较好的解决学习率问题，一般的说，回溯线性搜索和二次插值线性搜索能够基本满足实践中的需要。

那么：




  * 可否在搜索过程中，随着信息的增多，使用三次或者更高次的函数曲线，从而得到更快的学习率收敛速度？


  * 为避免高次产生的震荡，可否使用三次 Hermite 多项式，在端点保证函数值和一阶导都相等，从而构造更光顺的简单低次函数？















# 搜索方向


若搜索方向不是严格梯度方向，是否可以？

思考：因为函数 二阶导数反应了函数的凸凹性；二阶导越大，一阶导的变化越大。在搜索中，可否用二阶导做些“修正”？如：二者相除？


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/CdAKccJ2eB.png?imageslim)




### 附：平方根算法


在任意点x 0 处Taylor展开


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Leb66CIieE.png?imageslim)

一般若干次(5、6次)迭代即可获得比较好的近似值


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/LK7mKG4AE3.png?imageslim)




### 实验：搜索方向的探索


y=x^4 ，初值取x=1.5，负梯度除以二阶导


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/58cEjcFb96.png?imageslim)

效果出奇的好！




  *  经过12次迭代，x=0.00770735；


  *  经过100次迭代，x=3.68948×10 -18


试比较二次插值线性搜索时:


  *  经过12次迭代，x=0.0000282229；


  *  经过1000次迭代，x=3.61217×10 -7




### 分析上述结果的原因




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8idA3dl01l.png?imageslim)




### 牛顿法


上述迭代公式，即牛顿法

该方法可以直接推广到多维：用方向导数代替一阶导，用Hessian矩阵代替二阶导


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/2F4h8aIa22.png?imageslim)

### 牛顿法的特点






  * 经典牛顿法虽然具有二次收敛性，但是要求初始点需要尽量靠近极小点，否则有可能不收敛。


  *  计算过程中需要计算目标函数的二阶偏导数，难度较大。


  *  目标函数的Hessian矩阵无法保持正定，会导致算法产生的方向不能保证是f在x_k 处的下降方向，从而令牛顿法失效；


  *  如果Hessian矩阵奇异，牛顿方向可能根本是不存在的。


二阶导非正定的情况(一维则为负数)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/a9FJLBfHE2.png?imageslim)

修正牛顿方向


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/G7ijm0g8I1.png?imageslim)




### 拟牛顿的思路


求Hessian矩阵的逆影响算法效率，同时，搜索方向只要和负梯度的夹角小于90°即可，因此，可以用近似矩阵代替Hessian矩阵，只要满足该矩阵正定、容易求逆，或者可以通过若干步递推公式计算得到。




  * BFGS / LBFGS


  * Broyden – Fletcher – Goldfarb - Shanno




### BFGS




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1JF1fd0KmE.png?imageslim)

### L-BFGS




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/80H8kDJifl.png?imageslim)


# COMMENT：
