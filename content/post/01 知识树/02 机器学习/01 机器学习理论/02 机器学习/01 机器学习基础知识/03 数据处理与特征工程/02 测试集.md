---
title: 02 测试集
toc: true
date: 2018-07-30 15:11:50
---
# 测试集

## 需要补充的

1. <span style="color:red;">这几种采样的方法都要自己用 Python 和C++进行实现，同时在 sklearn、tensorflow上进行实现，要找个简单的例子。要补充到这里或者新开一个总结的地方</span>
2. <span style="color:red;">这个关于调参的还是要单独拿出来吧？</span>
3. <span style="color:red;">关于验证集 还是要仔细看看，这个只是在最后一段提了一下，实际上还是有很多问题的。验证集一般什么时候使用？一般什么情况下使用？</span>


OK，我们从降低泛化误差出发，看看有什么可以做的



## 测试集的划分是有必要的

我们知道，泛化误差是无法获得的，但是，我们可以用一种方法来计算出一个近似的泛化误差：我们可以建立一个 “测试集” (testing set) ，然后用这个测试集来测试模型对新样本的判别能力，然后，我们就可以把测试集上得出的 “测试误差” (testing error) 作为我们这个模型的泛化误差的近似。**嗯，是可以的。**

注意：测试集中的测试样本，我们假设它是从样本的真实分布中独立同分布采样得到的。而且，我们在建立测试集的时候要尽可能与训练集互斥，也就是说测试样本尽量不被用于训练。<span style="color:red;">怎么能保证他是独立同分布采样得到的？</span>

嗯，关于测试样本为什么要尽量不出现在训练集中这一点，我们这样考虑，比如说：老师出了 10 道习题供同学们练习，考试时老师又用同样的这 10 道题作为试题，那么这个考试成绩肯定不能够反应出学生的学习情况的。而我们希望得到泛化性能强的模型，就好比是希望学生对课程学得尽可能的好，这个训练的样本就是相当于给学生练习的习题，测试的样本就相当于考试的题目，很明显，如果测试样本被用作训练了，那么则得到的将是过于 “乐观” 的估计结果。<span style="color:red;">是的。</span>

OK，但是，我们现在只有一个包含 m 个样例的数据集 $D=\{(x_1,y_1),(x_2,y_2),\cdots (x_m,y_m)\}$ ，现在既要训练，又要测试，怎么做呢？

OK，我们可以对 $D$ 进行适当的处理，从中产生出训练集 $S$ 和测试集 $T$ 。

## 几种常见的训练集/测试集划分方法

下面我们介绍几种常见的做法：

### 留出法

#### 留出法介绍

<span style="color:red;">从这一段，感觉是不是作者梳理的不是很透彻？因为分层采样实际上还是有几种同等的东西的，没讲，而且分层采样讲的并不丝丝入扣，与留出法本身割离了。而且，这一段的举例子的层级也不对吧，怎么一层套一层，里面还讲概念？</span>

<span style="color:red;">确认下留出法这个名字是不是通用的？</span>

最普通的方法就是“留出法” (hold-mit) ：

直接将数据集 $D$ 划分为两个互斥的集合，其中一个集合作为训练集 $S$，另一个作为测试集 $T$ ，即 $D=S\cup T$ ，$S\cap T=\Phi$ 。在 $S$ 上训练出模型后，用 $T$ 来评估其测试误差，作为对泛化误差的估计。

#### 留出法划分的时候使用分层采样

OK，这种留出法实际上是最常用的，但是，这中间有个问题，我们在划分数据集的时候，是需要尽可能的保持数据分布的一致性的，不然的话会引入额外的偏差，这个偏差会对最终结果产生影响。<span style="color:red;">什么额外的偏差？若 $S$、$T$中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。什么偏差？需要确认下。</span>

那么怎么在划分的时候保持数据分布的一致性呢？

首先，什么是保持数据分布的一致性？其实就是在分类任务中至少要保持样本的类别比例相似。<span style="color:red;">这里只提了分类任务，如果是回归任务，怎么分层采样？如果是聚类任务呢？</span>

我们常用的划分方式就是分层采样 (stratified sampling)。

OK，这就是我们使用分层采样的原因，因为它正是一种保留类别比例的采样方式。

比如说：我们想对 $D$ 进行 7/3 分 ，若 $D$ 包含 500 个正例、500 个反例，那么分层采样得到的 $S$ 应包含 350 个正例、350 个反例，而 $T$ 则包含 150 个正例和 150 个反例。

嗯，看起来是 OK 了。

#### 单次使用留出法得到的结果还不够可靠

但是，实际上，即便在给定 7/3 分的比例之后，我们仍然是有很多种划分方式来对 $D$ 进行分割的。

比如上面的例子中，可以把 $D$ 中的样本排序，然后把前 350 个正例放到 $S$ 中，也可以把后 350 个正例放到 $S$ 中，等等，这些不同的划分还是会将导致不同的训练集/测试集，相应的，模型评估的结果也还是会有差别。<span style="color:red;">是的。</span>

可见，单次使用留出法得到的估计结果往往还是不够稳定可靠的。<span style="color:red;">是的。</span>

因此，我们在使用留出法时，一般要采用若干次随机划分、然后重复进行实验评估后取平均值作为留出法的评估结果。<span style="color:red;">嗯，这就是留出法一定要多次验证取平均值的原因。</span>

例如我们可以进行 100 次随机划分，每次产生一个训练集/测试集用于实验评估，那么 100 次后就得到 100 个结果，而留出法返回的则是这 100 个结果的平均。（同时可得估计结果的标准差，<span style="color:red;">这个标准差有用吗？能衡量什么吗？</span>）

OK，到这里，应该没有什么问题了吧？实际上还是有一个问题的：

#### 留出法这样的划分存在的一个问题

在开始的时候，我们希望评估的是用 $D$ 训练出的模型的性能，但是现在留出法需要划分训练集/测试集，这就会导致一个问题：<span style="color:red;">我们有说要评估的是 $D$ 训练处的模型的性能吗？</span>

* 如果我令 $S$ 包含绝大多数样本，那么训练出的模型可以更接近用全部的 $D$ 训练出的模型，但是，这时候，由于 $T$ 比较小，因此评估结果可能不够稳定准确。

* 而如果我们令 $T$ 多包含一些样本，那么 $S$ 与 $D$ 的差别就更大了，也就是说，根据 $S$ 生成的模型与用 $D$ 训练出的模型是有可能有较大差别的，这就而降低了评估结果的保真性 (fidelity)。<span style="color:red;">是的，但是什么是保真性？</span>

事实上这个问题现在还没有完美的解决方案，常见做法是将大约  `2/3 ~ 4/5` 的样本用于训练，剩余样本用于测试。一般而言，测试集至少应含 `30` 个样例 。

OK，我们对留出法有了基本的认识了，感觉上，的确应该这么做。

下面我们看看别的一些方法。


### 交叉验证法

#### 交叉验证法介绍

交叉验证法 (cross validation) ：

先将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集，即 $D=D_1\cup D_2\cup \cdots \cup D_k$，$D_i\cap D_j=\Phi\; (i\neq j)$。每个子集 $D_i$ 都尽可能保持数据分布的一致性，即从 $D$ 中通过分层采样得到这些子集。<span style="color:red;">这个有这个分层采样的要求吗？之前我看的代码里面好像没有特别注意这个。确认下</span>

然后，每次用 $k-1$ 个子集的并集作为训练集，余下的那个子集作为测试集。这样就可获得 $k$  组训练/测试集，从而可进行 $k$ 次训练和测试，最终返回的是这 $k$ 个测试结果 的均值。

显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 $k$ 的取值，为强调这一点，通常把交叉验证法称为 $k$ 折交叉验证 (k fold cross validation)， $k$ 最常用的取值是 $10$ ，此时称为 $10$ 折交叉验证。其他常用的 $k$ 值有 $5$、$20$ 等。<span style="color:red;">为什么稳定性和保真性很大程度取决于 $k$ ？如果 $k$ 太大或太小会怎么样？太大应该没问题，因为最多就是变成留一法。</span>

下图是 $10$ 折交叉验证的示意图：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/hEB55JD99C.png?imageslim)

#### 多次 $k$ 折交叉验证取平均


与留出法相似，将数据集 $D$ 划分为 $k$ 个子集同样存在多种划分方式。为了减小因样本划分不同而引入的差别，$k$ 折交叉验证通常要随机使用不同的划分重复 $p$ 次，最终的评估结果是这 $p$ 次 k 折交叉验证结果的均值，例如常见的有：10 次 10 折交叉验证。<span style="color:red;">是这样吗？之前没有注意过。以前的sklearn 的 kaggle 的代码里面有写要多次进行 $k$ 折交叉验证码？确认下。</span>


#### 特殊的 $k=1$ 折交叉验证：留一法

假定数据集 $D$ 中包含 $m$ 个样本，如果令 $k = m$ ，我们就得到了交叉验证法的一个特例：留一法(Leave-One-Out，简称LOO)。

显然，留一法是不受随机样本划分方式的影响的，因为 $m$ 个样本只有唯一的方式划分为 $m$ 个子集，每个子集包含一个样本。<span style="color:red;">是的。</span>

留一法使用的 $S$ 与 $D$ 相比只少了一个样本，这就使得在绝大多数情况下，留一法的 $S$ 得到的模型与期望评估的用 $D$ 训练出的模型很相似。<span style="color:red;">嗯，是的，是个好方法。</span>

因此，留一法的评估结果往往被认为比较准确。

然而，留一法也有其缺陷：

在数据集比较大时，训练 $m$ 个模型的计算开销可能是难以忍受的 (例如数据集包含 1 百万个样本，则需训练 1 百万个模型)，而且这还是在未考虑算法调参的情况下的。<span style="color:red;">是的，如果考虑 grid_search 那会更多。</span>另外，留一法的估计结果也未必永远比其他评估方法准确。没有免费的午餐定理对实验评估方法同样适用。<span style="color:red;">为什么？ 那么到底用不用留一法呢？什么情况下使用呢？还是说更倾向于使用 10 折交叉法？ 为什么说未必永远比其他评估方法更准确？</span>


### 有放回采样法 bootstrapping

#### 有放回采样法的概念

bootstrap 为什么要翻译成有放回采样法？是因为像自助餐一样吗？

我们希望评估的是用 $D$ 训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比 $D$ 小，这必然会引入一些因训练样本规模不同而导致的估计偏差。<span style="color:red;">是的。</span>留一法虽然受训练样本规模变化的影响较小，但是计算复杂度又太高了。（关于样本复杂度与泛化性能之间的关系，参见第 12 章 <span style="color:red;">要把第12章的样本复杂度与泛化性能之间的关系的内容放到这里说明一下。</span>）

那么，有没有什么办法既可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？ 

<span style="color:red;">一直想知道这种 bootstrapping 采样与 boostrap 有没有事什么关系。</span>

自助法 (bootstrapping) 就是一个比较好的解决方案，它直接以自助采样法 (bootstrap sampling) 为基础：

* 给定包含 $m$ 个样本的数据集 $D$ ，我们对它进行采样产生数据集 $D'$ ，每次随机从 $D$ 中挑选一个 样本，将其拷贝后放入 $D'$ ,然后再将该样本放回初始数据集 $D$ 中，使得该样本在下次采样时仍有可能被采到。

* 这个过程重复执行 $m$ 次后，我们就得到了包含 $m$ 个样本的数据集 $D'$ ，这就是自助采样的结果。<span style="color:red;">是的，还是比较好理解的。</span>


#### 有放回采样法生成的训练集和测试集

显然， $D$ 中有一部分样本会在 $D'$ 中多次出现，而另一部分样本不会出现在 $D'$ 中。

实际上，我们是可以算出一个样本在这 $m$ 次采样中从来没有被采到的概率的： $(1-\frac{1}{m})^m$ ，取极限得到：

$$\underset{m\mapsto \infty}{lim}(1-\frac{1}{m})^m\mapsto \frac{1}{e}\approx 0.368$$

也就是说，通过自助采样，初始数据集 $D$ 中约有 $36.8%$ 的样本未出现在采样数据集 $D'$ 中。OK，那么我们就可将 $D'$ 用作训练集， $D-D'$ 用作测试集。

这样的话，实际评估的模型与期望评估的模型都使用 $m$ 个训练样本，而我们仍有数据总量约 $1/3$ 的、没在训练集中出现的样本可以用于测试。

<span style="color:red;">感觉这样的采样法真的挺厉害的，但是总是感觉怪怪的，为什么这样的方法是有道理的？</span>

这样的测试结果，亦称包外估计  (out-of-bag estimate)。<span style="color:red;">厉害呀，不过包外估计这个词还是比较陌生的。</span>

#### 有放回采样法的使用场景

那么这么好的方法，我们在什么情况下使用呢？

自助法在数据集较小、难以有效划分训练集/测试集的时候还是很有用的。<span style="color:red;">那么大的数据集的时候不推荐使用吗？而且小的数据集的时候这种自助法不是会产生一些重复的数据吗？不会对模型有影响吗？</span>而且，自助法由于能够从初始数据集中产生多个不同的训练集，这就对集成学习等方法有很大的好处。<span style="color:red;">感觉很厉害的，不过还是要确认下 ensemble 与这种自助法的详细关系。</span>

然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差，因此，在初始数据量足够多的时候，还是留出法和交叉验证法更常用一些。<span style="color:red;">估计偏差要怎么消除？难道小数据量的时候就可以不考虑估计偏差吗？</span>

<span style="color:red;">嗯，这些方法的选取在实践中再确认下，然后将经验补充到这里。</span>





## 最终交给用户模型的时候需要注意的

给定包含 $m$ 个样本的数据集 $D$ ，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集 $D$ 重新训练模型，这个模型在训练过程中使用了所有 $m$ 个样本，这才是我们最终提交给用户的模型。 <span style="color:red;">真的是这样吗？ 以前我看到这个地方的时候就有疑问。之前看到的一些kaggle 的例子好像都没有这么做。哦，不对，应该是已经这样做了。因为有些 kaggle 的放出来的方法文档里面都是直接用的找到的超参数，而且也的确是使用的全部的数据进行直接训练。</span>




## 需要消化的


自助采样法在机器学习中有重要用途，[Efron and Tibshirani，1993]对此进行了详细的讨论。<span style="color:red;">什么重要用途？要把论文总结到这里。</span>

[Dietterich, 1998]指出了常规 k 折交叉验证法存在的风险，并提出了 5x2 交叉验证法。[Demsar, 2006]讨论了对多个算法进行比较检验的方法。<span style="color:red;">常规的k 折交叉验证也会存在风险吗？怎么对多个算法进行比较检验？都总结到这里。</span>



## 相关资料

* 《机器学习》
