---
title: 04 性能度量
toc: true
date: 2018-07-30 16:04:42
---
# 性能度量

## 需要补充的

* <span style="color:red;">一直想知道这些指标对于多分类问题怎么办？而且，对于两列目标的要怎么进行衡量？</span>
* <span style="color:red;">聚类的性能度量参见第 9章  这里每写，看来这种性能度量还是要拆开，根据是对应的分类问题还是聚类问题分成两张。</span>
* <span style="color:red;">对于 P-R 也没有明白</span>
* 引用文档中的 ROC 要补充进来，因为对于 ROC 完全没明白。




当模型从训练集上训练出来之后，我们就要在测试集上衡量一下这个模型的泛化能力了，这就是性能度量 (performance measure) 。<span style="color:red;">这个性能度量这个词感觉用的不恰当，直接说泛华能力不行吗？</span>


## 对模型进行评价实际上取决于任务需求

不同的任务需求对应的评判指标也不同。而，对于不同的模型来说，使用不同的评判指标往往会得到不同的评判结果。这就意味着，模型的好坏是相对的，什么样的模型是好的，不仅取决于我们的算法和数据，还取决于任务需求。<span style="color:red;">是的，比如有的模型AUC会好一些，F1就差一些，因此到底要按照神马来比较是很重要的。</span>

## 不管需求是什么，肯定要涉及到 $f(x)$ 与 $y_i$ 的比较

在预测任务中，给定样本集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}$ ，其中 $y_i$ 是示例 $x_i$ 的真实标记。要评估学习器的性能，我们就肯定要把学习器预测结果 $f(x)$ 与真实标记 $y_i$ 进行比较。<span style="color:red;">那么非预测任务中呢？有没有非预测任务？</span>


## 回归任务中常用的性能度量

回归任务中最常用的性能度量就是 “均方误差”(mean squared error)：<span style="color:red;">是的，很容易想象一个曲线拟合的任务把均方误差当做性能度量的指标。</span>

![mark](http://images.iterate.site/blog/image/180713/iKK1DcEHiE.png?imageslim)

更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot)$ ，均方误差可描述为

![mark](http://images.iterate.site/blog/image/180713/mGk94J98bi.png?imageslim)

<span style="color:red;">这个式子放在这里是想说明什么，是想说明把一个回归任务等价到一个分类任务中的时候的对应的均方误差吗？实际上的分类任务中会使用这个均方误差吗？</span>



## 分类任务中常用的性能度量

下面，我们主要介绍分类任务中常用的性能度量。

### 错误率与精度


错误率和精度，是分类任务中最常用的两种性能度量，既适用于二分类任务，也适用于多分类任务。

错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例。

对样例集 $D$。

错误率定义为：<span style="color:red;">$D$ 不是拆开成 $S$ 和 $T$ 了吗？为什么还要放在 $D$ 中看？ 在书上确认下是不是打错了</span>

![mark](http://images.iterate.site/blog/image/180713/2bl6dE8i0m.png?imageslim)

精度定义为：

![mark](http://images.iterate.site/blog/image/180713/GKBg511gcD.png?imageslim)

更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot )$ ，错误率与精度可分别描述为：<span style="color:red;">这个概率密度函数是从哪里来的？为什么这个是更一般的？</span>

![mark](http://images.iterate.site/blog/image/180713/3j12f76Af6.png?imageslim)

![mark](http://images.iterate.site/blog/image/180713/Amil8mK6Ie.png?imageslim)

### 准确率、召回率与 F1

#### 为什么需要准确率和召回率？

错误率和精度虽然常用，但是并不能满足所有任务需求。

比如：

假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率只能衡量了有多少比例的瓜被判别错误，但是如果我们关心的是 “挑出的西瓜中有多少比例是好瓜” 或者 “所有好瓜中有多少比例被挑了出来”，那么错误率就不够用了，这时我们就需要使用其他的性能度量。<span style="color:red;">是的。</span>

类似的需求在信息检索、Web搜索等应用中其实经常出现，比如：在信息检索 中，我们经常会关心 “检索出的信息中有多少比例是用户感兴趣的” ，“用户感兴趣的信息中有多少被检索出来了”。这些是错误率表达不出来的。<span style="color:red;">是的。</span>

OK，因此，我们就有了：查准率 ( 准确率 ，precision ) 与 查全率 ( 召回率，recall) 。<span style="color:red;">注意：这里是准确率与上面介绍的精度是区别的。</span>

#### 以二分类问题为例

我们以二分类问题为例：

我们可以将样例根据其真实类别与学习器预测类别的组合划分为：

* 真正例 (true positive)   个数：TP
* 假正例 (false positive)  个数：FP
* 真反例 (true negative)   个数：TN
* 假反例 (false negative)  个数：FN

显然：TP+FP+TN+FN =样例总数 。

分类结果的 “混淆矩阵” (confusion matrix) 如下表所示：<span style="color:red;">为什么叫混淆矩阵呢？</span>


![mark](http://images.iterate.site/blog/image/180713/Jgd259IDfG.png?imageslim)

OK，那么查准率 P 与查全率 R 的定义为：

![mark](http://images.iterate.site/blog/image/180713/mk3iB8dd5K.png?imageslim)

#### 准确率与召回率的关系

查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

例如：

* 如果我们希望将好瓜尽可能多地选出来， 那么可以通过增加选瓜的数量来实现，如果将所有西瓜都选上，那么所有的好瓜也必然都被选上了，但是这样的查准率就会较低。

* 如果我们希望选出的瓜中好瓜比例尽可能的高，那么我们就可以只挑选最有把握的瓜，但是这样就难免会漏掉不少好瓜，使得查全率较低。

通常只有在一些简单任务中，才可能使得查全率和查准率都很高。<span style="color:red;">嗯，好吧 看来还是有可能都高的。</span>

<span style="color:red;">那么这种矛盾的情况怎么解决呢？</span>

#### P-R 曲线

<span style="color:red;">关于 P-R 曲线基本没明白，一定要弄清楚</span>

在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为 “最可能” 是正例的样本，排在最后的则是学习器认为 “最不可能” 是正例的样本。

那么按照这个顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率。我们以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称 “P-R曲线” ，显示该曲线的图称为 “P-R图” 。<span style="color:red;">没明白，什么叫逐个把样本作为正例进行预测？还是有点不清楚，这个图到底是怎么画出来的？对于一个固定的模型来说，准去率和召回率不应该是一个大概比较固定的数吗？怎么会是一条线呢？</span>

P-R 曲线与平衡点示意图：

![mark](http://images.iterate.site/blog/image/180713/JjBF4HAl99.png?imageslim)

<span style="color:red;">没明白，查准率为什么会到 0 ？ TP/(TP+FP) 不应该到 0 的吧？</span>

为绘图方便和美观，示意图显示出单调平滑曲线，但现实任务中的 P-R 曲线常是非单调、不平滑的， 在很多局部有上下波动。<span style="color:red;">P-R 曲线是怎么画出来的？为什么会有波动？</span>

P-R 图直观的显示出学习器在样本总体上的查全率、查准率。在进行比较时：

* 如果一个学习器的 P-R 曲线被另一个学习器的曲线完全 “包住” ，则可断言后者的性能优于前者，例如上图中学习器 A 的性能优于学习器 C 。<span style="color:red;">为什么呢？</span>
* 如果两个学习器的P-R曲线发生了交叉，例如上图中的 A 与 B，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较。


然而，很多情形下，人们往往仍希望把学习器 A 与 B 比出个高低，这时一个比较合理的判据是比较 P-R 曲线下面积的大小，它在一定程度上表征了学习器在查准率和查全率上取得相对 “双高” 的比例。但这个面积值不太容易估算，那么怎么办呢？<span style="color:red;">为什么比较曲线下的面积大小是合理的？</span>

我们可以看下 “平衡点” (Break-Event Point，简称BEP )，平衡点是 “查准率=查全率” 时的取值，例如上图中学习器 C 的 BEP 是0.64，基于 BEP 的比较，我们可以认为学习器 A 优于 B。<span style="color:red;">为什么平衡点可以用来衡量？</span>

### F1

但是 BEP 还是过于简单了些，我们更常用的是 F1 度量。F1 是基于查准率与查全率的调和平均。<span style="color:red;">为什么使用调和平均？书上说，调和平均与算数平均 $\frac{P+R}{2}$ 和集合平均 $\sqrt{P\times R}$ 相比，更重视较小值，为什么要重视较小值？</span>

<span style="color:red;">有个问题之前就想知道：F1 与 BEP 或者 P-R 的面积会有冲突吗？冲突的时候以哪个为准？</span>


![mark](http://images.iterate.site/blog/image/180713/3HmkF7Kg5c.png?imageslim)

![mark](http://images.iterate.site/blog/image/180713/788maFD8De.png?imageslim)

### $F_\beta$

在一些应用中，对查准率和查全率的重视程度是有所不同的，比如：

* 在商品推荐系统中，为了尽可能的少打扰用户，我们更希望推荐内容是用户感兴趣的，这个时候查准率就更重要。

* 而在逃犯信息检索系统中，我们更希望尽可能的少漏掉逃犯，因此这个时候查全率更加重要。


F1 度量的一般形式 $F_\beta$ ，能够让我们表达出对查准率/查全率的不同偏好，它是基于查准率与查全率的加权调和平均：<span style="color:red;">之前从来不知道还有  $F_\beta$ ，有可能看到过，但是忘记了。</span>

![mark](http://images.iterate.site/blog/image/180713/Hlm6mgLEgd.png?imageslim)

![mark](http://images.iterate.site/blog/image/180713/hJg49behK3.png?imageslim)

其中 $\beta >0$ 度量了查全率对查准率的相对重要性：

* $\beta =1$ 时退化为标准的 F1
* $\beta >1$ 时查全率有更大影响
* $\beta <1$ 时查准率有更大影响

<span style="color:red;">kaggle 比赛中有使用这个作为衡量指标的吗？一般什么时候用？sklearn 里面怎么使用？</span>

### 多个二分类混淆矩阵时候的比较方法

很多时候我们有多个二分类混淆矩阵，比如说：

* 进行了多次训练/测试，每次得到一个混淆矩阵
* 或者是在多个数据集上进行了训练/测试，我们希望能够估计算法的 “全局” 性能；
* 或者是执行多分类任务，每两两类别的组合都对应一个混淆矩阵。<span style="color:red;">这也可以？</span>
* ……

<span style="color:red;">上面这几种情况一定要补充对应的例子。之前有遇到过类似的比赛吗？</span>

总之，我们希望在 $n$ 个二分类混淆矩阵上综合考察查准率和查全率。怎么办呢？

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率， 记为 $(P_1,R_1),(P_2,R_2),\cdots ,(P_n,R_n)$ ，再计算平均值，这样就得到了 “宏查准率”（macro-P）、“宏查全率”（macro-R）,以及相应的 “宏F1” （macro-F1）:


![mark](http://images.iterate.site/blog/image/180713/bjEb70Lfg2.png?imageslim)

我们还可以将各混淆矩阵的对应元素进行平均，得到 TP、FP、TN、FN 的平均值，分别记为 $\overline{TP}$ 、$\overline{FP}$ 、$\overline{TN}$ 、$\overline{FN}$ ，然后，我们再基于这些平均值计算出 “微查准率”（micro-P）、“微查全率”（micro-R）和 “微F1” （micro-F1）:

![mark](http://images.iterate.site/blog/image/180713/CebkfAgjgl.png?imageslim)

<span style="color:red;">到底用哪个？到底怎么评价？需要补充一下对应的案例</span>


### ROC 与 AUC

<span style="color:red;">还是没明白，结合别的资料仔细看下，要弄明白</span>

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值 (threshold) 进行比较，如果大于阈值则分为正类，否则为反类。

例如：神经网络在一般情形下是对每个测试样本预测出一个 `[0.0,1.0]` 之间的实值， 然后将这个值与 `0.5` 进行比较，大于 `0.5` 则判为正例，否则为反例。这个实值或概率预测结果的好坏，直接决定了学习器的泛化能力。

实际上，根据这个实值或概率预测结果，我们可以将测试样本进行排序，“最可能” 是正例的排在最前面， “最不可能” 是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个 “截断点” (cut point) 将样本分为两部分，前一部分判作正例，后一部分则判作反例。<span style="color:red;">是的。</span> 在不同的应用任务中，我们可根据任务需求来采用不同的截断点，例如若我们更重视 “查准率”，则可选择排序中靠前的位置进行截断；若更重视 “查全率”，则可选择靠后的位置进行截断。<span style="color:red;">有些厉害。没明白，这个截断点不是应该我们自己固定的吗？</span>

因此，排序本身的质量好坏，体现了综合考虑学习器在不同任务下的 “期望泛化性能” 的好坏，或者说，“一般情况下” 泛化性能的好坏。<span style="color:red;">嗯，但是这个排序本身的质量好坏只与模型本身有关吧？</span>

`ROC` 曲线则是从这个角度出发来研究学习器泛化性能 的有力工具。 ROC 全称是 “受试者工作特征” (Receiver Operating Characteristic) 曲线，它源于二战中用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，此后被引入机器学习领域。

`P-R` 曲线相似，我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、级坐标作图，就得到了 “ROC曲线”。<span style="color:red;">什么叫逐个把样本作为正例进行预测？两个重要量的值是什么？</span>

与`P-R`曲线使用查准率、查全率为纵、横轴不同，ROC 曲线的纵轴是 “真正例率” (True Positive Rate，简称 TPR)，横轴是 “假正例率” (False Positive Rate，简称FPR)，两者定义如下：

![mark](http://images.iterate.site/blog/image/180713/HiHAha3L2d.png?imageslim)

`ROC` 曲线的图称为 “ROC图” ，下图为 `ROC` 曲线与 `AUC` 示意图：


![mark](http://images.iterate.site/blog/image/180713/bej8fCgEHf.png?imageslim)

显然， 对角线对应于 “随机猜测” 模型，而点 `(0, 1)` 则对应于将所有正例排在所有反例之前的 “理想模型”。<span style="color:red;">没看懂。而且，一直没明白，为什么这个ROC是一个曲线？这个曲线怎么得到的？</span>

现实任务中通常是利用有限个测试样例来绘制 ROC 图，此时仅能获得有限个(真正例率，假正例率)坐标对，无法产生上图 (a) 中的光滑 ROC 曲线，只能 绘制出如上图 (b) 中所示的近似 ROC 曲线。绘图过程很简单：给定 $m^+$ 个正例和 $m^-$ 个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大， 即把所有样例均预测为反例，此时真正例率和假正例率均为 `0`，在坐标 `(0,0)` 处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为 $(x,y)$，当前若为真正例，则对应标记点的坐标为 $(x,y+\frac{1}{m^+})$ ，当前若为假正例，则对应标记点的坐标为 $(x+\frac{1}{m^-},y)$  ，然后用线段连接相邻点即得。

进行学习器的比较时，与 P-R 图相似，：

* 若一个学习器的 ROC 曲线被另一 个学习器的曲线完全 “包住”，则可断言后者的性能优于前者；
* 若两个学习器 的ROC曲线发生交叉，则难以一般性地断言两者孰优孰劣，此时如果一定要进行比较，则较为合理的判据是比较ROC曲线下的面积，即 AUC (Area Under ROC Curve)。


从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得。假定 ROC 曲线是由坐标为 $\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m)\}$ 的点按序连接而形成 $(x_1=0,x_m=1)$ ，则 AUC 可估算为

$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot (y_i+y_{i+1})$$

形式化地看，AUC 考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定 $m^+$ 个正例和 $m^-$ 个反例，令 $D^+$ 和 $D^-$ 分别表示正、反例集合，则排序 “损失” (loss) 定义为：

![mark](http://images.iterate.site/blog/image/180713/6cJ8L7gd0d.png?imageslim)

即考虑每一对正、反例，若正例的预测值小于反例，则记一个 “罚分” ，若相 等，则记 `0.5` 个 “罚分”。容易看出，$\mathcal{l}_{rank}$ 对应的是 ROC 曲线之上的面积：若 一个正例在 ROC 曲线上对应标记点的坐标为 $(x,y)$ ，则 x 恰是排序在其之前的反例所占的比例，即假正例率。因此有：


![mark](http://images.iterate.site/blog/image/180713/cD94jd1dB8.png?imageslim)

### 代价敏感错误率与代价曲线

<span style="color:red;">没有明白，还是不清楚，而且，在 sklearn 中怎么使用这个？</span>

在现实任务中常会遇到这样的情况：不同类型的错误所造成的后果不同。

例如：

* 在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者, 看起来都是犯了 “一次错误”，但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机。

* 再比如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故。


为权衡不同类型错误所造成的不同损失，可为错误赋予 “非均等代价” (unequal cost)。

以二分类任务为例，我们可根据任务的领域知识设定一个 “代价矩阵” (cost matrix)，如下图所示：

![mark](http://images.iterate.site/blog/image/180713/E843hf15KJ.png?imageslim)

其中 $cost_{ij}$ 表示将第 $i$ 类样本预测为第 $j$ 类样本的代价。一般来说， $cost_{ii}=0$ ；若将第 0 类判别为第 1 类所造成的损失更 大，则 $cost_{01}>cost_{10}$ ；损失程度相差越大， $cost_{01}$ 与 $cost_{10}$ 值的差别越大。一般情况下，重要的是代价比值而非绝对值，例如 $cost_{01}:cost_{10}=5:1$  与 $50:10$ 所起的效果相当。

回顾前面介绍的一些性能度量可看出，它们大都隐式地假设了均等代价，<span style="color:red;">嗯，是的。</span> 例如错误率与精度小节中所定义的错误率是直接计算 “错误次数”，并没有考虑不同错误会造成不同的后果。在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化 “总体代价”(total cost)。若将上面这个代价矩阵中的第 0 类作为正 类、第 1 类作为反类，令 $D^+$ 与 $D^-$ 分别代表样例集 $D$ 的正例子集和反例子，则 “代价敏感” (cost-sensitive) 错误率为：

![mark](http://images.iterate.site/blog/image/180713/m94ldBD3EK.png?imageslim)

类似的，可给出基于分布定义的代价敏感错误率，以及其他一些性能度量如精度的代价敏感版本。若令 $cost_{ij}$ 中的 $i$、$j$ 取值不限于0、1，则可定义出多分类任务的代价敏感性能度量。<span style="color:red;">怎么定义？</span>


在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而 “代价曲线” (cost curve) 则可达到该目的。代价曲线图的横轴是取值为 `[0,1]` 的正例概率代价：

![mark](http://images.iterate.site/blog/image/180713/bkc022ljB8.png?imageslim)

其中 p 是样例为正例的概率；纵轴是取值为 `[0,1]` 的归一化代价：（规范化 (normalization)是将不同变化范围的值映射到相同的固定范围中，常见的是 `[0,1]` ，此时亦称为 “归一化”）

![mark](http://images.iterate.site/blog/image/180713/833H1ah1H2.png?imageslim)

其中 `FPR` 是假正例率，`FNR = 1-FPK` 是假反例率。

代价曲线 的绘制很简单：ROC曲线上每一点对应了代价平面上的一条线段，设 ROC 曲线上点的坐标为 `(TPR,FPR)`， 则可相应计算出FNR，然后在代价平面上绘制 一条从 `(0,FPR)` 到 `(1,FNR)` 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将 ROC 曲线上的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价。

下图即为代价曲线与期望总体代价：


![mark](http://images.iterate.site/blog/image/180713/eJKKI1kk9E.png?imageslim)






## 需要补充的


ROC 曲线在二十世纪八十年代后期被引入机器学习 [Spackman, 1989] ， AUC则是从九十年代中期起在机器学习领域广为使用[Bradley, 1997]，但利用ROC曲线下面积来评价模型期望性能的做法在医疗检测中早已有之[Hanley and McNeil, 1983]. [Hand and Till，2001]将 ROC 曲线从二分类任务推广到多 分类任务.[Fawcett, 2006]综述了 ROC曲线的用途.
[Drummond and Holte, 2006]发明了代价曲线.需说明的是，机器学习过 程涉及许多类型的代价，除了误分类代价，还有测试代价、标记代价、属性代 价等，即便仅考虑误分类代价，仍可进一步划分为基于类别的误分类代价以及 基于样本的误分类代价.代价敏感学习(cost-sensitive learning) [Elkan, 2001; Zhou and Liu，2006]专门研究非均等代价下的学习。

<span style="color:red;">还是有很多需要补充进来的。</span>




## 相关资料

1. 《机器学习》周志华
2. [ROC曲线](https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF#.E6.9B.B2.E7.B7.9A.E4.B8.8B.E9.9D.A2.E7.A9.8D.EF.BC.88AUC.EF.BC.89) 这个还是要补充进来的。这个还没有补充进来，而且，还是没有透彻理解，更别提发散使用了。
