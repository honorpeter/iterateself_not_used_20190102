---
title: 02 假设空间
toc: true
date: 2018-07-30 10:00:28
---
# 假设空间

## 需要补充的

* <span style="color:red;">这个前面提的什么归纳 演绎 析合范式 什么的，有必要在这个地方提吗？感觉有点太多了，或者可以拆分到数学里面。</span>



## 假设空间

归纳 (induction) 与演绎 (deduction) 是科学推理的两大基本手段。

* 归纳：是从特殊到一般的 “泛化” (generalization) 过程，即从具体的事实归结出一般性规律。
* 演绎：则是从一般到特殊的 “特化” (specialization) 过程，即从基础原理推演出具体状况。


比如在数学公理系统中，基于一组公理和推理规则推导出与之相洽的定理，这就是演绎。

那么机器学习所做的 “从样例中学习”  就显然是一个归纳的过程，因此亦称 “归纳学习” (inductive learning)。<span style="color:red;">是的，机器学习有演绎的能力吗？怎么才能做到演绎？现在对于演绎的发展到了什么程度了？</span>

归纳学习有狭义与广义之分：

* 广义的归纳学习：大体相当于从样例中学习
* 狭义的归纳学习：要求从训练数据中学得概念 (concept)，因此亦称为 “概念学习” 或 “概念形成”。

<span style="color:red;">上面的广义和狭义的区别在哪里？看起来好像一样的。</span>

概念学习技术目前研究、应用都比较少，因为要学得泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生 “黑箱” 模型。然而，对概念学习有所了解，有助于理解机器学习的一些基础思想。**到底什么是概念学习，要看下。**

概念学习中最基本的是布尔概念学习，即对 “是” 、“不是” 这样的可表示为 0/1 布尔值的目标概念的学习。

举一个简单的例子，假定我们获得了这样一 个训练数据集：

西瓜数据集：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/6le2ajFjF2.png?imageslim)

这里要学习的目标是 “好瓜“。

我们暂且假设 “好瓜” 可由 “色泽” “根蒂” “敲声” 这三个因素完全确定，换言之，只要某个瓜的这三个属性取值明确了， 我们就能判断出它是不是好瓜。

于是，我们学得的将是：“好瓜是某种色泽、某种根蒂、某种敲声的瓜” 这样的概念。


用布尔表达式写出来则是 $好瓜(色泽=?)\wedge (根蒂=?)\wedge (敲声=?)$ ，这里的 “?” 表示尚未确定的取值，而我们的任务就是通过对上表的训练集进行学习，把 “?” 确定下来。（更一般的情况是考虑形如 $(A\wedge B)\vee (C\wedge D)$ 的析合范式 ）<span style="color:red;">什么是析合范式？ 为什么是这个式子？</span>

OK，可能有人马上发现，表中的第一行：$(色泽=青绿)\wedge (根蒂=蜷缩)\wedge (敲声=浊响)$ 不就是好瓜吗？

是的，但这是一个已见过的瓜，别忘了我们学习的目的是 “泛化”，即通过对训练集中瓜的学习以获得对没见过的瓜进行判断的能力。如果仅仅把训练集中的瓜 “记住” ，今后再见到一模一样的瓜当然可判断，但是，对没见过的瓜，例如  $(色泽=浅白)\wedge (根蒂=蜷缩)\wedge (敲声=浊响)$ 怎么办呢？

我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程。<span style="color:red;">是的。</span>

搜索目标是找到与训练集 “匹配”（fit）的假设，即能够将训练集中的瓜判断正确的假设。

假设的表示一旦确定，假设空间及其规模大小就确定了。

这里我们的假设空间由形如 $(色泽=?) \wedge (根蒂=?)\wedge(敲声=?)$   的可能取值所形成的假设组成。

* 比如对于色泽来说，就有 “青绿” “乌黑” “浅白” 这三种可能取值。
* 当然，我们还需考虑到，也许 “色泽” 无论取什么值都合适。可以用通配符 `*` 来表示， 例如 “ $好瓜 \leftrightarrow （色泽=*） \wedge （根蒂=踡缩）\wedge（敲声=浊响）$”，即：“好瓜是根蒂蜷缩、敲声浊响的瓜，什么色泽都行”。<span style="color:red;">这个为什么要在这个地方提？根假设空间没有很大关系吧？</span>


若 “色泽” “根蒂” “敲声”分别有 3、2、2 种可能取值，则我们面临的假设空间规模大小为 4 x 3 x 3  = 36。（这里我们假定训练样本不含噪声，并且不考虑 “非青绿” 这样的 $\neg A$ 操作。<span style="color:red;">这种要在什么时候考虑？没有很理解？还是没有理解这种情况什么时候考虑？</span>）

但是，实际上，这个 “好瓜” 的概念有可能是不成立的，也就是说世界上没有 “好瓜” 这种东西，因此我们用 $\phi$ 表示这个假设，那么这个时候的空间规模就是： 4x3x3+1=37。（由于训练集包含正例，因此 $\phi$ 假设自然不出现）<span style="color:red;">什么意思？到底会不会出现？</span>

下图显示出了这个西瓜问题假设空间：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/h3eJE0KC78.png?imageslim)

实际上，我们有很多策略可以对这个假设空间进行搜索。比如：自顶向下、从一般到特殊，或是自底向上、从特殊到一般。

我们在搜索过程中可以不断删除与正例不一致的假设、和（或）与反例一致的假设。最终将会获得与训练集一致（即对所有训练样本能够进行正确判断）的假设，这就是我们学得的结果。<span style="color:red;">文字还是没有太明确理解</span>

需要注意的是，现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，<span style="color:red;">是的。</span>我们称之为 “版本空间”（version space）。<span style="color:red;">嗯</span>

例如，在西瓜问题中，与表中的训练集所对应的版本空间如图所示：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/B20fl3b7bJ.png?imageslim)





## 相关资料

1. 《机器学习》周志华
