---
title: 06 稳定性
toc: true
date: 2018-07-30 11:04:41
---
# 稳定性


TODO

- 这一章也没有整理




12.6稳定性
无论是基于VC维还是Rademacher复杂度来推导泛化误差界，所得到的 结果均与具体学习算法无关，对所有学习算法都适用.这使得人们能够脱离具 体学习算法的设计来考虑学习问题本身的性质，但在另一方面，若希望获得与 算法有关的分析结果，则需另辟蹊径.稳定性舛ability?'分析是这方面一个值 得关注的方向.

顾名思义，算法的“稳定性”考察的是算法在输入发生变化时，输出是否 会随之发生较大的变化.学习算法的输入是训练集，因此下面我们先定义训练 集的两种变化.

给定 1? = {勿=(3?1，yi)jZ2 — (®2,1/2)3 • • • 3 = (®m，Vrri} }，是来 自分布P的独立同分布示例，讲={-1，+1}.对假设空间H.. X 4 {-1，+1} 和学习算法私令e W表示基于训练集D从假设空间丸中学得的假设.考 虑P的以下变化：

•    表示移除2?中第S个样例得到的集合

= ｛zlj 之2, • • •，之i—1，^i+1? • . •，zm｝，
•    表示替换D中第i个样例得到的集合

= ｛之1，之2，• • •，之i—1，：i，么i+1，• • •，之饥｝，
其中= (x^y^x,服从分布P并独立于

损失函数奴Ao(a:)，2/) : J x    R+刻画了假设Up的预测标记£d(x)与
真实标记2/之间的差别，简记为^D,z).下面定义关于假设£d的几种损失.

•泛化损失

= ^,Z=(X,y)    .    (12.54)

•经验损失

1 m

?(£, D)=丄 V €(£p,zn .    (12.55)
m i=l

•留一 (leave-one - out)损失
-m
^ioo(^)D) = : —〉:    Zi) .    (12.56)

m i=i

下面定义算法的均匀稳定性(uniform stability):
定义12.10对任何《 G z =    若学习算法£满足

|€(£n,z)-€(£nV,z)| i =    (12.57)
则称£关于损失函数€满足糸均匀稳定性.

显然，若算法H关于损失函数€满足失均匀稳定性，则有

5 z) I
|^(Ad，z) — ^(^dV)| +    —

d
也就是说,移除示例的稳定性包含替换示例的稳定性.

若损失函数€有界，即对所有D和z = (% 2/)有0彡则有 [Bousquet and Elisseeff, 2002]:
证明过程参阅[Bous-quet and Elisseeff, 2002].


定理12.8给定从分布P上独立同分布采样得到的大小为m的示例集 D,若学习算法I!满足关于损失函数彳的/3-均匀稳定性，且损失函数彳的上界 为M，0 < 5 < 1,则对任意m > 1，以至少1 一 J的概率有

P) + 2" + (4m々 + M)    ,

€(£,!?)< ^oo(£, P)+/3+. (4m/3 + M)    .


(12.58)


(12.59)
定理12.8给出了基于稳定性分析推导出的学习算法£学得假设的泛化误 差界.从式(12.58)可看出，经验损失与泛化损失之间差别的收敛率为若 13 = 0(^),则可保证收敛率为0(+).与定理12.3和定理12.6比较可知，这 与基于VC维和Rademacher复杂度得到的收敛率一致.

需注意，学习算法的稳定性分析所关注的是而假设空 间复杂度分析所关注的是supkw |宏㈨-五(叫；也就是说,稳定性分析不必考 虑假设空间中所有可能的假设，只需根据算法自身的特性(稳定性)来讨论输出 假设2d的泛化误差界.那么，稳定性与可学习性之间有什么关系呢？

首先，必须假设口扣4 0,这样才能保证稳定的学习算法£具有一定的泛 化能力，即经验损失收敛于泛化损失，否则可学习性无从谈起.为便于计算，我 们假定冷=去，代入式(12.58)可得

，(二 P) < P) + $ + (4 + M)    .    (12.60)
最小化经验'误差和最小 化经验损失有时并不相同, 这是由于存在某些病态的 损失函数€使得最小化经 驗损失并不是最小化经验 误差.为简化讨论,本章假 定最小化经验损失的同时 会最小化经骚误差.


对损失函数若学习算法公所输出的假设满足经验损失最小化,则称算法 £满足经验风险最小化(Empirical Risk Minimization)原贝U，简称算法是ERM 的.关于学习算法的稳定性和可学习性,有如下定理：

定理12.9若学习算法公是ERM且稳定的，则假设空间并可学习.

证明令g表示并中具有最小泛化损失的假设，即

€(仏 P) = min£(/i，P).

再令

t e e = 2 5

丢= 2exp(-2爪(e’)2),

由Hoe迁ding不等式(I2.6)可知，当m > . In誉时，

以至少1 ~ 5/2的概率成立.令式(12.60)中

| +(4 + M)^M2/5)-e


2m


解得饥=(9(去In!)使

€(£5P)^€(£,D) + |

以至少1 - d/2的概率成立.从而可得

£(£, P) — £(g, T>)    €(£, B) + - — (€(仏 D)—-

£(£, D) — €(^, I?) + e 彡e

以至少1 - d的概率成立.定理12.9得证.

对上面这个定理读者也许会纳闷，为什么学习算法的稳定性能导出假设空 间的可学习性？学习算法和假设空间是两码事呀.事实上，要注意到稳定性与 假设空间并非无关，由稳定性的定义可知两者通过损失函数彳联系起来.





***

下面都是补充阅读


[Valiant, 1984]提出PAC学习，由此产生了 “计算学习理论”这个机器学 习的分支领域.[Kearns and Vazirani, 1994]是一本很好的入门教材.该领域最

重要的学术会议是国际计算学习理论会议(COLT).

VC维的名字就来自两 位作者的姓氏缩写.


VC维由［Vapnik and Chervonenkis, 1971］提出，它的出现使研究无限假 设空间的复杂度成为可能.Sauer引理由于［Sauer, 1972］而命名，但［Vapnik and Chervonenkis, 1971］和［Shelah, 1972］也分别独立地推导出了该结果.本 章主要讨论了二分类问题，对多分类问题，可将VC维扩展为Natamjan维 ［Natarajan, 1989; Ben-David et al., 1995］.

Rademacher 复杂度最早被［Koltchinskii and Panchenko, 2000］引入机器 学习，由［Bartlett and Mendelson，2003］而受至!j重视.［Bartlett et al., 2002］提 出了局部Rademacher复杂度,对噪声数据可推导出更紧的泛化误差界.

机器学习算法稳定性分析方面的研究始于［Bousquet and Elissee迁，2002］ 的工作，此后很多学者对稳定性与可学习性之间的关系进行了讨论，［Mukherjee et al.，2006］和［Shalev-Shwartz et al.? 2010］证明了 ERM 稳定性与 ERM 可学 习性之间的等价关系;但并非所有学习算法都是ERM的，因此［Shalev-Shwartz et al., 2010］进一步研究了 AERM (Asymptotical Empirical Risk Minimization) 稳定性与可学习性之间的关系.

本章介绍的内容都是关于确定性(deterministic)学习问题，即对于每个示 例®都有一个确定的标记y与之对应;大多数监督学习都属于确定性学习问题. 但还有一种随机性(stochastic)学习问题，其中示例的标记可认为是属性的后 验概率函数，而不再是简单确定地属于某一类.随机性学习问题的泛化误差界 分析可参见［Devroye et al., 1996］.




## 相关资料

1. 《机器学习》周志华
