---
title: 06 偏差与方差
toc: true
date: 2018-07-30 16:24:02
---

# 偏差与方差

TODO

- 这一节是不是没有确认完全？好对应pdf 确认下，因为下面的 vd 指的是什么？
- **这一节很厉害呀，讲的还是很清楚的，不过还是没有特别理解，再看下。**




对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它 “为什么” 具有这样的性能。“偏差-方差分解” (bias-variance decomposition) 是解释学习算法泛化性能的一种重要工具。

有可能出现噪声使得 vd

偏差-方差分解试图对学习算法的期望泛化错误率进行拆解。我们知道，算法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布。 对测试样本 \(x\) ，令 \(yD\) 为\(x\) 在数据集中的标记，\(y\) 为 \(x\) 的真实标记，\(f(x;D\) 为训练集 D 上学得模型 f 在 \(x\) 上的预测输出。以回归任务为例，学习算法的期望预测为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/I61DhL34eh.png?imageslim)

使用样本数相同的不同训练集产生的方差为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/G5b29bd6f3.png?imageslim)

噪声为：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/ekaKeiaJ51.png?imageslim)

期望输出与真实标记的差别称为偏差 (bias) ，即


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/lb6iE41C41.png?imageslim)

为便于讨论，假定噪声期望为零，即 \(\mathbb{E}_D[y_D-y]=0\) 。通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/57f444G7Ha.png?imageslim)

上面的式子说明一下：


* 中间的第三步到第四步，由期望预测的公式，最后一项为0
* 倒数第二步到倒数第一步，因为噪声期望为0，因此最后一项为0.


于是：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/lKGk6iJ38m.png?imageslim)

也就是说，泛化误差可分解为偏差、方差与噪声之和.

OK，我们回顾一下上面几个偏差、方差、噪声的公式：




* 偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；
* 方差度量了同 样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的 影响；
* 噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。


偏差-方差分解说明，泛化性能是由：


* 学习算法的能力
* 数据的充分性
* 学习任务本身的难度


所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并 且使方差较小，即使得数据扰动产生的影响小。

很多学习算法都可控制训练程度，例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数。

一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境(bias-variance dilemma)。下图给出了一个示意图：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/9GcAGkiHGf.png?imageslim)

给定学习任务，假定我们能控制学习算法的训练程度，则：


* 在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；
* 随着训练程度的加深, 学习器的拟合能力逐渐増强，训练数据发生的扰动渐渐能被学习器学到，方差， 逐渐主导了泛化错误率；
* 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。







# COMMENT


[Geman et al.5 1992]针对回归任务给出了偏差-方差-协方差分解(bias-variance-covariance decomposition),后来被简称为偏差-方差分解.虽然偏差 和方差确实反映了各类学习任务内在的误差决定因素，但式(2.42)这样优美的 形式仅在基于均方误差的回归任务中得以推导出.对分类任务，由于0/1损失 函数的跳变性，理论上推导出偏差-方差分解很困难.已有多种方法可通过实 验对偏差和方差进行估计[Kong and Dietterich, 1995; Kohavi and Wolpert, 1996; Breiman, 1996; Priedmaa，1997; Domingos, 2000].



# REF

1. 《机器学习》周志华
