---
title: 随机森林 例子1：声纳信号分类
toc: true
date: 2018-08-12 19:51:57
---
# 随机森林 例子1：声呐信号分类


## 项目说明

给你一些声纳信号数据，来区分声纳的类型。

## 项目数据

链接：https://pan.baidu.com/s/1J7SfU1R0eMMq8QztzCzQew 密码：kwrg

## 完整代码



```python
from random import seed, randrange, random
import copy
import math


​
# 导入csv文件
def load_data_set(file_name):
    datas = []
    with open(file_name, 'r') as fr:
        for line in fr.readlines():
            if not line:
                continue
            line_arr = []
            for featrue in line.split(','):
                str_f = featrue.strip()  # 移除空格
                try:
                    # 这里不能用str_f.isdigits()来判断是不是数值，因为小数点会被判定为 false
                    n = float(str_f)
                    line_arr.append(n)
                except:
                    if str_f == "R":  # 分类标签
                        line_arr.append(1)
                    else:
                        line_arr.append(0)
            datas.append(line_arr)
    return datas


​
# 交叉采样 这个采样的方式的名字叫什么？忘记了
# 数据集dataset分成n_flods份，每次都是从dataset中抽取一个dataset/n_folds大小的list，为了用于交叉验证
# 抽出一个list的时候是又放回的还是无放回的？
def cross_validation_split(dataset, n_folds):
    dataset_split = list()
    fold_size = len(dataset) / n_folds
    for i in range(n_folds):
        dataset_copy = copy.deepcopy(dataset)
        fold = list()
        while len(fold) < fold_size:
            index = randrange(len(dataset_copy))
            # 在采样出一个fold的时候，到底使用的是有放回的采样还是无放回的采样？
            fold.append(dataset_copy.pop(index))  # 无放回的方式
            # fold.append(dataset_copy[index])  # 有放回的方式
        dataset_split.append(fold)
    return dataset_split


​
# 尝试根据这一行的这个特征的特征值来划分 dataset
def get_split_result_with_feature_value(feature_index, feature_value, dataset):
    # 根据每个样本对应的这个特征的特征值大于或者小于这个值，来划分为两个列表
    left, right = list(), list()
    for row in dataset:
        if row[feature_index] < feature_value:
            left.append(row)
        else:
            right.append(row)
    return left, right


​
# 为这一次的分割计算基尼指数
def calc_gini_index(groups, labels):  # 个人理解：计算代价，分类越准确，则 gini 越小
    gini = 0.0
    for label in labels:  # labels = [0, 1]
        for group in groups:  # groups = (left, right)
            size = len(group)
            if size == 0:
                # 说明左子树或者右子树没有
                continue
            proportion = [row[-1] for row in group].count(label) / float(size)
            gini += (proportion * (1.0 - proportion))  # 个人理解：计算代价，分类越准确，则 gini 越小
    # 左右两边的数量越一样，说明数据区分度不高，gini系数越大
    return gini


​
# 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）
def get_current_features_split_result(dataset, n_features):
    # 获得标记的set
    labels = list(set(row[-1] for row in dataset))  # class_values =[0, 1]

    # b_index：最优的分类特征。b_value：分类特征值 。b_score：这里使用的是基尼系数。 b_groups：分类结果 ？
    b_index, b_value, b_score, b_groups = 999, 999, 999, None

    # 开始选择出 n_features 个特征
    ifeatures = list()
    while len(ifeatures) < n_features:
        index = randrange(len(dataset[0]) - 1)
        if index not in ifeatures:
            ifeatures.append(index)
    # 从这些特征中找到一个最优的分法
    for index in ifeatures:  # 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性
        for row in dataset:
            # 尝试根据这一行的这个特征的特征值来划分dataset
            groups = get_split_result_with_feature_value(index, row[index], dataset)  # groups=(left, right)
            gini = calc_gini_index(groups, labels)
            if gini < b_score:
                b_index, b_value, b_score, b_groups \
                    = index, row[index], gini, groups
                # 将此时这些特征中分类最好的特征的index及其值返回，同时返回分类的左右类
    return {'index': b_index, 'value': b_value, 'groups': b_groups}


​
# 输出group中出现次数较多的标签
def to_terminal(group):
    outcomes = [row[-1] for row in group]
    # 输出 group 中出现次数较多的标签
    # max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准 max这种用法要总结下。
    return max(set(outcomes), key=outcomes.count)


​
# 为一个结点创建子分割器，递归分类，直到分类结束
# max_depth = 10, min_size = 1,
def split(node, max_depth, min_size, n_features, current_depth):
    left, right = node['groups']
    del (node['groups'])
    # 左边或者右边已经没有了
    if not left or not right:
        # 就不用分了 设定这时候的 node 的左和右是同一种 label
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查是不是超过了最大深度
    if current_depth >= max_depth:
        # 如果超过了，但是还是没有结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左边的
    if len(left) <= min_size:
        # 左边的只有一个样本
        node['left'] = to_terminal(left)
    else:
        # 这个时候 node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，
        # 也就是说 node是一个多层字典
        node['left'] = get_current_features_split_result(left, n_features)
        # 这个地方不明白了，难道每次递归的时候，n_features也要重新选择出来吗？不是使用的开始随机选出来的那些吗？
        # 这个n_features 的确是要重新选择出来的，我试过把features一开始选择出来，然后后面就用这些feature，但是效果基本维持在60% 上不去了
        split(node['left'], max_depth, min_size, n_features, current_depth + 1)  # 递归，depth+1计算递归层数
    # 处理右边的
    if len(right) <= min_size:
        # 右边的只有一个样本
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_current_features_split_result(right, n_features)
        split(node['right'], max_depth, min_size, n_features, current_depth + 1)


​
# 创建一个决策树
# max_depth       决策树深度不能太深，不然容易导致过拟合 为什么？
# min_size        叶子节点的大小
def build_tree(train, max_depth, min_size, n_features):
    # 进行第一次划分
    root = get_current_features_split_result(train, n_features)
    # 对左右两边的数据 进行递归的调用 按照split里面的算法来看，之前用过的feature仍然是可能被选择的，是这样吗？
    split(root, max_depth, min_size, n_features, current_depth=1)
    return root


​
# 简单投票法判断出该行所属分类
def predict(tree, row):  # 预测模型分类结果
    if row[tree['index']] < tree['value']:
        if isinstance(tree['left'], dict):
            return predict(tree['left'], row)
        else:
            return tree['left']
    else:
        if isinstance(tree['right'], dict):
            return predict(tree['right'], row)
        else:
            return tree['right']


​
# 使用一批树，对一行的结果进行预测
def bagging_predict(trees, row):
    predictions = [predict(tree, row) for tree in trees]
    # 返回结果中出现次数最多的结果，相当于投票
    return max(set(predictions), key=predictions.count)


​
# 可放回的重复采样，创建数据集的随机子样本
def sub_sample(dataset, ratio):
    sample = list()
    n_sample = round(len(dataset) * ratio)  # round() 方法返回浮点数x的四舍五入值。
    while len(sample) < n_sample:
        # ，此则自助采样法。从而保证每棵决策树训练集的差异性
        index = randrange(len(dataset))
        sample.append(dataset[index])
    return sample


​
# 随机森林算法
# train           训练数据集
# test            测试数据集
# max_depth       决策树深度不能太深，不然容易导致过拟合
# min_size        叶子节点的大小
# sample_size     训练数据集的样本比例
# n_trees         决策树的个数
# n_features      选取的特征的个数
def evaluate_rf_effect(train, test, max_depth, min_size, sample_ratio, tree_num, n_features):
    trees = list()
    for i in range(tree_num):
        # 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性
        sample = sub_sample(train, sample_ratio)
        # 创建一个决策树
        tree = build_tree(sample, max_depth, min_size, n_features)
        trees.append(tree)
    # OK，现在已经得到这些树了，我们使用这些树，对每一行进行预测
    predictions = [bagging_predict(trees, row) for row in test]
    return predictions


​
# 根据预测值和实际值，计算精确度
def calc_accuracy(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 100.0


​
​
if __name__ == '__main__':
    # 加载数据
    dataset = load_data_set('sonar-all-data.txt')
    # 准备样本数据
    # 将数据集采样为 n_folds 份，其中一个 fold 作为测试集，其余作为训练集，这样可以有 n_folds 套训练和测试样本
    # 遍历整个 folds ，实现交叉验证
    # 数据可以重复重复抽取，每一次 list 的元素是无重复的
    n_folds = 5  # 准备将样本数据分成5份，来进行交叉验证
    folds = cross_validation_split(dataset, n_folds)
    train_sets = []
    test_sets = []
    for fold in folds:
        train_set = list(folds)
        train_set.remove(fold)
        train_set = copy.deepcopy(sum(train_set, []))  # 这也可以，将4个folds合并成一个集合
        test_set = copy.deepcopy(fold)
        train_sets.append(train_set)
        test_sets.append(test_set)

    max_depth = 15  # 调参（自己修改） #决策树深度不能太深，不然容易导致过拟合
    min_size = 1  # 决策树的叶子节点最少的元素数量
    sample_ratio = 0.8  # 做决策树时候的样本的比例
    n_features = int(
        math.sqrt(len(dataset[0]) - 1))  # 需要自己调整，准确性与多样性之间的权衡，为什么n_features 要等于 int(sqrt((dataset[0])-1))？ 有什么说法吗？
    print(n_features)
    for n_trees in [1, 5, 10, 20]:  # 理论上树是越多越好   , 5,10, 20
        scores = list()
        for i in range(n_folds):
            predicted = evaluate_rf_effect(train_sets[i], test_sets[i],
                                           max_depth, min_size, sample_ratio, n_trees, n_features)
            actual = [row[-1] for row in test_sets[i]]
            print(predicted)
            print(actual)
            # 计算随机森林的预测结果的正确率
            accuracy = calc_accuracy(actual, predicted)
            scores.append(accuracy)
        seed(1)
        print('Trees: %d' % n_trees)
        print('Scores: %s' % scores)
        print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))
```


输出如下：

```
7
[1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 1
Scores: [57.14285714285714, 64.28571428571429, 57.14285714285714, 54.761904761904766, 73.80952380952381]
Mean Accuracy: 61.429%
[0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 5
Scores: [73.80952380952381, 73.80952380952381, 61.904761904761905, 78.57142857142857, 80.95238095238095]
Mean Accuracy: 73.810%
[0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 10
Scores: [80.95238095238095, 78.57142857142857, 61.904761904761905, 78.57142857142857, 88.09523809523809]
Mean Accuracy: 77.619%
[0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 20
Scores: [85.71428571428571, 78.57142857142857, 69.04761904761905, 83.33333333333334, 88.09523809523809]
Mean Accuracy: 80.952%
```

嗯，还是有几个问题要说一下的：


## 几个参数的说明与调整


* max_depth 我现在调到15，调到20的时候反而没有15的时候效果好，**那么实际中到底怎么确定哪个参数好呢？**
* sample_ratio 我用的是0.8，也就是说每棵树在生成的时候是根据train中的80%的样本生成的。
* n_features 这里我用的是 math.sqrt(len(dataset[0]) - 1)) **没明白为什么要用这个？**
* **min_size 会有设置为不为1 的时候吗？**




## 对于一颗树来说，每次split的时候都要重新选择 n_features 个 feature吗？


之前，我以为，随机森林的每棵树在选择特征集的时候，只是在开始的时候选择一些特征，然后后面的split就用这些特征，但是这个代码看下来好像不是这样，而是每次的split，都是从完整特征集里面重新选出n_features个特征。**为什么要这样做呢？**

不过，我试了下，如果只是开始的时候选一些特征，然后后面就用这些，那么最后的accuracy基本上就是60%左右，而且上不去了，如果像上面这样的话，就可以随着tree_num的增加一直增加accuracy。**嗯，但是还是不清楚，这样做有什么道理呢？**


## 判断一个string是不是一个数的时候，怎么判断？


之前程序里用的是 a.isdigits() 来判断 a 里面是不是每个字符都是数字，但是这样对于 0.02 这样的float 会判断为False，因为有小数点。因此这里直接用 try 来进行转换。


## list里面的item也是list的时候，怎么拷贝？


这个，用 import copy    b=copy.deepcopy(a) 就可以，其它的比如[:] ，list(a) 什么的都不能将里面的list也进行拷贝。


## 标签的名字应该怎么取？用 class_value 还是用 labels？


**这个一直想知道，我这里用的是labels，但是到底应该用什么？**







## 相关资料

1. [第7章 集成方法 ensemble method](http://ml.apachecn.org/mlia/ensemble-random-tree-adaboost/#_3)
