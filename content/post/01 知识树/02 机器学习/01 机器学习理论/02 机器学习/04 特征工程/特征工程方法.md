---
title: 特征工程方法
toc: true
date: 2018-08-21 18:16:48
---
# 特征工程方法


# REF

1.[数据挖掘中常见的特征工程方法](https://zhuanlan.zhihu.com/p/33072222)

TODO

* **没有看，需要修改，要以书上为基准，博客的只能是辅助**


这几天在做一个数据挖掘相关的东西，炼丹练久了，突然发现对特征工程这一块还存在比较大的空白。于是查阅了一些资料，权当记录阅读笔记。

特征工程在数据挖掘应用中直接影响模型最终的性能；尤其在很多计算机视觉任务中，特征提取的重要性甚至超过了分类器本身（比如CNN提取的feature是比很多hand-crafted features更具备表示能力的）。良好的特征（feature）应该与标签（label）高度相关，并且与其他特征不相关。

特征工程（feature engineering）包括**特征提取**和**特征选择**两个方面。

**特征提取**广义上指的是一种变换, 将处于高维空间的样本通过映射或变换的方式转换到低维空间, 达到降维的目的；

**特征选择**指从一组特征中去除冗余或不相关的特征来降维。

这里暂时只介绍特征选择部分！如有错误，欢迎指正！



## 主要思路


特征获取需要解决两个问题，

一是**确定选择算法**，在允许的时间内，以可以忍受的代价找出最小的、最能描述类别的特征组合；

二是**确定评价标准**, 衡量特征组合是否最优,得到特征获取操作的停止条件。 因此, 一般分两步进行特征获取，先产生特征子集，然后对子集进行评价，如果满足停止条件，则操作完毕，否则重复前述两步直到条件满足为止。

**按照特征评价标准分类：**



  * 选择使分类器的错误概率最小的特征或者特征组合。
  * 利用距离来度量样本之间相似度。
  * 利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。
  * 利用相关系数, 找出特征和类之间存在的相互关系；
利用特征之间的依赖关系, 来表示特征的冗余性加以去除。





## Search

* forward selection：以空集开始，逐渐向里面添加特征，直到满足stopping criterion（常用于高维数据挖掘场景）。
* backward elimination：包含所有特征并逐渐删除，直到满足stopping criterion。
* forward selection + backward elimination






## Evaluation


  * Filters：不依赖于学习算法，对独立特征或特征子空间进行评估；
    * Mutual Information(MI)：度量每个特征与标签的MI值，选取其中**Top N**个的特征。
      * $I(A,B)=\sum_{i}{\sum_{j}{Pr(a_i, b_j)log\frac{Pr(a_i, b_j)}{Pr(a_i)*Pr( b_j)} } }$

    * Chi-Square：被用于测试两个相互独立的事件A，B的偏差程度。
      * $\chi^2(t,c)=\frac{N\times(AD-CB)^2}{(A+C)(B+D)(A+B)+(C+D)}$

      * 式中，N 表示训练语料中的文档总数；c 为某一特定类别; t 表示特定的词条； A 表示属于 c 类且包含 t 的文档频数； B 表示不属于 c 类但是包含 t 的文档频数；C 表示属于 c 类但是不包含 t 的文档频数; D 是既不于 c 也不包含 t 的文档频数。
      * 若chi-square足够小，就认为两者是独立的；若chi-square大到一定程度，就认为两者是相关的。

    * Pearson Correlation Coefficients
    * Euclidean Distance
    * T-Test：评估两组样本均数之间的差异程度。
      * $T=\frac{\bar{X}-\bar{Y} }{\sqrt{\frac{S_1^2}{n_1}+\frac{S_s^2}{n_2} } }$

    * Laplacian Score(LS)：LS通常用于unsupervised learning如聚类中，它假定相互关联的特征在整个特征空间中，其临近记录与该特征向量也很接近。
在Label存在的supervised learning任务，可使用Fisher Criterion Score (FCS)。
    * Information Gain：考察某特征 t 出现与不出现的时候两者差值，即为该特征给系统带来的信息量；差值越大，说明该特征越重要。
      * $IG(t)=-\sum_{i=1}^m{P(c_i)log{P(c_i)} }+P(t)\sum_{i=1}^m{P(c_i|t)log{P(c_i|t)} }+P(\bar{t})\sum_{i=1}^m{P(c_i|\bar{t})log{P(c_i|\bar{t})} }$

      * $P(c_i)$表示 $c_i$类文档在语料中出现的概率； $P(t)$ 表示语料中包含词条 _t_ 的文档的概率； $P(c_i|t)$ 表示文档包含词条 _t _时属于 $c_i$ 类的条件概率； $P(\bar{t})$ 表示语料中不包含词条 _t _的文档的概率； $P(c_i|\bar{t})$ 表示文档不包含词条 _t_ 时属于 $c_i$ 的条件概率; _m _表示类别数。

  * Wrappers：利用学习算法来评估特征子空间，如Classifier的准确率。优点是Wrappers通常比Filters更准确；缺点是计算量较大。
  * Embedded： 特征选择算法是作为学习算法的部分嵌入其中的，特征选择和训练过程同时进行。常见的有Decision Tree和 Deep Neural Networks。





# COMMENT
