---
title: 06 类别不平衡问题
toc: true
date: 2018-06-26 09:24:58
---


# TODO
- aa



类别不平衡问题

前面介绍的分类学习方法都有一个共同的基本假设，即不同类别的训练样例数目相当。如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别很大，则会对学习过程造成困扰.例如有998个反例，但正例只有2个,那么学习方法只需返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度;然而这样的学习器往往没有价值，因为它不能预测出任何正例。<span style="color:red;">嗯，是呀</span>

对 OvR、MvM 来说，由于对每个类进行了相同的处理，其拆解出的二分类 任务中类别不平衡的影响会相互抵消，因此通常不需专门处理.<span style="color:red;">没明白</span>

类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。为了不失一般性，本节假定正类样例较少，反类样例较多. 在现实的分类学习任务中，我们经常会遇到类别不平衡，例如在通过拆分 法解决多分类问题时，即使原始问题中不同类别的训练样例数目相当，在使用 OvR、MvM 策略后产生的二分类任务仍可能出现类别不平衡现象，因此有必要了解类别不平衡性处理的基本方法.

从线性分类器的角度讨论容易理解，在我们用 $y = w^Tx + b$ 对新样本 $x$ 进行分类时，事实上是在用预测出的 $y$ 值与一个阈值进行比较，例如通常在 $y> 0.5$ 时判别为正例，否则为反例.实际上表达了正例的可能性，几率 $\frac{y}{1-y}$ 则反映了正例可能性与反例可能性之比值，阈值设置为0.5恰表明分类器认为 真实正、反例可能性相同，即分类器决策规则为

若 $\frac{y}{1-y}>1$ 则预测为正例. (3.46)

然而，当训练集中正、反例的数目不同时，令 $m^+$ 表示正例数目， $m^-$ 表示反例数目，则观测几率是由于我们通常假设训练集是真实样本总体的无偏釆样，因此观测几率就代表了真实几率.于是，只要分类器的预测几率高于观测 几率就应判定为正例，即

若 $\frac{y}{1-y}>\frac{m^+}{m^-}$ 则预测为正例. (3.47)

但是，我们的分类器是基于式(3.46)进行决策，因此，需对其预测值进行调整，使其在基于式(3.46)决策时，实际是在执行式(3.47).要做到这一点很容易，只需令

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/Fb47Ecf5Kg.png?imageslim)

亦称“再平衡” (rebalance).

这就是类别不平衡学习的一个基本策略——“再缩放”(rescaling).

再缩放的思想虽简单，但实际操作却并不平凡，主要因为“训练集是真实 样本总体的无偏采样”这个假设往往并不成立。也就是说，我们未必能有效地基于训练集观测几率来推断出真实几率.现有技术大体上有三类做法：

- 第一类是直接对训练集里的反类样例进行“欠采样”(undersampling)，即去除 一些反例使得正、反例数目接近，然后再进行学习；
- 第二类是对训练集里的 正类样例进行“过采样”(oversampling)，即增加一些正例使得正、反例数目接近，然后再进行学习；
- 第三类则是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将式(3.48)嵌入到其决策过程中，称为“阈值移动” (threshold-moving).

关于欠采样法和过采样法：

- 欠采样法的时间开销通常远小于过采样法，因为前者丢弃了很多反例，使得分类器训练集远小于初始训练集，欠采样法若随机丢弃反例，可能丢失一些重要信息；欠采样法的代表性算法 EasyEnsemble [Liu et al.5 2009]则是利用集成学习机制，将反例划分为若干个 集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息.<span style="color:red;">怎么实现的？</span>
- 而过采样法増加了很多正例，其训练集大于初始训练集。需注意的是，过采样法不能简单地对初始正例样本进行重复采样，否则会招致严重的过拟合；过采样法的代表性算法 SMOTE [Chawla et al.5 2002]是通过对训练集里的正例进行插值来产生额外的正例<span style="color:red;">怎么实现的？</span>


值得一提的是，“再缩放” 也是 “代价敏感学习”(cost-sensitive learning) 的基础. 在代价敏感学习中将式(3.48)中的 $m^-/m^+$ 用 $cost^-/cost^+$ 代替即可，其中 $cost^+$ 是将正例误分为反例的代价，$cost^-$ 是将反例误分为正例的代价.<span style="color:red;">没明白？</span>



















# COMMENT


下面是补充阅读的材料

“稀疏表示” (sparse representation)近年来很受关注，但即便对多元线性 回归这样简单的模型，获得具有最优“稀疏性”(sparsity)的解也并不容易.稀 疏性问题本质上对应了 Lo范数的优化，这在通常条件下是NP难问题.LASSO [Tibshirani, 1996]通过范数来近似Lo范数，是求取稀疏解的重要技术.

可以证明，OvO和OvR都是ECOC的特例［Allwein et at, 2000］.人们以 往希望设计通用的编码法，［Crammer and Singer, 2002］提出要考虑问题本身 的特点，设计“问题依赖”的编码法，并证明寻找最优的离散编码矩阵是一个 NP完全问题.此后，有多种问题依赖的ECOC编码法被提出，通常是通过找 出具有代表性的二分类问题来进行编码［Pujol et al., 2006, 2008］. ［Escalera et al.，2010］开发了一个开源ECOC库.

MvM除了 ECOC还可有其他实现方式，例如DAG (Directed Acyclic Graph)拆分法［Platt et al., 2000］将类别划分表达成树形结构，每个结点对应 于一个二类分类器.还有一些工作是致力于直接求解多分类问题,例如多类支 持向量机方面的一些研究［Crammer and Singer, 2001; Lee et al., 2004］.

代价敏感学习中研究得最多的是基于类别的“误分类代 价” (misclassification cost),代价矩阵如表2.2所示；本书在提及代价敏感 学习时，默认指此类情形.已经证明，对二分类任务可通过“再缩放”获得理论 最优解［Elkan, 2001］,但对多分类任务，仅在某些特殊情形下存在闭式解［Zhou and Liu? 2006a］.非均等代价和类别不平衡性虽然都可借助“再缩放”技术， 但两者本质不同［Zhou and Liu, 2006b］.需注意的是，类别不平衡学习中通常 是较小类的代价更髙，否则无需进行特殊处理.

多分类学习中虽然有多个类别，但每个样本仅属于一个类别.如果希望为 一个样本同时预测出多个类别标记，例如一幅图像可同时标注为“蓝天”、 “白云”、“羊群”、“自然场景”，这样的任务就不再是多分类学习，而是 “多标记学习”(multi-labellearning)，这是机器学习中近年来相当活跃的一个 研究领域.对多标记学习感兴趣的读者可参阅［Zhang and Zhou, 2014］.


# COMMENT
欠采样亦称“下采样” (downsampling),过采样 亦称“上采样” (upsampling)-


## 相关资料
  1. 《机器学习》周志华
