---
title: 回归
toc: true
date: 2018-08-21 18:16:23
---






# TODO

- **认真总结**
- **需要添加 Andrew Ng,Machine Learning。和统计学习方法的东西。**
* **将线性回归拆分出去，并且局部加权线性回归要说清楚点，机器学习实战 书上提到了**
* **把Logistic回归拆分出去，而且，为什么logistic回归叫着回归的名字却是用来做分类的呢？**
* **把梯度下降拆分出去**
* **岭回归和Lasso要添加进来，并拆分到线性回归里面**




# MOTIVE

  * 对机器学习中回归问题相关的总结


# 知识前提


  * 矩阵变换与特征值


  * 向量求导


  * **还有一些知识这个地方会用到，要补充下。**





# 回归介绍




## 到底什么是回归？


我们已经知道一些分类的算法了，分类的目标变量是标称型数据，也就是说，只要把样本对应到某个类别上就行。

但是，有些问题，比如说给定一些过去每天的股价信息，然后让你预测今天的股价，那么这个股价就不再是一个类别了，而是一个数，把每天的股价看作样本，就相当于根据这些样本得到今天的样本对应的值。这其实就是一个回归问题，回归的目的是预测样本的数值型目标值。


## 为什么叫回归这个名字？


回归的目的是预测数值型的目标值。

而我们最直接的办法就是依据输入写出一个目标值的计算公式。

比如说，你想要预测兰博基尼跑车的功率大小，可能会这样计算：


HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio


那么这个方程就是所谓的 回归方程(regression equation)，其中的 0.0015 和 -0.99 被称作 回归系数（regression weights），而求这些回归系数的过程就是回归。所以回归的大体意思就是这样。**还是优点不清晰。**


## 有那些回归的方法？


说到回归，一般都是指 线性回归(linear regression)。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。






# 线性回归




## 简单介绍


回归就是：万事万物都向中间靠拢，这个过程叫做回归，比如太阳不断的回归到赤道。

y=ax+b， x是房屋面积，y是房屋价格


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/856eIehmCC.png?imageslim)

考虑两个变量的时候呢：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/EBFemBC2H3.png?imageslim)



![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/B133b68Bhk.png?imageslim)

在上面的式子中，可以在\(\theta_0\)的地方加上一个强制为1的\(x_0\)，这样就得到了下面的式子，而且可从i=0开始。

要注意的是，这个地方的\(\theta^Tx\)里面的\(\theta\)和\(x\)都是向量了。

那么现在也就是说已经有了 \(x\) 和 \(h_{\theta}(x)\) ，让你来估计 \(\theta\)，这个就是线性回归的基本想法。这个地方要注意 \(x\)  是已知的， \(h_{\theta}(x)\) 也是已知的，这个式子是关于\(\theta\) 的一个函数，虽然我们写成的是 \(h_{\theta}(x)\) 。

那么我们怎么求这个 \(\theta\) 呢？


## 先给样本添加满足正态分布的误差


首先对于第i个样本来说：

\[y^{i}=\theta^Tx^{i}+\epsilon^{i}\]

每个样本有一个对应的\(\epsilon\)，是一个噪声或者说误差。

因为每个样本是独立的，所以可以认为每个\(\epsilon\)是独立的，而且可以认为是相同分布的。

而它可以认为是大量的细节的未知的东西影响得到的，而这样的东西根据[中心极限定理](http://106.15.37.116/2018/03/31/ai-probability-statistics-the-law-of-large-numbers-and-the-central-limit-theorem/)可以认为它的分布是最最正常状态的分布，即正态分布，也即高斯分布。

由于\(\theta\)里面含有\(\theta_0\)，对应的\(x_0\)为1，因此，如果\(\epsilon\)的均值不是0，那么总是可以将不是0的部分划到\(\theta_0\)里面。所以可以认为\(\epsilon\)满足的这个高斯分布均值是0，方差是\(\sigma^2\)。**嗯 是的，可以这么认为。**


##




## 进而转化为 y 的似然函数


\(\epsilon\)的概率密度函数为：

\[p(\epsilon^(i))=\frac{1}{\sqrt{2\pi}\omega}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})\]

将\(y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\)带入得到：

\[p(y^{(i)}\mid x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\]

这个地方有两个地方要注意下：① 这个式子本质上是一个关于\(\theta\)的一个函数。② 这个式子实际上是表达在 \(x^{(i)}\) 和 \(\theta\) 的条件下 \(y^{(i)}\) 的概率，也就是说 \(y^{(i)}\) 出现的概率，这样我们就可以把所有的概率密度函数乘起来，就得到下面的似然函数：

\[\begin{align*}L(\theta)&=\prod_{i=1}^{m}p(y^{(i)}\mid x^{(i)};\theta)\\&=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\end{align*}\]


## 这时就可以求这个似然函数的最小值




然后，我们怎么去求这个似然函数的最小值呢？可以先取对数，得到对数似然函数：


\[\begin{align*}l(\theta)&=log\,L(\theta)\\&=log(\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}))\\&=\sum_{i=1}^{m}log(\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}))\\&=m\,log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}\cdot \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^Tx^{(i)})^2\end{align*}\]


## 那么这个时候就可以得到最小二乘


我们想做的是[极大似然估计](http://106.15.37.116/2018/03/31/ai-probability-statistics-moment-estimation-and-maximum-likelihood-estimation/#_MLE)，即想求\(l(\theta)\)的最大值，而m是样本的数量，\(\sigma\)是噪声的方差，可以认为是一个固定的值，那么就可以转化成求下面这个式子的最小值：**这个\(h_\theta (x^{(i)})\)就是上面的\(\theta^Tx^{(i)}\)，因此实际上没错。**

\[J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta (x^{(i)})-y^{(i)})^2\]

**求这个式子的最小，就是所谓的最小二乘。**可见，**最小二乘是在假定误差为正态分布的情况下，使用极大似然估计推导得出的结论。****这个地方要注意：如果在特定场景下噪声的分布不是正态分布，那么就不能用正太分布的公式来做。**

对于上面这个式子来说，只有\(\theta\)是未知数，那么怎么去求呢？




## 那么怎么求最小二乘时候的最优解呢？





### 先明确一下目标函数


将M个N维样本组成矩阵X，其中，X的每一行对应一个样本，共M个样本，X的每一列对应样本的一个维度，共N维。其实还有额外的一维常数项，全为1。

那么，目标函数就是：因为模的平方相当于转置与自身的点乘。

\[J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta (x^{(i)})-y^{(i)})^2\]

因为模的平方相当于转置与自身的点乘：

\[J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta (x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)\]


### 然后对\(\theta\)求梯度


这是一个关于\(\theta\)的方程，那么可以对\(\theta\)求梯度：

\[\begin{align*}\bigtriangledown_\theta J(\theta)&=\bigtriangledown_\theta (\frac{1}{2}(X\theta-y)^T(X\theta-y))\\&=\bigtriangledown_\theta (\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y))\\ &=\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))\\&=\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)\\&=X^TX\theta-X^Ty\\&\overset{求驻点}{\rightarrow}0\end{align*}\]

说明一下。在\(\bigtriangledown_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))\)中，\(\theta^TX^TX\theta\) 的\(\X^TX\)一定是个对称阵，因此根据前提知识点：标量对向量的导数得到\(2X^TX\theta\)，而 \(\theta^TX^Ty\) 和 \(y^TX\theta\) 根据前提知识点：向量的导数 得到\(X^Ty\)和(y^TX)^T。


### 于是就得到了\(\theta\)




现在既然\(X^TX\theta-X^Ty=0\)，那么\(\theta\)就是下面的式子，参数的解析式如下：


\[\theta=(X^TX)^{-1}X^Ty\]

上面这个就是我们得到的结论。


### 那么如果\(X^TX\)不可逆或过拟合怎么办呢？


如果\(X^TX\)不可逆或防止过拟合，增加\(\lambda\)扰动，保证它一定是正定的。因为\(X^TX\)一定是半正定的。如果\(X^TX\)是不可逆的，那么就不能求逆，而且，如果有100个样本而n的维度是10000，那么肯定过拟合了，而且一定不可逆，而\(\lambda\)的取值一般是从小的值开始试就行。这个加 \(\lambda\) 其实不是随便加的，其实可以把它认为是极大后验估计MAP。**为什么？****为什么可以保证是正定的？为什么可以加这个\(\lambda\).  \(X^TX\)不一定满秩，所以一定不可逆，然后\(\lambda\)是L2正则项。**

\[\theta=(X^TX+\lambda I)^{-1}X^Ty\]


### 对加入的扰动 λ 再做一些讲解




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/LlEclDBDme.png?imageslim)




### 对涉及到的 广义逆矩阵（伪逆）的讲解


A为非奇异矩阵,则线性方程组Ax=b的解为\(x=A^{-1}b\)其中A的逆矩阵\(A^{-1}\)满足\(A^{-1}A=AA^{-1}=I\) (I为单位矩阵)。若A是奇异阵或长方阵,\( x=A^+b\)。\(A^+\)叫做A的伪逆阵。**什么是非奇异？**

1955年R.Penrose证明了对每个m×n阶矩阵A,都存在惟一的n×m阶矩阵X，满足：




  1. AXA=A


  2. XAX=X


  3. (AX)*＝I


  4. (XA)*＝I


通常称 X 为 A 的穆尔-彭罗斯广义逆矩阵,简称M-P逆，记作\(A^+\) 。

在矛盾线性方程组Ax＝b的最小二乘解中, \(x=A^+b\)是范数最小的一个解。

在奇异值分解SVD的问题中，将继续该话题的讨论。






# 梯度下降算法




## 为什么要用梯度下降法？


为什么要用梯度下降法？上面不是已经把最优点的\(\theta\)求出来了吗？

事实上，一般而言，不是直接用上面的\(\theta\)的解析式求的，因为里面有一个方阵求逆运算的操作，这个一般来说时间复杂度是\(O(N^3)\)的。**现在有没有更好的方法？**

因此一般而言，也为了变成方便 因此使用梯度下降

而梯度下降是几乎任何一个函数都可以使用的方法。也是机器学习里面用的最广泛的。实践中也是应用的最广泛的。


## 那么什么是梯度下降算法呢？


以第 j 维对应的 \(\theta\) 为例：




  1. 初始化\(theta) 随机初始化


  2. 迭代，新的\(theta)能够使\(J(\theta)\)更小。\(\theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)\)。即对\(J(\theta)\)求偏导，然后沿着负的方向不断走。其中 \(\alpha\) 被称为学习率或者步长。**为什么是 \(\theta_j\) ？哦，这个是第 j 维对应的 \(\theta\) ，实际上，每个维度对应的\(\theta\) 都是要自己不断迭代的。**


  3. 如果\(J(\theta)\)能够继续减小，返回2


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mc3b8BEFag.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/ABm46IK4Cc.png?imageslim)




## 好了，开始计算最小二乘的梯度是什么

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/FIE2Ida4dd.png?imageslim)


## 有了梯度就可以进行梯度下降了


因此将这个梯度带入到梯度下降的式子里，而且如何利用这m个样本呢？可以对所有的样本每个都求梯度，假定所有的样本的重要程度是一样的（**实际上，样本的重要程度可能是不一样的，就比如下面介绍的局部加权线性回归**），然后加起来，走一个很小的\(\alpha\)，因此得到下面的式子：

\[\theta_j:=\theta_j+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}\]


由于是n维的数据，有 n 个 \(\theta\)，因此上面的式子要计算 n 遍，然后进行下一轮迭代。


这个就是所谓的**批处理的梯度下降算法**。能够保证每走一步就能够接近我们最真实的\(\theta\)。

由于\(J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta (x^{(i)})-y^{(i)})^2\) 关于\(\theta\)是一个凸函数，因此对于任何一点，可以沿着梯度求出局部最小值，并且一定会收敛到全局最小值。


## 梯度下降的一些信息


梯度下降的缺点就是需要给定一个\(\alpha\)，这个值是需要给定的，但是确可以不固定的，每一步给一个\(\alpha\)也是可以的。这个的给定有一些比较有趣的方案，方案下次讲，**讲了之后补充过来。**


对于最小二乘法来说，梯度下降能够得到全局最优解，但是对于别的方法就不一定，比如：深度学习的softmax回归，可以用梯度下降的方式来求解，但是求得的是一个局部的最优解。但是深度学习往往会忽略这个问题，第一是因为太复杂了不太容易求最优解，第二是因为往往求出局部最优解运行的效果还不错。


如果我先求\(theta_1\)，那么我求\(\theta_2\)的时候，不用老的\(theta_1\)，而用新的\(theta_1\)行不行？求完\(\theta_2\)之后，再用更新之后的\(\theta_1\)和\(\theta_2\)来求\(\theta_3\) 行不行？对于线性回归这个目标函数而言，这种做法是没有问题的，并且应该在其他条件都一样的情况下，是可以加速的，这个有个名字叫做坐标上升。**没明白？**


## 批处理梯度下降图示




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/2A0ijfLkl7.png?imageslim)




## 随机梯度下降算法


刚才是m个样本加起来之后再去更新\(\theta\)，能不能每拿到一个样本就把所有的\(\theta\)给更新一遍？再拿到一个样本再更新一遍？

实际上是可以的：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/8eI07eB2Ja.png?imageslim)

这种方式比较激进，叫做随机梯度下降，实际上这种方式比批处理的方式更值得推荐，因为m一般比较大，而且以网络流的方式的时候m不可能全部拿到，而是一点一点拿到。

当然可以在随机之上做一个折中：拿到一些，对这一些进行求和，再更新。这样就是下面的 mini-batch


## mini-batch 梯度下降算法


不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。**这个是非常值得推荐的。**






# 梯度下降代码




## 批处理梯度下降  要自己实现一下




### 代码


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/E67EDb9AEf.png?imageslim)




### 学习率代码


可以按照一个规则，再当前梯度下给出一个比较好的\(\alpha\).


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/ajba0523fH.png?imageslim)




### 线性回归、rate、Loss

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/25d95ehAi9.png?imageslim)


##  随机梯度下降




### 代码




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/70gcD5bb21.png?imageslim)




###  输出


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/1If1iJB1AJ.png?imageslim)

对比看一下，基本看不出那个好，那个差。不过可以看出随机梯度下降是比较震荡的，但是批处理的方式是一直往下的 。


## 结果对比：




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/48583H4L9E.png?imageslim)

可见，基本是差不多的，非常接近，批处理是1.69,3   随机梯度下降是 1.65,3  。在实际中优先使用随机梯度下降。




# 线性回归的进一步分析




## 线性回归解决分类问题是否可行？


可不可以？


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mKc1FaiEfj.png?imageslim)

如下图：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/8Fc9g2B8mm.png?imageslim)

上图中：紫色： 线性回归   绿色：Logistic回归，如果样本有偏，那么进行线性回归的时候，紫色的线显然不适合做分类，而绿色的是适合的。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/dmelF6haAH.png?imageslim)

上图中：左侧：线性回归   右侧： Softmax回归 三个样本，拟合出两条线，那么线性回归显然也不合理。

所以说，直接用线性回归解决分类是不合理的。


所以说出了线性回归以外，还需要研究别的回归。比如Logistic回归。





#




## 多项式回归


线性回归为什么叫线性回归呢？线性体现在那里呢？可以对样本是非线性的，只要对参数θ线性就行。

左边是：\(y=\theta_0+\theta_1x\) 右边是： \(y=\theta_0+\theta_1x+\theta_2x^2\)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/F1K9L680a5.png?imageslim)

这个时候右边虽然有\(x^2\)，但是仍然是可以用线性回归，只要比左边多添加一个\(x^2\)的维度就行。所以只要系数是线性的就可以用线性回归。


## 局部加权线性回归


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/2E9D7Ajgmb.png?imageslim)



黑色是样本点，那么我们用红色的线来拟合肯定是不太合理的。


但是虽然整体而言不是线性的，但在某一点的周围应该是线性的，而这个点对于别的点的影响呢，离这个点越远，被影响的越小，越近的，被影响的越大。因此可以根据这个让所有的样本对这个点做加权，离得远的权值低，离得近的权值高。而之前我们在批量梯度下降中的时候，是认为所有的样本的权重是相同的，现在我们认为每个样本有自己的不同的权重。而且这个权重是跟样本与我选中的这个点的距离有关的。我就对这一点做插值，做回归。也就是所谓的局部的回归，但是怎么做加权呢？**还是有些没明白？怎么设定加权？为什么可行？**


### 局部加权线性回归




局部加权线性回归 LWR：Locally Weighted linear Regression

之前是这样的：即求最小二乘


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mb523LfgH3.png?imageslim)

现在加了一个权重\(w^{(i)}\)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/ffB68GBg79.png?imageslim)

那么这个权值怎么定呢？


### 权值的设置 核函数


之所以叫核函数，感觉是因为只是对数据之间做的一些变化，像是一种数值映射。跟算法是相对独立的。高斯核可以用在线性回归上，也可以用在SVM上**。**

\(\omega\) 的一种可能的选择方式(高斯核函数)：**什么是高斯核函数？**

\[\omega^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2}\]

对上式进行理解：




  * \(\tau\) 称为带宽，它控制着训练样本随着与 \(x^{(i)}\) 距离的衰减速率，即控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。


  * x 某个预测点，就是我想算的那个当前的那个点，就是上面图中的大红点。


  * x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。


\(\omega\) 也可以用多项式核函数：

\[\kappa(x_1,x_2)=(\left\langle x_1,x_2 \right\rangle +R)^d\]

上面的式子中\(\left\langle x_1,x_2 \right\rangle\)是做点乘的意思，加上一个常数\(R\)。比如d=2，得到一个跟距离的平方求反比的一个核。

推荐用高斯核。

**这个理解的不够，有什么实践吗？线性回归加高斯核好用吗？**


## 局部回归与多项式回归那个好


如果方便去计算的话，可以用多项式回归，即加个平方像，关键是要真的可以满足条件，如果是不知道是不是多项式什么的，而且拐来拐去的就用局部加权。

局部加权回归最大的麻烦就是：它是非参数化的，每个样本都无法丢掉，模型的参数的数目和我的样本点是一个量级的，它是一个非参数化的一个学习方法。最烦的就是样本是丢不掉的。也就是说，上面的图中的绿色的线是没有一个解析式的。

但是用多项式回归就可以丢掉样本，因为我的模型已经学出来了。








# Logistic回归


现在我们回到Logistic回归。

之前用二项分布推导出来过Logistic函数。




## Logistic 函数


\[g(z)=\frac{1}{1+e^{-z} }\]

\(\theta^Tx\)是线性的，把这个线性变化带到这个Logistic函数里面去：

\[h_{\theta}(x)=g(\theta^T x)=\frac{1}{1+e^{-\theta^Tx} }\]

这个就是我们的Logistic回归函数，有时候也叫sigmoid函数，图像如下：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/l2kl6356mI.png?imageslim)

这个地方提一下，\(g(z)\)的导数是：

\[\begin{align*} g'(z)&=\frac{d}{dz}\frac{1}{1+e^{-z} }\\&= \frac{1}{(1+e^{-z})^2}(e^{-z})\\&=\frac{1}{(1+e^{-z})}\cdot (1-\frac{1}{(1+e^{-z})})\\&=g(z)(1-g(z))\end{align*}\]


## 让 y 服从二项分布


这个时候我们再去看这个Logistic回归的参数分布：

之前我们说过如果样本的噪声服从的是高斯分布，那么我们的样本 y 服从的是x和\(\theta\)的一个高斯分布。即：\(p(y^{(i)}\mid x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})\)。现在我们不让这个 y 服从高斯分布，而是服从二项分布。那么可以这么写：假定：**为什么可以设定维服从二项分布？**

\[P(y=1\mid x;\theta)=h_{\theta}(x)\]

\[P(y=0\mid x;\theta)=1-h_{\theta}(x)\]

为什么呢？因为Logistic函数的值域是0-1的，而且还是递增的，所以可以把它看成是一个概率分布，所以可以这么写。**没明白？\(h_{\theta}(x)\)是什么？**

上面两个式子合起来：等于1时候的1次方乘以等于0时候的1次方。**为什么？**

\[p(y\mid x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}\]


## 这个时候求Logrange函数


\[\begin{align*}L(\theta) &=p(\overrightarrow{y}\mid X;\theta)\\&=\prod_{i=1}^{m}p(y^{(i)}\mid x^{(i)};\theta )\\&=\prod_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)} }(1-h_{\theta}(x^{(i)}))^{1-y^{(i)} }\end{align*}\]


## 然后求对数似然函数及其梯度


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/I8h7bbmLc7.png?imageslim)

那么现在已经求出了一个偏导了，就可以给定一个学习率来按照梯度进行下降。


## 将梯度带入得到Logistic回归参数的学习规则


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/hCh52Kl21h.png?imageslim)

Logistic回归，任意给定一个值，都可以得到一个全局最优值。

注意这个地方的\(h_{\theta}(x)\)是\(h_{\theta}(x)=g(\theta^T x)=\frac{1}{1+e^{-\theta^Tx} }\)，而在线性回归的时候的\(h_{\theta}(x)\) 是\(\theta^Tx\)，可见，他们的\(h_{\theta}(x)\)不同，但是整体的式子是一致的。

那么线性回归和Logistic回归肯定有某种关联性。


# Logistic回归与线性回归的关系


先定义一个概念：

一个事件的几率odds，是指该事件发生的概率与该事件不发生的概率的比值。

对数几率：对这个odds取对数。log it 函数。

对于Logistic回归来说，




  * 发生的概率是：\(P(y=1\mid x;\theta)=h_{\theta}(x)\)


  * 不发生的概率是：\(P(y=0\mid x;\theta)=1-h_{\theta}(x)\)


那么得到：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/JClCKa9Il9.png?imageslim)



我们发现，这个Logistic模型的对数几率与x是线性的，因此，这个模型**本质上是一个对数的线性模型。**


那么我们再回忆一下指数族分布：


## 那么可以根据指数族扩展到广义线性模型 GLM


事实上指数族概念的目的的提出，是为了解释广义线性模型Generalized Linear Models （GLM）

比如上面用到了高斯分布和二项分布，他们都是指数族分布，他们最终的学习的规则是一样的。也就是说： 凡是符合指数族分布的随机变量，都可以用 GLM 回归分析

所以因变量 y 可以不是正态分布，而是扩大为指数族中的任一分布。比如说想做分类，那么就是0-1记号的问题，就是两点分布的问题。

解释变量x的线性组合不再直接用于解释因变量y的均值u，而是通过一个联系函数g来解释g(u)；这里，要求g连续单调可导。**没明白？什么叫直接解释因变量y的均值u？联系函数是什么？**

如Logistic 回归中的\(g(z)=\frac{1}{1+e^{-z} }\) ，可以添加一个\(\lambda\)进行拉伸变换后：\(g(z)=\frac{1}{1+e^{-\lambda z} }\) **这个拉伸变换有什么用？**





# 使用Lagrange对偶来求 \(x^Tx\)的最小值


这个思路是凸优化里面最核心的东西。这个再SVM和HMM部分都会看到这个解法。

先推Lagrange函数得到对偶函数，在对对偶函数求极大值，就得到了x*应该是什么


## 线性方程的最小二乘问题


没明白这个怎么跟最小二乘有关系了？


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/hdDDb3DA8f.png?imageslim)




## 通过原始的Lagrange函数怎么求对偶函数：


上面的Lagrange函数的对偶函数是这样求的：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/d6ijeLc0KF.png?imageslim)

求偏导之后：**这个求偏导为0的结果x*是什么意义？**


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/9CmK0lffCi.png?imageslim)

这个值不是真的，是关于v的一个值，把这个值回带到Lagrange函数里面去。就得到了Lagrange对偶函数。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/4bFk2gHE58.png?imageslim)




## 求对偶函数的极大值




![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/1EIcJf713j.png?imageslim)

这个就是之前求线性回归的时候的最优解


## 带进去就求到了极值点

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/eEAeIDA8cE.png?imageslim)

我们会发现，这个对偶函数的极大值，等于原函数的极小值，也就是说这个满足了强对偶条件，即线性回归满足了强对偶性。**什么是强队偶？**



顺便提下，KKT条件不是强对偶性的充要条件，甚至不是充分条件。只是一个必要条件。**为什么，要了解下。**


## REF

* 七月在线 机器学习
* [第8章 预测数值型数据：回归](http://ml.apachecn.org/mlia/regress/) **这里面由关于局部加权线性回归、岭回归、Lasso 要整理进来**




# COMMENT：
