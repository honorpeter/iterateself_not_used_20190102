---
title: 岭回归 例子2：预测乐高玩具套装的价格
toc: true
date: 2018-08-03 12:12:13
---
---
author: evo
comments: true
date: 2018-05-13 00:33:00+00:00
layout: post
link: http://106.15.37.116/2018/05/13/ridge-regression-sample2/
slug: ridge-regression-sample2
title:
wordpress_id: 5632
categories:
- 随想与反思
tags:
- NOT_ADD
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


# ORIGINAL






    1. [第8章 预测数值型数据：回归](http://ml.apachecn.org/mlia/regress/)

  2.



## 需要补充的






  * **还是有很多问题的，在下面有写**

  * **要把cross_valiadation 拆分出来，太臃肿了，要只实现交叉采样的功能**




# MOTIVE






  * aaa





* * *





# 项目要求


用回归技术为乐高套装估价。


# 项目数据


书上说用 Google Shopping 的API收集数据，从返回的JSON数据中抽取价格。

但是 api 好像连不上了，因此将 lego 的几个搜索信息页面下载下来：

链接：https://pan.baidu.com/s/1gBPTnS-nxq6x1tiJDI9jNA 密码：umpx

程序将从本地加载这些 html 页面抽取数据进行分析。


# 完整代码




    import numpy as np
    import matplotlib.pylab as plt
    from time import sleep
    from bs4 import BeautifulSoup
    import json

    # from urllib import request 将会报错
    # 在Python3中将urllib2和urllib3合并为一个标准库urllib,其中的urllib2.urlopen更改为urllib.request.urlopen
    import urllib.request


​
    # 到底怎么使用google的api？
    def search_for_set_with_google_api(datas, labels, set_num, yr, num_pce, orig_prc):
        sleep(10)
        my_api_str = 'AIzaSyD2cR2KFyx12hXu6PFU-wrWot3NXvko8vY'
        dest_url = 'https://www.googleapis.com/shopping/search/v1/public/products?key=%s&country=US&q=lego+%d&alt=json' \
                   % (my_api_str, set_num)
        print(dest_url)
        page = urllib.request.urlopen(dest_url)
        info_dict = json.loads(page.read())  # 转换为json格式
        for i in range(len(info_dict['items'])):
            try:
                current_item = info_dict['items'][i]
                if current_item['product']['condition'] == 'new':
                    new_flag = 1
                else:
                    new_flag = 0
                inventory_list = current_item['product']['inventories']
                for item in inventory_list:
                    selling_price = item['price']
                    if selling_price > orig_prc * 0.5:
                        print("%d\t%d\t%d\t%f\t%f" % (yr, num_pce, new_flag, orig_prc, selling_price))
                        datas.append([yr, num_pce, new_flag, orig_prc])
                        labels.append(selling_price)
            except:
                print('problem with item %d' % i)


​
    def get_data_set(datas, labels):
        search_for_set_with_google_api(datas, labels, 8288, 2006, 800, 49.99)
        search_for_set_with_google_api(datas, labels, 10030, 2002, 3096, 269.99)
        search_for_set_with_google_api(datas, labels, 10179, 2007, 5195, 499.99)
        search_for_set_with_google_api(datas, labels, 10181, 2007, 3428, 199.99)
        search_for_set_with_google_api(datas, labels, 10189, 2008, 5922, 299.99)
        search_for_set_with_google_api(datas, labels, 10196, 2009, 3263, 249.99)


​
    # 这个解析的过程没有看
    # 从页面读取数据，生成 datas 和 labels列表
    def scrape_page(datas, labels, file_path, year, num_pce, orig_prc):
        # 打开并读取 HTML 文件
        with open(file_path, 'r', encoding='utf-8') as fr:
            info = fr.read()
            soup = BeautifulSoup(info, "lxml")
            i = 1
            # 根据HTML页面结构进行解析
            current_row = soup.findAll('table', r="%d" % i)
            while (len(current_row) != 0):
                current_row = soup.findAll('table', r="%d" % i)
                title = current_row[0].findAll('a')[1].text
                lwr_title = title.lower()
                # 查找是否有全新标签
                if (lwr_title.find('new') > -1) or (lwr_title.find('nisb') > -1):
                    new_flag = 1.0
                else:
                    new_flag = 0.0
                # 查找是否已经标志出售，我们只收集已出售的数据
                sold_unicde = current_row[0].findAll('td')[3].findAll('span')
                if len(sold_unicde) == 0:
                    print("item #%d did not sell" % i)
                else:
                    # 解析页面获取当前价格
                    sold_price = current_row[0].findAll('td')[4]
                    price_str = sold_price.text
                    price_str = price_str.replace('$', '')  # strips out $
                    price_str = price_str.replace(',', '')  # strips out ,
                    if len(sold_price) > 1:
                        price_str = price_str.replace('Free shipping', '')
                    selling_price = float(price_str)
                    # 去掉不完整的套装价格
                    if selling_price > orig_prc * 0.5:
                        print("%d\t%d\t%d\t%f\t%f" % (year, num_pce, new_flag, orig_prc, selling_price))
                        datas.append([year, num_pce, new_flag, orig_prc])
                        labels.append(selling_price)
                i += 1
                current_row = soup.findAll('table', r="%d" % i)


​
    # 依次读取六种乐高套装的数据，并生成数据矩阵
    def get_data_set_local(datas, labels):
        def get_local_path(lego_id):
            return "setHtml/lego%d.html" % lego_id

        scrape_page(datas, labels, get_local_path(8288), 2006, 800, 49.99)
        scrape_page(datas, labels, get_local_path(10030), 2002, 3096, 269.99)
        scrape_page(datas, labels, get_local_path(10179), 2007, 5195, 499.99)
        scrape_page(datas, labels, get_local_path(10181), 2007, 3428, 199.99)
        scrape_page(datas, labels, get_local_path(10189), 2008, 5922, 299.99)
        scrape_page(datas, labels, get_local_path(10196), 2009, 3263, 249.99)


​
    # 给定 lambda 下的岭回归求解。
    # lam 引入的一个λ值，使得矩阵非奇异
    def ridge_regression(data_mat_m, label_mat_m, lam=0.2):
        column_num = np.shape(data_mat_m)[1]
        data_square = data_mat_m.T * data_mat_m
        # 岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆
        denom = data_square + np.eye(column_num) * lam
        # 检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。
        if np.linalg.det(denom) == 0.0:
            print("This matrix is singular, cannot do inverse")
            return
        weight = denom.I * (data_mat_m.T * label_mat_m)
        return weight


​
    # 用于在一组 λ 上测试结果
    def try_ridge(data_mat, label_mat):
        label_mean = np.mean(label_mat, 0)
        label_mat_m = label_mat - label_mean  # Y的所有的特征减去均值

        data_mean = np.mean(data_mat, 0)  # xMat 平均值
        data_var = np.var(data_mat, 0)  # X的方差
        data_mat_m = (data_mat - data_mean) / data_var

        # 可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。
        num_test_pts = 30  # 为什么用这个名字？
        column_num = np.shape(data_mat_m)[1]
        weight_mat = np.zeros((num_test_pts, column_num))  # 30 * column_num
        for i in range(num_test_pts):
            lam = np.exp(i - 10)  # 为什么是这么计算的？
            weight = ridge_regression(data_mat_m, label_mat_m, lam)
            weight_mat[i, :] = weight.T
        # 返回所有的回归系数
        return weight_mat


​
    # residual sum of squares error
    def calc_rss_error(label_mat, label_pred_mat):
        # 这个地方如果是(label_mat- label_pred_mat)**2 则错误：
        # ValueError: input must be a square array
        # 因为 array可以平方，而matrix只有方阵时才可以平方
        return (np.square(label_mat - label_pred_mat)).sum()


​
    # 试试 np.random.sample(data_arr,int(len(data_arr)*0.9))
    # 从样本中随机选择 90% 作为训练集，剩下的作为测试集
    def get_split_data_set(data_arr, label_arr):
        row_num = len(label_arr)
        index_list = list(range(len(label_arr)))
        train_data_arr = [];
        train_label_arr = []
        test_data_arr = [];
        test_label_arr = []
        np.random.shuffle(index_list)
        for j in range(row_num):
            if j < row_num * 0.9:
                train_data_arr.append(data_arr[index_list[j]])
                train_label_arr.append(label_arr[index_list[j]])
            else:
                test_data_arr.append(data_arr[index_list[j]])
                test_label_arr.append(label_arr[index_list[j]])
        return train_data_arr, train_label_arr, test_data_arr, test_label_arr


​
    def cross_validation(data_arr, label_arr, num_val=10):
        error_mat = np.zeros((num_val, 30))  # num_val*30
        for i in range(num_val):
            train_data_arr, train_label_arr, test_data_arr, test_label_arr \
                = get_split_data_set(data_arr, label_arr)
            train_data_mat = np.mat(train_data_arr)
            train_label_mat = np.mat(train_label_arr).T
            test_data_mat = np.mat(test_data_arr)
            test_label_mat = np.mat(test_label_arr).T

            weights_mat = try_ridge(train_data_mat, train_label_mat)  # get 30 weight vectors from ridge

            train_data_mean = np.mean(train_data_mat, 0)
            train_data_var = np.var(train_data_mat, 0)
            train_label_mean = np.mean(train_label_mat)
            test_data_mat_m = (test_data_mat - train_data_mean) / train_data_var  # regularize test with training params

            # 循环遍历矩阵中的30组回归系数
            for k in range(30):  # loop over all of the ridge estimates
                # weights_mat[k, :] 是np.array格式的，如果不转换成np.mat 则无法通过.T 转化成列向量
                test_label_pred = test_data_mat_m * np.mat(weights_mat[k, :]).T \
                                  + train_label_mean  # test ridge results and store
                print(test_label_pred,test_label_mat)
                error_mat[i, k] = calc_rss_error(test_label_pred, test_label_mat)
                print(error_mat[i, k])
        # 计算误差估计值的均值
        mean_error = np.mean(error_mat, 0)  # calc avg performance of the different ridge weight vectors
        min_mean_error = float(min(mean_error))
        print('min_mean_error: ',min_mean_error)
        best_weights = weights_mat[np.nonzero(mean_error == min_mean_error)]
        # can unregularize to get model
        # when we regularized we wrote Xreg = (x-meanX)/var(x)
        # we can now write in terms of x not Xreg:  x*w/var(x) - meanX/var(x) +meanY
        data_mat = np.mat(data_arr);
        label_mat = np.mat(label_arr).T
        data_mean = np.mean(data_mat, 0);
        data_var = np.var(data_mat, 0)
        un_reg = best_weights / data_var
        print("the best model from Ridge Regression is:\n", un_reg)
        print("with constant term: ", -1 * sum(np.multiply(data_mean, un_reg)) + np.mean(label_mat))


​
​
    if __name__ == "__main__":
        datas = []
        labels = []
        # get_data_set(datas,labels) #怎么正确使用google现在的api？
        get_data_set_local(datas, labels)
        cross_validation(datas, labels, 10)


输出：


    2006	800	0	49.990000	85.000000
    2006	800	0	49.990000	102.500000
    2006	800	0	49.990000	77.000000
    item #4 did not sell
    2006	800	0	49.990000	162.500000
    ...略去
    2009	3263	1	249.990000	499.990000
    item #10 did not sell
    2009	3263	0	249.990000	399.950000
    item #12 did not sell
    2009	3263	1	249.990000	331.510000
    212860.71385
    208610.055141
    204750.720059
    223403.16587
    ...略去
    949504.518154
    949505.161571
    949505.398275
    949505.485355
    min_mean_error:  357870.7137990535
    the best model from Ridge Regression is:
     [[ -1.82448488e+01  -4.57134255e-03  -4.02055207e+01   2.34437884e+00]]
    with constant term:  [[ 37295.56909677    704.76813889    705.18128796    -90.45669738]]


还是有很多不清楚的地方的：


## beautiful soup 的使用还是要好好总结下的


**需总结**


## 上面对于html文件的解析没有看


**要看下**

注意读取html文件的时候的 encoding='utf-8' 。**是不是用utf-8就一定对？**


## 爬虫的时候怎么翻墙？


可见：[python 翻墙](http://106.15.37.116/2018/05/13/python-cross-wall/) **不过还需要补充**


## google 的 api 要怎么用？


**而且，不仅google的，baidu的，携程的等等都要总结下。**


## array 是不能.T的，一定要线转化成 np.mat


**要总结到 numpy的array与mat 的文章里面。**


## urllib 的使用还是要总结下的


**需总结。**


## 这个输出的rss_error好像很大呀，这个是正常结果吗？


**未确认**


## 对于ridge_regression 的理解还是不够，理论结合这里的代码再看下


**需补充**













* * *





# COMMENT
