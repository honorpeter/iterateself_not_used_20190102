---
title: 线性回归 例子2：根据鲍鱼壳的层数推算鲍鱼年龄
toc: true
date: 2018-08-03 12:22:26
---
---
author: evo
comments: true
date: 2018-05-12 14:14:00+00:00
layout: post
link: http://106.15.37.116/2018/05/12/line-regression-sample2/
slug: line-regression-sample2
title:
wordpress_id: 5607
categories:
- 随想与反思
---

<!-- more -->

[mathjax]

**注：非原创，推荐直接看原文**


## 相关资料






  1. [第8章 预测数值型数据：回归](http://ml.apachecn.org/mlia/regress/)




## 需要补充的






  * **没明白这些输出能说明什么？**

  * **lwlr的东西要不要拆出来单独？因为岭回归和前向逐步回归都拆出来了**




# MOTIVE






  * aaa





* * *






# 项目要求


我们有一份来自 UCI 的数据集合的数据，记录了鲍鱼（一种介壳类水生动物）的年龄。鲍鱼年龄可以从鲍鱼壳的层数推算得到。


# 项目数据


链接：https://pan.baidu.com/s/1RbWaNrasuBSOcSiussLbWg 密码：yn77


数据存储格式：


    1   0.455   0.365   0.095   0.514   0.2245  0.101   0.15    15
    1   0.35    0.265   0.09    0.2255  0.0995  0.0485  0.07    7
    -1  0.53    0.42    0.135   0.677   0.2565  0.1415  0.21    9
    1   0.44    0.365   0.125   0.516   0.2155  0.114   0.155   10
    0   0.33    0.255   0.08    0.205   0.0895  0.0395  0.055   7




# 完整代码




    import numpy as np
    import matplotlib.pylab as plt


​
    # 加载数据集
    def load_data_set(file_name):
        feature_num = len(open(file_name).readline().split('\t')) - 1
        data_arr = []
        label_arr = []
        fr = open(file_name)
        for line in fr.readlines():
            line_arr = []
            cur_line = line.strip().split('\t')
            # 特征值
            for i in range(feature_num):
                line_arr.append(float(cur_line[i]))
            data_arr.append(line_arr)
            # 标签
            label_arr.append(float(cur_line[-1]))
        data_mat = np.mat(data_arr)
        label_mat = np.mat(label_arr).T
        return data_mat, label_mat


​
    # 还是不是很明白，要配合理论看一下
    # 局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。
    # 关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关
    def lwlr_pred_label(data, data_mat, label_mat, k=1.0):
        # 获得xMat矩阵的行数
        row_num = np.shape(data_mat)[0]
        # 创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重
        # eye()：对角线为1，其他为0
        weight_mat = np.mat(np.eye((row_num)))
        for i in range(row_num):
            # 计算 data 与每个样本点之间的距离
            diff_mat = data - data_mat[i, :]
            # 计算出每个样本贡献误差的权值，k控制衰减的速度
            weight_mat[i, i] = np.exp(diff_mat * diff_mat.T / (-2.0 * k ** 2))
        # 根据矩阵乘法计算 xTx ，weights 是样本点对应的权重矩阵
        data_square = data_mat.T * (weight_mat * data_mat)
        if np.linalg.det(data_square) == 0.0:
            print("This matrix is singular, cannot do inverse")
            return
        # 计算出回归系数的一个估计
        weights = data_square.I * (data_mat.T * (weight_mat * label_mat))
        # 得到预测点
        label_pred = data * weights
        return label_pred


​
    # k：控制核函数的衰减速率 要怎么确定？
    # 测试局部加权线性回归，对数据集中每个点调用 lwlr() 函数
    def lwlr(data_mat_for_pred, data_mat_for_train, label_mat, k=1.0):
        row_num = np.shape(data_mat_for_pred)[0]
        label_pred_mat = np.zeros((row_num, 1))  # m*1
        for i in range(row_num):
            label_pred_mat.itemset(i, lwlr_pred_label(data_mat_for_pred[i], data_mat_for_train, label_mat, k))
        return label_pred_mat


​
    # residual sum of squares error
    def rss_error(label_mat, label_pred_mat):
        # 这个地方如果是(label_mat- label_pred_mat)**2 则错误：
        # ValueError: input must be a square array
        # 因为 array可以平方，而matrix只有方阵时才可以平方
        return (np.square(label_mat - label_pred_mat)).sum()


​
    # 标准线性回归
    def standard_lr(data_mat, label_mat):
        data_square = data_mat.T * data_mat
        # 因为要用到xTx的逆矩阵，因此先确定xTx是否可逆
        # 可逆的条件是矩阵的行列式不为0，如果为0，即不可逆，则无法继续进行计算 为什么？
        # linalg.det()可以求得矩阵的行列式
        if np.linalg.det(data_square) == 0.0:
            print("This matrix is singular, cannot do inverse")
            return
        # 根据书中的公式，求得w的最优解
        weights = data_square.I * (data_mat.T * label_mat)
        label_pred_mat = data_mat * weights
        return label_pred_mat, weights


​
    if __name__ == "__main__":
        # 加载数据
        data_mat, label_mat = load_data_set("../../../input/8.Regression/abalone.txt")
        # 训练集与预测集是同一个数据集
        label_pred_mat_01 = lwlr(data_mat[0:99], data_mat[0:99], label_mat[0:99], 0.1)
        label_pred_mat_1 = lwlr(data_mat[0:99], data_mat[0:99], label_mat[0:99], 1)
        label_pred_mat_10 = lwlr(data_mat[0:99], data_mat[0:99], label_mat[0:99], 10)
        print("lwlr same pred train dataset :")

        print("0.1: ", rss_error(label_mat[0:99], label_pred_mat_01),
              "1: ", rss_error(label_mat[0:99], label_pred_mat_1),
              "10: ", rss_error(label_mat[0:99], label_pred_mat_10))

        # 训练集与预测集是不同的数据集
        label_pred_mat_01 = lwlr(data_mat[100:199],data_mat[0:99], label_mat[0:99], 0.1)
        label_pred_mat_1 = lwlr(data_mat[100:199],data_mat[0:99], label_mat[0:99], 1)
        label_pred_mat_10 = lwlr(data_mat[100:199],data_mat[0:99], label_mat[0:99], 10)
        print("lwlr different pred train dataset :")
        print("0.1: ", rss_error(label_mat[100:199], label_pred_mat_01),
              "1: ", rss_error(label_mat[100:199], label_pred_mat_1),
              "10: ", rss_error(label_mat[100:199], label_pred_mat_10))

        # 使用简单的 线性回归 进行预测，与上面的计算进行比较
        label_pred_mat, weights = standard_lr(data_mat[0:99], label_mat[0:99])
        label_pred_mat_d = data_mat[100:199] * weights
        print("std lr :")
        print(rss_error(label_mat[100:199], label_pred_mat_d))


输出：


    lwlr same pred train dataset :
    0.1:  56.7842091184 1:  429.89056187 10:  549.118170883
    lwlr different pred train dataset :
    0.1:  25119.4591112 1:  573.52614419 10:  517.571190538
    std lr :
    518.636315325


**没明白这些输出能说明什么？为什么different的时候，从0.1到10 是从大到小的？为什么0.1的时候这么大？程序错了吗？**

根据我们上边的测试，可以看出:

简单线性回归达到了与局部加权现行回归类似的效果。这也说明了一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是 10 吗？或许是，但如果想得到更好的效果，应该用 10 个不同的样本集做 10 次测试来比较结果。













* * *





# COMMENT
