---
title: 02 划分选择
toc: true
date: 2018-07-31 20:11:34
---
# 划分选择

TODO

- **还是理解的不够，公式的推导，和缘由不知道。**



## 我们想要的是什么结果

由之前决策树的生成过程可以知道，决策树生成的关键就是如何选择最优划分属性。<span style="color:red;">是的这个很关键。</span>

首先，要判断什么样的划分是最优的，首先我们要看下我们想要的是什么结果。

一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的 "纯度" (purity) 越来越高。<span style="color:red;">是的，但是仅仅这样还是不够的吧？</span>

<span style="color:red;">从纯度讲到下面的衡量标准太突兀了吧？</span>

## 衡量划分好坏的几种标准

事实上，我们主要有三种方式来衡量划分的优劣：

- 信息增益       对应 ID3 决策树学习算法
- 信息增益率     对应 C4.5 决策树算法
- 基尼系数       对应 CART 决策树


### 信息增益

#### 没有划分的时候，样本集的信息熵

说道信息增益，首先，我们要知道什么是信熵。

"信息熵" (information entropy) 是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$  中第 $k$  类样本所占的比例为  $p_k(k=1,2,\cdots ,|\mathcal{Y}|)$ ，则 $D$ 的信息熵定义为


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/175GfAEKK3.png?imageslim)

<span style="color:red;">嗯，这个式子是怎么来的呢？</span>

$Ent(D)$ 的最小值为 $0$ ，最大值为 $log_2|\mathcal{Y}|$。$Ent(D)$ 的值越小，则 $D$ 的纯度越高。比如，只有一个类别的时候，$Ent(D)$ 就是 0。

注意：在计算信息熵的时候，我们约定：若 $p=0$，则 $plog_2p=0$。

上面我们就介绍了一个样本集合的信息熵。OK，要注意，这是一个没有进行分类的集合的信息熵。

#### 按照某个属性进行划分后，样本集的信息熵。

现在，我们看一下这个样本集的某个属性的属性值。

我们假设，离散属性 $\alpha$ 有 $V$ 个可能的取值 $\{\alpha^1,\alpha^2,\cdots ,\alpha^V\}$，如果我们使用属性 $\alpha$ 来对整个样本集 $D$ 进行划分，就会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $\alpha$ 上取值为 $\alpha^v$ 的样本，记为 $D^v$ 。

OK，也就是说，经过对属性 $\alpha$ 进行划分，我们得到了 $V$ 个分支，每个分支的样本数为 $D^v$ 。

我们把其中的一个分支 $v$ 拿出来看，它包含的样本数量为 $D^v$ ，我们但看这个样本集，它其实也可以计算出一个信息熵的 $Ent(D^v)$ 。

OK，那么我们就可以算出这个划分后的总的样本集的信息熵：

$$\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)$$

其中，我们给每个分支样本集的信息熵赋予权重 $\frac{|D^v|}{|D|}$。
<span style="color:red;">嗯，不知道这个权重有什么理论依据吗？虽然看起来好像是 OK 的。</span>

#### 这种划分引起的信息增益

OK，我们得到了划分后的信息熵，那么与没有划分时候的信息熵进行比较久得到了对这个属性进行划分所引起的熵的变化：

$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)$$

这个就是所谓的信息增益。

一般而言，信息增益越大，则意味着使用属性 $\alpha$  来进行划分所获得的"纯度提升"越大。是的，因为信息增益越大，说明划分后的信息熵越小，即说明样本集越纯。最小为0。

OK，因此，我们可用信息增益来进行决策树的划分属性选择，即，当我们想要对这个样本集进行划分时，我们选择什么属性进行划分呢？我们选择能够使信息增益最大的属性：

$$a_*= \underset{a\in A}{arg\,max}\;Gain(D,α)$$

著名的 ID3 决策树学习算法就是以信息增益最大为准则来选择要划分的属性的。

OK，到这里，我们对以信息增益为划分的形式还是比较熟悉了，下面我们介绍一个例子，看看实际划分的时候的具体计算过程。

#### 一个例子

OK，我们来根据上面的样本举个例子：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/Liia4JEC7j.png?imageslim)

以上表中的西瓜数据集为例：该数据集包含了 17 个训练样本，用来学习一棵西瓜决策树。显然，$|\mathcal{Y}|=2$。

在决策树学习开始时，根结点包含 $D$ 中的所有样例，其中正例占 $p_1 =\frac{8}{17}$，反例占的 $p_2 =\frac{9}{17}$。

于是，计算出根结点的信息熵为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/mcjageJJBJ.png?imageslim)


然后，我们要计算出当前属性集合  {色泽，根蒂，敲声，纹理，脐部，触感}  中每个属性的信息增益。

以属性"色泽"为例：它有 3 个可能的取值:  {青绿，乌黑，浅自}，如果使用该属性对 $D$ 进行划分，则可得到 3个子集，分别记为:

- $D^1$ (色泽=青绿)，
- $D^2$ (色泽=乌黑)，
- $D^3$ (色泽=浅白)。

分析每个子集：


- 子集 $D^1$ 包含编号为 { 1, 4, 6, 10, 13, 17 } 的 6 个样例，其中正例占 $p_1 =\frac{3}{6}$ ，反例占 $p_2 =\frac{3}{6}$;
- 子集 $D^2$ 包含编号为 { 2, 3, 7, 8, 9, 15 } 的 6 个样例，其中正、反例分别占 $p_1 =\frac{4}{6}$， $p_1 =\frac{2}{6}$
- 子集 $D^3$ 包含编号为 { 5, 11, 12, 14, 16 } 的 5 个样例，其中正、反例分别占 $p_1 =\frac{1}{5}$，$p_1 =\frac{4}{5}$


可以计算出用 "色泽" 划分之后所获得的 3 个分支结点的信息熵为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/4K7Bfe11gl.png?imageslim)


因此，可以计算出属性 "色泽" 的信息增益为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/738ehgmgLe.png?imageslim)


类似的，我们可计算出其他属性的信息增益:

- Gain(D，根蒂) = 0.143
- Gain(D，敲声) = 0.141
- Gain(D，纹理) = 0.381
- Gain(D，脐部) = 0.289
- Gain(D，触感) = 0.006

显然，属性 "纹理" 的信息增益最大，于是它被选为划分属性。下图给出了基于 "纹理" 对根结点进行划分的结果，各分支结点所包含的样例子集显示在结点中.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/BeF6FHbm3G.png?imageslim)


OK，现在，第一步的划分已经结束了，我们使用的特是纹理。接下来，我们对每个分支结点做进一步划分。

以上图第一个分支结点 ( "纹理=清晰" ) 为例，该结点包含的样例集合 $D^1$ 中有编号为 {1 ,2, 3, 4, 5, 6, 8, 10, 15} 的 9 个样例，可用属性集合为{色泽，根蒂，敲声，脐部，触感}。那么基于 $D^1$ 我们可以计算出各属性的信息增益：

- Gain($D^1$ ，色泽) = 0.043;
- Gain($D^1$ ，根蒂) = 0.458;
- Gain($D^1$ ，敲声) = 0.331;
- Gain($D^1$ ，脐部) = 0.458;
- Gain($D^1$ ，触感) = 0.458.

可见，"根蒂"、 "脐部"、 "触感" 3 个属性均取得了最大的信息增益，这里我们可以任选其中之一作为划分属性。

类似的，我们对每个分支结点都进行上述操作，最终得到的决策树如图所示：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/k0cg9dm8cm.png?imageslim)

<span style="color:red;">嗯，还是很清晰的。这种原理配例子的讲解方式还是很清楚的。要推广。</span>

到这里，关于以信息增益为划分原则的方式已经介绍完了。还是很清晰的。

### 增益率

#### 信息增益对可取数目较多的属性是有偏好的

看起来，上面的以信息增益进行划分的方式是没有问题的，但是实际上，还是存在一个严重的问题的：

如果，有一个属性，每个样本值在这个属性上的属性值都不同，那么会发生什么？

我们可以很容易计算出这个属性对应的信息增益，由于每个属性值都不同，因此如果按照这个属性来划分，那么每个节点只有一个样本，这个节点的信息熵就是 0，也就是说，这种划分的信息增益就是 0.998-0=0.998 。

我们很容易就想到了这种情况造成的后果，由于这个信息增益肯定是大于以别的属性为划分的信息增益的，所以，我们肯定会选择这个属性进行划分，但是，一旦按照这个属性进行划分，整个划分过程也就完结了，因为每个节点只有一个样本了。

很容易感觉到，这样的划分肯定是不具有泛化能力的，是无法对新样本进行有效预测的。

这就是按照信息增益进行划分的时候一个潜在的坑。

实际上，信息增益准则对可取值数目较多的属性是有所偏好的，<span style="color:red;">的确</span>，为了减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法不直接使用信息增益，而是使用 "增益率" (gain ratio) 来选择最优划分属性。

#### 为了减少这种偏好带来的影响，使用增益率

我们定义增益率为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/J5gbLGf3Fa.png?imageslim)

其中：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/943Bk7e7K9.png?imageslim)

称为属性 $a$ 的 "固有值" (intrinsic value) 。属性 $a$ 的可能取值数目越多(即 $V$ 越大)，则 $IV(a)$ 的值通常会越大。<span style="color:red;">这个固有值是怎么定的？为什么这么计算？</span>

比如，对于西瓜数据集 2.0，有

* IV(触感) = 0.874 (V = 2)
* IV(色泽) = 1.580 (V = 3)
* IV(编号) = 4.088 (V = 17)

但是，同样需要注意的是：增益率准则对可取值数目较少的属性是有所偏好的。<span style="color:red;">好吧，这个之前没有注意到。</span>

因此 ， C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。<span style="color:red;">好吧，有点精，怪不得 C4.5 这么强，整合了信息增益与增益率两者的优点。嗯，nice。不过启发式是什么意思？</span>

<span style="color:red;">我之前还以为 C4.5 就是直接对应的增益率，没想到是对信息增益和增益率的整合，做得好。</span>

### 基尼指数

<span style="color:red;">说实话，我看到基尼指数的时候，感觉这个肯定是比增益率更高级的，但是好像并不是这样。之前好像在哪里看到说还是推荐 C4.5 需要确认下。或者也许某种情景下基尼指数更好。</span>

<span style="color:red;">还是没怎么理解。</span>

CART 决策树使用 "基尼指数" (Gini index) 来选择划分属性。采用与信息熵相同的一些符号，数据集 $D$ 的纯度可用基尼值来度量：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/KiGDAH12Bd.png?imageslim)


直观来说， $Gini(D)$ 反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此， $Gini(D)$ 越小，则数据集的纯度越高。<span style="color:red;">嗯，再理解下，对这个式子还是不够理解。</span>

属性 $a$ 的基尼指数定义为：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/k14lLl82h0.png?imageslim)


于是，我们在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/24BcI5dlbE.png?imageslim)


<span style="color:red;">奇怪，对于这个基尼系数好型没有怎么讲。</span>


# REF

1. 《机器学习》周志华
