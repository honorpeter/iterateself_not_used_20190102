---
title: 04 软间隔与正则化
toc: true
date: 2018-08-12 20:06:01
---
# 软间隔与正则化

TODO

- <span style="color:red;">这一章基本没有看懂，要找个好懂的仔细补充道这里。</span>


在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。

然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；退一步说, 即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。<span style="color:red;">好吧，这也可以，嗯，考虑周到</span>

缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入 “软间隔” (soft margin) 的概念，如图6.4所示.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/0EDHkdih8m.png?imageslim)


具体来说，前面介绍的支持向量机形式是要求所有样本均满足约束(6.3)， 即所有样本都必须划分正确，这称为“硬间隔” (hard margin)，而软间隔则是允许某些样本不满足约束：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/ca4CDklH1d.png?imageslim)


当然，在最大化间隔的同时，不满足约束的样本应尽可能少。于是，优化目标可写为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/922AH89AH0.png?imageslim)

其中 $C>0$ 是一个常数，$\ell_{0/1}$ 是 “0/1损失函数”

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/l7Ai2cKHCF.png?imageslim)


显然，当 $C$为无穷大时，式(6.29)迫使所有样本均满足約束(6.28)，于是 式(6.29)等价于(6.6)；当 $C$ 取有限值时，式(6.29)允许一些样本不满足約束。

然而，$\ell_{0/1}$ 非凸、非连续，数学性质不太好，使得式(6.29)不易直接求解。于是，人们通常用其他一些函数来代替 $\ell_{0/1}$ ，称为 “替代损失”(surrogate loss)。

替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数且是 $\ell_{0/1}$  的上界。图6.5给出了三种常用的替代损失函数：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/kJFBF7diF4.png?imageslim)

若采用 hinge 损失，则式(6.29)变成：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/9I54iDAaE1.png?imageslim)

引入“松弛变量”(slack variables) $\xi_i\geqslant 0$ ，可将式(6.34)重写为：<span style="color:red;">什么是松弛变量？为什么可以这么写？没看懂</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/Ac0bkKK16f.png?imageslim)
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/9kakjlELj1.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/88iE6e3l66.png?imageslim)

这就是常用的 “软间隔支持向量机”.

显然，式(6.35)中每个样本都有一个对应的松弛变量，用以表征该样本不满足约束(6.28)的程度。但是，与式(6.6)相似，这仍是一个二次规划问题。于是，类 似式(6.8)，通过拉格朗日乘子法可得到式(6.35)的拉格朗日函数

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/gh9fBc7E8h.png?imageslim)

其中 $\alpha_i\geqslant 0$ ，$\mu_i\geqslant 0$ 是拉格朗日乘子。

令 $L(w, b, \alpha,\xi,\mu)$ 对 $w$，$b$，$\xi_i$ 的偏导为零可得：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/ClKF898Lig.png?imageslim)

将式(6.37)-(6.39)代入式(6.36)即可得到式(6.35)的对偶问题

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/G6hbFlibkh.png?imageslim)

将式(6.40)与硬间隔下的对偶问题(6.11)对比可看出，两者唯一的差别就在于对偶变量的约束不同：前者是 $0\leqslant \alpha_i\leqslant C$ ，后者是 $0\leqslant \alpha_i$ 。于是，可采用 6.2 节中同样的算法求解式(6.40)；在引入核函数后能得到与式(6.24)同样的支持向量展式。

类似式(6.13)，对软间隔支持向量机，KKT 条件要求

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/hh1HIDbIfJ.png?imageslim)

于是，对任意训练样本 $(x_i,y_i)$ ，总有 $\alpha_i=0$ 或 $y_if(x_i)=1-\xi_i$ ，若 $\alpha_i=0$ ，则该样本不会对 $f(x)$ 有任何影响；若 $\alpha_i>0$ ，则必有 $y_if(x_i)=1-\xi_i$，即该样本是支持向量：由式(6.39)可知，若 $\alpha_i<C$ ，则 $\mu_i>0$ ,进而有 $\xi_i=0$ ，即该样本恰在最大间隔边界上；若 $\alpha_i=C$ ，则有 $\mu_i = 0$ ，此时若 $\xi_i\leq 1$ 则该样本落在最大间隔内部，若 $\xi_i>1$ 则该样本被错误分类.由此可看出，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用 hinge 损失函数仍保持了稀疏性.

那么，能否对式(6.29)使用其他的替代损失函数呢？

可以发现，如果使用对率损失函数来替代式(6.29)中的 $0/1$ 损失函数， 则几乎就得到了对率回归模型 (3.27) <span style="color:red;">对率回归模型是什么？</span>。实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需进行特殊处理[Platt，2000]；此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广[psu and Lin， 2002]，另一方面，从图6.5可看出，hinge损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。

我们还可以把式(6.29)中的 $0/1$ 损失函数换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项 $\sum_{i=1}^{m}\ell (f(x_i),y_i)$ 用来表述训练集上的误差，可写为更一般的形式

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180627/CiA0c7i184.png?imageslim)


其中 $\Omega(f)$ 称为“结构风险” (structural risk)，用于描述模型 $f$ 的某些性质；第二项 $\sum_{i=1}^{m}\ell (f(x_i),y_i)$ 称为“经验风险”(empirical risk)，用于描述模型与训练数据的契合程度； $C$ 用于对二者进行折中。

从经验风险最小化的角度来看，$\Omega(f)$ 表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型) 这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险.从这个角度来说，式(6.42)称为“正则化”(regularization)问题， $\Omega(f)$ 称为正则化项，$C$ 则称为正则化常数. $L_p$ 范数(norm)是常用的正则化项，其中 $L_2$ 范数 $||w||_2$ 倾向于 $w$ 的分量取值尽量均衡，即非零分量个数尽量稠密，而 $L_0$ 范数 $||w||_0$ 和 $L_1$ 范数 $||w||_1$ 则倾向于 $w$ 的分量尽量稀疏，即非零分量个数尽量少。




## 相关资料

1. 《机器学习》周志华
