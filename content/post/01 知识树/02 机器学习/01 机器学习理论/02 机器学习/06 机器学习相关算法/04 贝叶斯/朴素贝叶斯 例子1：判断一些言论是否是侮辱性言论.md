---
title: 朴素贝叶斯 例子1：判断一些言论是否是侮辱性言论
toc: true
date: 2018-08-12 17:55:13
---

# 朴素贝叶斯 例子1：判断一些言论是否是侮辱性言论



## 项目使用的场景

想要构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。


# 项目要求


要做到：能够根据一些已知的侮辱性言论和非侮辱性言论来判断一些新的言论是不是侮辱性言论。

本题中，侮辱类和非侮辱类，分别使用 1 和 0 分别表示。


# 项目数据


只在代码中生成了几条数据。


# 完整代码




    import numpy as np


​
    # 创建数据集
    # 单词列表postingList, 所属类别classVec
    def create_data_set():
        data = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
        label = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not
        return data, label


​
    # 根据样本中的词汇创建词汇表
    def create_vocabulary_list(data_set):
        vocabulary_set = set([])  # create empty set
        for document in data_set:
            vocabulary_set = vocabulary_set | set(document)  # union of the two sets
        return list(vocabulary_set)


​
    # 将文本用矢量表示
    def convert_doc_to_vec(vocabulary_list, doc):
        vec = [0] * len(vocabulary_list)
        for word in doc:
            if word in vocabulary_list:
                vec[vocabulary_list.index(word)] = 1
            else:
                vec[vocabulary_list.index(word)] = 0
        return vec


​
    # 计算出概率值
    def calculate_probility(vec_data, label):
        doc_num = len(vec_data)
        word_num = len(vec_data[0])
        p_1 = sum(label) / float(doc_num)  # 正样本出现概率，sum(label)即其中所有的 1 的个数，
        vec_0 = np.ones(word_num)  # 构造单词出现次数列表 之所以不是zeros 是因为为了应对有生词出现的情况
        vec_1 = np.ones(word_num)
        word_num_0 = 2.0  # 整个数据集单词出现总数   为什么不是0？
        word_num_1 = 2.0

        for i in range(doc_num):
            if label[i] == 1:
                vec_1 += vec_data[i]  # 如果是正样本，对正样本的向量进行加和
                word_num_1 += sum(vec_data[i])  # 计算所有正样本中出现的单词总数   为什么要算这个？为了后面normalize吗？
            else:
                vec_0 += vec_data[i]
                word_num_0 += sum(vec_data[i])

        p_vec_0 = np.log(vec_0 / word_num_0)
        p_vec_1 = np.log(vec_1 / word_num_1)  # 即在正样本类别下，每个单词出现的概率   为什么使用log()？
        return p_vec_0, p_vec_1, p_1


​
    # 这个地方还有些不清楚？
    def classify_NB(test_vec, p_vec_not_abusive, p_vec_abusive, p_abusive):
        p1 = sum(test_vec * p_vec_abusive) + np.log(p_abusive)  # element-wise mult
        p0 = sum(test_vec * p_vec_not_abusive) + np.log(1.0 - p_abusive)
        if p1 > p0:
            return 1
        else:
            return 0


​
    if __name__ == "__main__":
        # 创建数据集
        data, label = create_data_set()  # 数据集
        # 根据数据集创建词汇列表
        vocabulary_list = create_vocabulary_list(data)  # 词汇表
        # 将数据集转化为矢量形式表示
        vec_data = []
        for doc in data:
            vec_data.append(convert_doc_to_vec(vocabulary_list, doc))
        # 得到侮辱词汇的概率矢量 和非侮辱词汇的概率矢量 和侮辱的文档的概率
        p_vec_not_abusive, p_vec_abusive, p_abusive = calculate_probility(np.array(vec_data), np.array(label))

        # OK，使用两组数据进行测试
        test_data = ['love', 'my', 'dalmation']
        test_vec = np.array(convert_doc_to_vec(vocabulary_list, test_data))
        print(test_data, 'classified as: ', classify_NB(test_vec, p_vec_not_abusive, p_vec_abusive, p_abusive))
        test_data = ['stupid', 'garbage']
        test_vec = np.array(convert_doc_to_vec(vocabulary_list, test_data))
        print(test_data, 'classified as: ', classify_NB(test_vec, p_vec_not_abusive, p_vec_abusive, p_abusive))


输出如下：


    ['love', 'my', 'dalmation'] classified as:  0
    ['stupid', 'garbage'] classified as:  1


实际上，上面的代码中还是有几个地方需要说明一下的：


## 1.为什么 vec_0 = np.ones(word_num) 而不是zeros？word_num_0 为什么等于 2.0 而不是0？


解答：在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。**没有很明白？为什么要计算p(w0|1) * p(w1|1) * p(w2|1)？**


## 2. 这句 p_vec_0 = np.log(vec_0 / word_num_0) 为什么加了np.log？


解答：这样写的原因是因为：如果不这样写会可能有下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。

下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/kh24fdjdfG.png?imageslim)




## 3.classify_NB 中的 p1 = sum(test_vec * p_vec_abusive) + np.log(p_abusive) 为什么这么算？


解答：classify_NB这个地方，我们把乘法转换为加法 ：




  * 乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)

  * 加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))


而且，由于p_vec_not_abusive 已经是np.log 了，也就是说：


  * p_vec_not_abusive：[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]

  * p_vec_abusive：[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]


**还是没理解 sum(test_vec * p_vec_abusive) 这个算的是什么？**







## REF

- [第4章 基于概率论的分类方法：朴素贝叶斯](http://ml.apachecn.org/mlia/)
