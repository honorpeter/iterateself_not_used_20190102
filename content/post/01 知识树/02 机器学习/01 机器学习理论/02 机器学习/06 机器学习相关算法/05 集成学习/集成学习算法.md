---
title: 集成学习算法
toc: true
date: 2018-08-12 20:14:26
---
# 集成学习算法

TODO

* **对集成学习算法进行总结，尤其是Xgboost Adaboost 随机森林。**
* **随机森林能不能与HMM融合在一起？**



# 什么是集成学习算法？

集成学习算法（ensemble learning），集成学习算法一般来说是必用的。**是这样吗？**






# 有那些集成学习算法？

集成学习算法主要分为两种：

* Bagging   最流行的是随机森林（Random Forest）是Bagging方法的典型
* Boosting  当前最流行的版本是AdaBoost，人脸识别里面最经典的那个就是使用的AdaBoost。还有GBDT（也就是Gradient Boosting）

OK，我们以选男友为例：

* Bagging ：美女选择择偶对象的时候，会先问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友
* Boosting ：3个帅哥追同一个美女，第1个帅哥失败->(传授经验：姓名、家庭情况) 第2个帅哥失败->(传授经验：兴趣爱好、性格特点) 第3个帅哥成功

也就是说：

Boosting 是把一些表现效果一般（可能仅仅优于随机猜测）的模型通过特定方法进行组合来获得一个表现效果较好的模型。从抽象的角度来看，Boosting算法是借助 convex loss function 在函数空间进行梯度下降的一类算法。**没明白，什么是借助convec loss function 在函数空间进行梯度下降？**


# bagging 和 boosting 区别是什么？


1. bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。
2. bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。
3. bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。


# Bagging的思路：


实际上Bagging的思路可以用在别的地方，比如说，不同类型的分类器的组合

比如说我用随机重复采样的2/3的样本构造100个SVM，然后用这100个分类器去投票，但是实际上这个并不一定能得到很好的结果，为什么呢？因为随机森林有个优势：树之间的相关度会没有那么相关。而如果用SVM来做，那么相关度就比较强，即使你用的是2/3的随机样本。**为什么随机森林的树之间没有那么相关？。。说 树会很好的overfit我的数据，而我的数据每次都是不一样的，所以踩没有那么相关，没想到overfit也是一个优点。再确认下。**

但是，如果你用集成算法集成不同种类的分类器，那么不同的分类器之间相关度就会比较弱，比如一个LR和一个SVM和一个naive bayes。三个方法来投票，效果就会很好



# 集成学习算法思路


## 举个例子


假如有T个朋友，每个人拥有一个预测A股的函数 gi(x)，如何更好的预测股价升降？


* validation：选择一个炒股成绩最好的gi(x)
* uniformly：平均每个人预测结果，投票的方式
* non-uniformly：加权求和每个人的结果，某些人的权重更大
* canditionally：有条件的加权求和每个的结果，满足条件下某些人的权重更大。


**什么叫有条件的加权求和每个的结果？**

## 分析一下不同的方式


存在函数 g1,g2,...,gT，每个gi(x)函数表示股价升降预测

validation：选择一个炒股成绩最好的gi(x)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/mH9jfc69Db.png?imageslim)

uniformly：平均每个人预测结果，投票的方式


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/F4f5k8kmF5.png?imageslim)

non-uniformly：加权求和每个人的结果，某些人的权重更大


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/CjAbGJ0j51.png?imageslim)

canditionally：有条件的加权求和每个的结果，满足条件下某些人的权重更大


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6LeD8I8Bbk.png?imageslim)

最后一种是很灵活的，因为模型变得很复杂，因此用的非常很少。主要的就是 uniformly 和 non-uniformly 这两种，今天讨论的主要就是这两种方法。


## uniformly 的，比如 Uniform blending


**到底 uniformly 与 Uniform blending 是什么关系？属于其中的一种？**

利用函数的多样化，实现G(x)比单个g(x)好

分类应用   ![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Ali9lKkb19.png?imageslim)

回归应用  ![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/37c1AA2miG.png?imageslim)

这种直接的线性，虽然不一定能提高准确度，但是能够提升稳定性，降低 variance。即在训练集上是什么样的成绩在验证机上也是什么样的成绩。


### non-uniformly 的，比如 Linear blending


**到底 non-uniformly 与 Linear blending 是什么关系？属于其中的一种？**

选择一组权重，满足：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/BkcgL37mf1.png?imageslim)

回归问题，等价于新的特征下的线性回归，唯一的约束条件是alpha非负。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6C0GF1g8B3.png?imageslim)

验证集，训练集


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1l6BhBKji5.png?imageslim)

怎么就等于新的特征下的线性回归了？

这个\(\alpha\)既可以在验证集上选，也可以在训练集混合验证集上选。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/LlffKlmKc9.png?imageslim)这里没有明白？





## 怎么构造多样性的 g1,g2,...gT


前面讲的是 g1,g2,...gT 都是固定的，都是知道参数的，但是实际应用中实际上不能提前知道 g1,g2,...gT 的，因此一般都是人工自己构造出来：




  * 基于模型


    * 不同的算法。比如 SVM一个，线性回归一个，Naive Bayes一个


    * 不同的参数。超参不同，比如线性回归的惩罚系数，SVM的c





  * 基于数据


    * 随机采样：比如有放回的boostrap的采样。


    * 设置权重，改变样本的概率分布，Adaboost会用的。


    * 随机选择特征，随机森林的思路。





因此，即使算法只有一个，也能构造出多样性的一组 g 。

注意：这种用来混合的 g 一定要有多样性。






# COMMENT：


**需要修正。**



# REF

* 七月在线 机器学习
