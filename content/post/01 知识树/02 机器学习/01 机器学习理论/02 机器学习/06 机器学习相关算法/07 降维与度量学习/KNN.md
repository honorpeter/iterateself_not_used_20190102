---
title: KNN
toc: true
date: 2018-08-12 19:46:17
---

TODO

* **感觉讲的不是很系统，重新总结下，找一些权威的资料。**
* **KNN到底跟聚类有没有关系？一般用来做什么？降维？**
* **KNN到底属于什么聚类？密度聚类？**
* **回归问题中的KNN算法是什么样子的？看了这两个例子感觉都是分类问题**
* **KNN算法有什么变种吗？**
* **决策的时候除了多数表决的方法还有什么方法吗？**





# KNN 是什么？


k-近邻（kNN, k-NearestNeighbor）算法其实是一种分类算法，我们这里只讨论分类问题中的 k-近邻算法。其实它也可以用在回归问题里面。**回归问题中的KNN算法是什么样子的？**

k 近邻算法的：

* 输入为实例的特征向量，对应于特征空间的点；
* 输出为实例的类别，可以取多类。


它的分类过程是这样的：假设我们已经给定一个训练数据集，其中的实例类别已定。OK，那么，对于一个新的实例，我们可以知道离它最近的 k 个样本的类别，然后，我们就可以通过多数表决等方式进行预测。**等方式？除了多数表决还有什么方式？**

因此，k近邻算法不具有显式的学习过程，它实际上是直接使用的训练集的数据来对特征空间进行划分，并作为它分类的“模型”。**这个还是比较特别的。这句好像不是很通顺。**

OK，从上面的分类过程可以看出，这其中的：

* K值的选择 ：到底选几个？
* 距离度量：什么叫最近？
* 分类决策规则：知道它周边的k个样本了，怎么来投票决定这个测试样本的类别？


这三点其实就决定了一个KNN算法的效果，它们也是KNN算法的三个基本要素。


# 再仔细看一下这三要素






  * k 值的选择
    * k 值的选择会对 k 近邻算法的结果产生重大的影响。
    * 如果选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，“学习” 的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是 “学习” 的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<span style="color:red;">什么是近似误差和估计误差</span>
    * 如果选择较大的 k 值，就相当于用较大的邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。 k 值的增大就意味着整体的模型变得简单。

  * 距离度量
    * 特征空间中两个实例点的距离是两个实例点相似程度的反映。
    * k 近邻模型的特征空间一般是 n 维实数向量空间 \(R^n\)，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 \(L_p\) 距离，或者 Minkowski 距离。

  * 分类决策规则
    * k 近邻算法中的分类决策规则往往是多数表决，即由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。**除了多数表决的方法还有什么方法吗？**


# OK，那么KNN算法有那些特点呢？


优点：

* 精度高、
* 对异常值不敏感、
* 无数据输入假定

缺点：

* 计算复杂度高、
* 空间复杂度高


适用数据范围：

* 数值型和标称型 **什么是标称型？**





# OK，那么我们的算法流程是什么样子的？


假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。那么：

1. 输入没有标签的新数据后，计算新数据与样本数据集中每条数据的距离。
2. 对求得的所有距离进行排序（从小到大，越小表示越相似）。
3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
4. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。





## 相关资料

1. [第2章 k-近邻算法](http://ml.apachecn.org/mlia/knn/)
