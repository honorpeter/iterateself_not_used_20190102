---
title: 01 子集搜索与评价
toc: true
date: 2018-06-29 12:31:13
---

子集搜索与评价

我们能用很多属性描述一个西瓜，例如色泽、根蒂、敲声、纹理、触感 等，但有经验的人往往只需看看根蒂、听听敲声就知道是否好瓜.换言之，对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些 属性则可能没什么用。

我们将属性称为“特征”(feature)，对当前学习任务有用的属性称为“相关特征”(relevant feature)、没什么用的属性称为“无关特征” (irrelevant feature).从给定的特征集合中选择出相关特征子集的过程，称为“特征选择” (feature selection).<span style="color:red;">嗯，这个相关不是线性相关的意思。只是有用的意思，确认下。</span>

特征选择是一个重要的“数据预处理”(data preprocessing)过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器.那么，为什么要进行特征选择呢？


有两个很重要的原因：
- 首先，我们在现实任务中经常会遇到维数灾难问题, 这是由于属性过多而造成的，若能从中选择出重要的特征，使得后续学习过程 仅需在一部分特征上构建模型,则维数灾难问题会大为减轻.从这个意义上说， 特征选择与第10章介绍的降维有相似的动机；事实上，它们是处理高维数据的 两大主流技术。
- 第二个原因是，去除不相关特征往往会降低学习任务的难度，这就像侦探破案一样，若将纷繁复杂的因素抽丝剥茧，只留下关键因素，则真相往往更易看清.

需注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程 会因为重要信息的缺失而无法获得好的性能。

给定数据集，若学习任务不同，则 相关特征很可能不同，因此，特征选择中所谓的“无关特征”是指与当前学习任务无关.有一类特征称为“冗余特征” (redundant feature),它们所包含的信息能从其他特征中推演出来.例如，考虑立方体对象，若已有特征“底面长” “底面宽”，则“底面积”是冗余特征，因为它能从“底面长”与“底面宽” 得到.冗余特征在很多时候不起作用，去除它们会减轻学习过程的负担.但有时冗余特征会降低学习任务的难度，例如若学习目标是估算立方体的体积，则 “底面积”这个冗余特征的存在将使得体积的估算更容易；更确切地说，若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的 <span style="color:red;">是呀，这么来看，冗余特征也是不是那么轻易就可以去掉的。</span>。为简化讨论，本章暂且假定数据中不涉及冗余特征，并且假定初始的特征集合包含了所有的重要信息.<span style="color:red;">好吧，那么有冗余特征的时候怎么对应呢？</span>

欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了；然而这在计算上却是不可行的，因为这样做会遭遇组合爆炸，特征个数稍多就无法进行.可行的做法是产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，再对其进行评价，……这个过程持续进行下去，直至无法找到更好的候选子集为止。

显然，这里涉及两个关键环节：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？



第一个环节是“子集搜索” (subset search)问题.给定特征集合$\{a_1,a_2,\cdots ,a_d\}$ ，我们可将每个特征看作一个候选子集，对这d个候选单特征子集进行评价，假定 $\{a_2\}$ 最优，于是将 $\{a_2\}$ 作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这 d-1 个候选两特征子集中  $\{a_2,a_4\}$ 最优，且优于 $\{a_2\}$ 于是将  $\{a_2,a_4\}$ 作为本轮的选定集；……假定在第 k+1 轮时，最优的候选 (k+1) 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的特征集合作为特征选择结果。这样逐渐増加相关特征的策略称为“前向”(fdrward)搜索.类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略 称为“后向”(backward)搜索.还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征(这些特征在后续轮中将确定不会被去除)、同时减少无关特征，这样的策略称为“双向”(bidirectional)搜索.

显然，上述策略都是贪心的，因为它们仅考虑了使本轮选定集最优，例如在 第三轮假定选择 $a_5$ 优于 $a_6$ ,于是选定集为  $\{a_2,a_4,a_5\}$ ,然而在第四轮却可能是  $\{a_2,a_4,a_6,a_8\}$ 比所有的 $\{a_2,a_4,a_5,a_i\}$ 都更优。遗憾的是，若不进行穷举搜索, 则这样的问题无法避兔.


第二个环节是“子集评价”(subset evaluation)问题。给定数据集 D ,假定 D 中第 i 类样本所占的比例为 $p_i(i=1,2,\cdots ,|\mathcal{Y}|)$ 。为便于讨论，假定样本属性均为离散型.对属性子集 A ，假定根据其取值将 D 分成了 V 个子集 $\{D^1,D^2,\cdots ,D^V\}$ ，每个子集中的样本在 A 上取值相同，于是我们可计算属性子集 A 的信息增益

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/JImllbHcCJ.png?imageslim)


其中信息熵定义为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180629/JaA48hDFJF.png?imageslim)

信息增益 Gain(A) 越大，意味着特征子集 A 包含的有助于分类的信息越多.于 是，对每个候选特征子集，我们可基于训练数据集 D 来计算其信息増益，以此作为评价准则.


更一般的，特征子集 A 实际上确定了对数据集D的一个划分，每个划分区域对应着 A 上的一个取值，而样本标记信息 Y 则对应着对 D 的真实划分，通过估算这两个划分的差异，就能对 A 进行评价.与 Y 对应的划分的差异越小，则说明A越好.信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异 的机制都能用于特征子集评价。

将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法.例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树结点的划分属性所组成的集合就是选择出的特征子集.其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种(或多种)子集搜索机制和子集评价机制.

常见的特征选择方法大致可分为三类：过滤式(filter)、包裹式(wrapper)、 嵌入式(embedding).




# REF
1. 《机器学习》周志华
