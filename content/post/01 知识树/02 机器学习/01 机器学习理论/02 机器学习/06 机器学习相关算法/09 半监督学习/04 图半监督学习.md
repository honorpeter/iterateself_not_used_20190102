---
title: 04 图半监督学习
toc: true
date: 2018-08-21 18:16:48
---


图半监督学习

给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图 中一个结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间 存在一条边，边的“强度” (strength)正比于样本之间的相似度(或相关性)•我 们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚 未染色.于是，半监督学习就对应于“颜色”在图上扩散或传播的过程.由于一个图对应了一个矩阵，这就使得我们能基于矩阵运算来进行半监督学习算法的推导与分析.

给定 $D_l=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_l,y_l)\}$和 $D_u=\{x_{l+1},x_{l+2},\ldots ,x_{l+u}\}$ 其中访 $l\ll u$ ，$l+u=m$ 我们先基于功 $\D_l\bigcup D_u$ 构建一个图 $G = (V,E)$,其中结点集 $V=\{x_1,x_2,\cdots,x_l,x_{k+1},\cdots ,x_{l+u})$ ,,边集 $E$ 可表示为一个亲和矩阵(affinity matrix),常基于高斯函数定义为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/FFDLKf8eI9.png?imageslim)


其中 $i,j\in\{1,2,\cdots,m\}$,$\rho>0$ 是用户指定的高斯函数带宽参数.

假定从图 $G = (V,E)$ 将学得一个实值函数 $f:V\rightarrow \mathbb{R}$,其对应的分类规则为 $y_i=sign(f(x_i))$,$y_i\in\{-1,+1\}$ .直观上看，相似的样本应具有相似的标记， 于是可定义关于$f$的“能量函数”(energy function) :

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/JBk85HJFJK.png?imageslim)



其中 $f=(f_l^Tf_u^T)^T$ ,$f_l=(f(x_1);f(x_2);\ldots ;\f(x_l))$, $f_u=(f(x_{l+1});f(x_{l+2});\ldots;f(x_{l+u}))$ 分别为函数$f$在有标记样本与未标记样本上的预测结果,$D=diag(d_1,d_2,\cdots,d_{l+u})$ 是一个对角矩阵，其对角元素 $d_i=\sum_{j=1}^{l+u}(W)_{ij}$ 为矩 阵 $W$ 的第 $i$ 行元素之和.

具有最小能量的函数 $f$ 在有标记样本上满足 $f(x_i)=y_i(i=1,2,\cdots,l)$ ，在未标记样本上满足 $\Delta f=0$,其中 $\Delta=D-W$ 为拉普拉斯矩阵(Laplacian matrix).以第 l 行与第 l 列为界，采用分块矩阵表示方式：![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/8jfmA3LkJ3.png?imageslim) ，![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/eKG5cDAE6f.png?imageslim)则式(13.12)可重写为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/HDf51EBL0D.png?imageslim)


由 $\frac{\partial{E(f)} }{\partial f_u}=0$ 可得

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/69DaGaeLIE.png?imageslim)

令

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/e324GfgafB.png?imageslim)


即 $P_{uu}=D_{uu}^{-1}W_{uu}$,$P_{ul}=D_{uu}^{-1}W_{ul}$,则式(13.15)可重写为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/Lbh2IKhDII.png?imageslim)


于是，将 $D_l$ 上的标记信息作为 $f_l=(y_1;y_2;\cdots;y_l)$ 代入式(13.17)，即可利用求得的 $f_u$ 对未标记样本进行预测.

上面描述的是一个针对二分类问题的标记传播(label propagation)方法，下 面来看一个适用于多分类问题的标记传播方法[Zhou et al., 2004].

假定 $y_i\in\mathcal{Y}$ 仍基于 $D_l\bigcup D_u$ ,构建一个图 $G=(V,E)$ ，其中结点集 $V=\{x_1,\cdots,x_l,\cdots,x_{l+u}$ ，边集$E$ 所对应的 $W$ 仍使用式(13,11)，对角矩阵 $D = diag(d_1,d_2,\cdots,d_{l+u})$ 的对角元素 $d_i=\sum_{j=1}^{l+u}(W)_{ij}$ 。定义一个 $(l+u)\times |\mathcal{Y}|$ 的非负标记矩阵 $F=(F_1^T,F_2^T,\cdots,F_{l+u}^T)^T ，其第 i 行元素 $F_i=((F)_{i1},(F)_{i2},\cdots ,(F)_{i|\mathcal{Y}|})$ 为示例 $x_i$ 的标记向量，相应的分类规则为: $y_i=arg\,max_{1\leq j\leq |\mathcal{Y}|}(F)_{ij}$ 。




对 $i=1,2,\cdots ,m,j=1,2,\cdots,|\mathcal{Y}|$，将 $F$ 初始化为

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/lceFd3kLec.png?imageslim)


显然，Y的前 l 行就是 l 个有标记样本的标记向量.

基于 W 构造一个标记传播矩阵 $S=D^{-\frac12} WD^{-\frac12}$ ,其中 $D^{-\frac12}=diag(\frac{1}{\sqrt{d_1} },\frac{1}{\sqrt{d_2} },\cdots,\frac{1}{\sqrt{d_{l+u} } })$ ,于是有迭代计算式

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/A6Ahkd0bmi.png?imageslim)

其中 $\alpha\in(0,1)$ 为用户指定的参数，用于对标记传播项 $SF(t)$ 与初始化项 $Y$ 的重要性进行折中.基于式(13.19)迭代至收敛可得

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/E5gi1h9GBl.png?imageslim)


由 $F^*$可获得 $D_u$ 中样本的标记 $(\frac{y}_{l+1},\frac{y}_{l+2},\cdots,\frac{y}_{l+u})$ 。算法描述如图13.5所示.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/5hGhkeb6a0.png?imageslim)

事实上，图13.5的算法对应于正则化框架[Zhou et al., 2004]

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180701/C4kEImAe9b.png?imageslim)



其中 $\mu>0$ 为正则化参数.当 $\mu=\frac{1-\alpha}{\alpha}$ 时，式(13.21)的最优解恰为图13.5算法的迭代收敛解 $F^*$

式(13.21)右边第二项是迫使学得结果在有标记样本上的预测与真实标记 尽可能相同，而第一项则迫使相近样本具有相似的标记，显然，它与式(13.12)都 是基于半监督学习的基本假设，不同的是式(13.21)考虑离散的类别标记，而 式(13.12)则是考虑输出连续值.

图半监督学习方法在概念上相当清晰，且易于通过对所涉矩阵运算的分析来探索算法性质.但此类算法的缺陷也相当明显.首先是在存储开销上，若样本数为 $O(m)$,则算法中所涉及的矩阵规模为 $O(m^2)$ ,这使得此类算法很难直接 处理大规模数据；另一方面，由于构图过程仅能考虑训练样本集，难以判知新样本在图中的位置，因此，在接收到新样本时，或是将其加入原数据集对图进行重 构并重新进行标记传播，或是需引入额外的预测机制，例如将$D_l$ 和经标记传播后得到标记的 $D_u$ 合并作为训练集，另外训练一个学习器例如支持向量机来对新样本进行预测.



# REF
1. 《机器学习》周志华
