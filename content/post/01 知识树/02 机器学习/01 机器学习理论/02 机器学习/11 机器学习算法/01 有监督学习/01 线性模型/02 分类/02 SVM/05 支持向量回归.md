---
title: 05 支持向量回归
toc: true
date: 2018-07-31 18:06:17
---
# 支持向量回归

## 需要补充的

- <span style="color:red;">看得云里雾里，说不清楚吧，每个字都还认识，说清楚吧，其实一团糊涂。</span>



现在我们来考虑回归问题，给定训练样本 $D=\{(x_1,y_1),(x_2,y_2),\cdots (x_m,y_m)\}，$y_i\in \mathbb{R}$ 。希望学得一个形如式(6.7)的回归模型，使得 $f(x)$ 与 $y$ 尽可能接近，$w$ 和 $b$ 是待确定的模型参数。

对样本 $(x,y)$ ，传统回归模型通常直接基于模型输出 $f(x)$ 与真实输出 $y$ 之间的差别来计算损失，当且仅当 $f(x)$ 与 $y$ 完全相同时，损失才为零。

与此不同，支持向量回归(Support Vector Regression，简称SVR) 假设我们能容忍 $f(x)$ 与 $y$ 之间最多有 $\epsilon$ 的偏差，即仅当 $f(x)$ 与 $y$ 之间的差别绝对值大于 $\epsilon$ 时才计算损失。

如图6.6所示，这相当于以为 $f(x)$ 中心，构建了一个宽度为 $2\epsilon$ 的间隔带，若训练样本落入此间隔带，则认为是被预测正确的.

![mark](http://images.iterate.site/blog/image/180627/eAKlA3BEmJ.png?imageslim)

于是，SVR 问题可形式化为

![mark](http://images.iterate.site/blog/image/180627/mjb41Hl5EL.png?imageslim)

其中 C 为正则化常数，$\ell_{\epsilon}$ 是图6.7所示的 $\epsilon$-不敏感损失($\epsilon$-insensitive loss) 函数。<span style="color:red;">嗯，是这样。</span>

![mark](http://images.iterate.site/blog/image/180627/kDk2bEG74F.png?imageslim)

引入松弛变量 $\xi_i$ 和 $\hat{\xi}_i$ 可将式(6.43)重写为 <span style="color:red;">什么是松弛变量，到底为什么要引入？</span>

![mark](http://images.iterate.site/blog/image/180627/HFJ5CHjhe4.png?imageslim)
![mark](http://images.iterate.site/blog/image/180627/fLd8e3l8bE.png?imageslim)

![mark](http://images.iterate.site/blog/image/180627/Ke2LHdHL6D.png?imageslim)

类似式 (6.36)，通过引入拉格朗日乘子 $\mu_i\geqslant 0$，$\hat{\mu}_i\geqslant 0$ ,$\alpha_i\geqslant 0$，$\hat{\alpha}_i\geqslant 0$ ，由拉格朗日乘子法可得到式子 (6.45) 的拉格朗日函数：

![mark](http://images.iterate.site/blog/image/180627/77Hh7b3l49.png?imageslim)

将式(6.7)代入，再令 $L(w,b,\alpha,\hat{\alpha},\xi,\hat{\xi},\mu,\hat{\mu})$ 对 $w$，$b$ ，$\xi_i$， $\hat{\xi}_i$ 的偏导为零可得：

![mark](http://images.iterate.site/blog/image/180627/5E2k8e8I03.png?imageslim)

将式(6.47)-(6.S0)代入式(6.46)，即可得到 SVR 的对偶问题

![mark](http://images.iterate.site/blog/image/180627/73IKjl7d18.png?imageslim)

上述过程中需满足 KKT 条件，即要求

![mark](http://images.iterate.site/blog/image/180627/jgdG9i32m7.png?imageslim)

可以看出，当且仅当 $f(x_i)-y_i-\epsilon-\xi_i=0$ 时 $\alpha_i$ 能取非零值，当且仅当$y_i-f(x_i)-\epsilon-\hat{\xi}_i=0$ 时 $\hat{\alpha}_i$ 能取非零值。换言之，仅当样本 $(x_i,y_i)$ 不落入 $\epsilon$-间隔带中，相应的 $\alpha_i$ 和 $\hat{\alpha}_i$ 才能取非零值。

此外，约束 $f(x_i)-y_i-\epsilon-\xi_i=0$ 和  $y_i-f(x_i)-\epsilon-\hat{\xi}_i=0$ 不能同时成立，因此 $\alpha_i$ 和 $\hat{\alpha}_i$ 中至少有一个为零。

将式(6.47)代入(6.7)，则 SVR 的解形如：

![mark](http://images.iterate.site/blog/image/180627/3E8DJfH8k2.png?imageslim)

能使式(6.53)中的 $(\hat{\alpha}_i-\alpha_i)\neq 0$ 的样本即为 SVR 的支持向量，它们必落在 $\epsilon$-间隔带之外。显然，SVR的支持向量仅是训练样本的一部分，即其解仍具有稀疏性。

由 KKT 条件(6.52)可看出，对每个样本 $(x_i,y_i)$ 都有 $(C-\alpha_i)\xi_i=0$ 且 $\alpha_i(f(x_i)-y_i-\epsilon-\xi_i)=0$ 。于是。于是，在得到 $\alpha_i$ 后，若 $0<\alpha_i<C$ ，则必有 $\xi_i=0$ ，进而有：

![mark](http://images.iterate.site/blog/image/180627/B8230JIEgh.png?imageslim)


因此，在求解式(6.51)得到 $\alpha_i$ 后，理论上来说，可任意选取满足 $0<\alpha_i<C$ 的样本通过式(6.54)求得 $b$。实践中常采用一种更鲁棒的办法：选取多个(或所有)满足条件  $0<\alpha_i<C$  的样本求解 $b$ 后取平均值。

若考虑特征映射形式(6.19)，则相应的，式(6.47)将形如

![mark](http://images.iterate.site/blog/image/180627/6ELkmDE3kA.png?imageslim)

将式(6.55)代入(6.19)，则SVR可表示为

![mark](http://images.iterate.site/blog/image/180627/GH49fiheL7.png?imageslim)

其中 $\kappa(x_i,y_i)=\phi (x_i)^T\phi(x_j)$ 为核函数。




## 相关资料

- 《机器学习》
