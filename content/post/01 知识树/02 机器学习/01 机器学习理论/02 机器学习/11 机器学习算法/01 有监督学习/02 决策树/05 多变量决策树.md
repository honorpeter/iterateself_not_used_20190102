---
title: 05 多变量决策树
toc: true
date: 2018-06-26 13:49:54
---



多变量决策树


若我们把每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本就对应了 d 维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。

决策树所形成的分类边界有一个明显的特点: 轴平行 (axis-parallel) ，即它的分类边界由若干个与坐标轴平行的分段组成.

以表4.5中的西瓜数据 3.0a为例，将它作为训练集可学得图 4.10 所示的决策树，这棵树所对应的分类边界如图 4.11 所示.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/i3K079JdCK.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/9e1K4FLaDl.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/F1KD67GAd3.png?imageslim)


显然，分类边界的每一段都是与坐标轴平行的，这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值.但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似：<span style="color:red;">是的。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/c63llmg2kJ.png?imageslim)

如图 4.12 所示；此时的决策树会相当复杂，由于要进行大量的属性测试，预测 时间开销会很大.

若能使用斜的划分边界，如图4.12中红色线段所示，则决策树模型将大为简化。“多变量决策树” (multivariate decision tree) 就是能实现这样的 “斜划分” 甚至更复杂划分的决策树。以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个属性,而是对属性的线性组合进行测试。换言之，每个非叶结点是一个形如 $\sum_{i=1}^{d}w_ia_i=t$ 的线性分类器，其中 $w_i$ 是属性 $a_i$  的权重, $w_i$ 和 $t$ 可在该结点所含的样本集和属性集上学得.于是，与传统的“单变量决策树”(univariate decision tree)不同，在多变量决策树的学习过程中， 不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。

类器.例如对西瓜数据3.0a,我们可学得图 4.13 这样的多变量决策树，其分类边界如图4.14所示.

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/mJfb1l2ajk.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180626/AdHFkdBb3f.png?imageslim)

<span style="color:red;">这个到底是真么做到的？好像没有怎么详细讲，还是很厉害的，而且，这样岂不是很多东西都可以作为节点？</span>



* * *





# COMMENT

这下面是补充阅读。

决策树学习算法最著名的代表是江)3 [Quinlan, 1979, 1986]、C4.5 [Quinlan, I"3]和 CART [Breiman et al., 1984]. [Murthy, 1998]提供了一个关于决 策树文献的阅读指南.C4.5Rule是一个将C4.5决策树转化为符号规则的算法 [Quinlan, 1993],决策树的每个分支可以容易地重写为一条规则，但C4.5Rule 算法在转化过程中会进行规则前件合并、删减等操作，因此最终规则集的泛化 性能甚至可能优于原决策树.

本质上，各种特征选择 方法均可用于决策树的划 分属性选择.特征选择参 见第11章.


在信息增益、增益率、基尼指数之外，人们还设计了许多其他的准则用 于决策树划分选择，然而有实验研究表明[Mingers, 1989b],这些准则虽然对 决策树的尺寸有较大影响，但对泛化性能的影响很有限.[Raileanu and Stoffel, 2004]对信息增益和基尼指数进行的理论分析也显示出，它们仅在2%的情况下 会有所不同.4.3节介绍了决策树剪枝的基本策略;剪枝方法和程度对决策树泛 化性能的影响相当显著，有实验研究表明[Mingers, 1989a],在数据帝有噪声时 通过剪枝甚至可将决策树的泛化性能提髙25%.

关于感知机和神经网络, 参见第5章.


多变量决策树算法主要有0C1 [Murthy et al., 1994]和[Brodley and Ut-goff, 1995]提出的一系列算法.0C1先贪心地寻找每个属性的最优权值，在局 部优化的基础上再对分类边界进行随机扰动以试图找到更好的边界[Brodley and Utgoff, 1995]则直接引入了线性分类器学习的最小二乘法、还有一些算法 试图在决策树的叶结点上嵌入神经网络，以结合这两种学习机制的优势，例如 “感知机树” (Perceptron tree) [Utgoff, 1989b]在决策树的每个叶结点上训练 一个感知机，而[Guo and Gelfand，1992]则直接在叶结点上嵌入多层神经网络.

有一些决策树学习算法可进行“增量学习”(incremental learning),即在 接收到新样本后可对已学得的模型进行调整，而不用完全重新学习.主要机 制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法 有 ID4 [Schlimmer and Fisher, 1986]、ID5R [Utgoff, 1989a] > ITI [Utgoff et al., 1997]等.增量学习可有效地降低每次接收到新样本后的训练时间开销，但多步 增量学习后的模型会与基于全部数据训练而得的模型有较大差别.






## 相关资料
  1. 《机器学习》周志华
