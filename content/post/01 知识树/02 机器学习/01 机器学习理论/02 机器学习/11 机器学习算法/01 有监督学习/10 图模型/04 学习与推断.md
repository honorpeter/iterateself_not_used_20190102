---
title: 04 学习与推断
toc: true
date: 2018-07-01 10:38:22
---
学习与推断



基于概率图模型定义的联合概率分布，我们能对目标变量的边际分 布(marginal distribution)或以某些可观测变量为条件的条件分布进行推断.条 件分布我们已经接触过很多，例如在隐马尔可夫模型中要估算观测序列 x 在给定参数 $\lambda$ 下的条件概率分布。

边际分布则是指对无关变量求和或积分后得到结果，例如在马尔可夫网中，变量的联合分布被表示成极大团的势函数乘积，于是，给定参数 $\Theta$ 求解某个变量 $x$ 的分布，就变成对联合分布中其他无关变量进行积分的过程，这称为“边际化”(marginalization).


对概率图模型，还需确定具体分布的参数，这称为参数估计或参数学习问 题，通常使用极大似然估计或最大后验概率估计求解.但若将参数视为待推测的变量，则参数估计过程和推断十分相似，可以 “吸收” 到推断问题中.因此，下面我们只讨论概率图模型的推断方法.

具体来说，假设图模型所对应的变量集 $x=\{x_1,x_2,\cdots,x_N\}$ 能分为$x_E$ 和 $x_F$ 两个不相交的变量集，推断问题的目标就是计算边际概率$P(x_F)$或条件概率$P(x_F|x_E)$ 。由条件概率定义有

![mark](http://images.iterate.site/blog/image/180701/eD7D0BhFE0.png?imageslim)


其中联合概率$P(x_E,x_F)$ 可基于概率图模型获得，因此，推断问题的关键就是 如何高效地计算边际分布，即

![mark](http://images.iterate.site/blog/image/180701/JBm53lHmfH.png?imageslim)

概率图模型的推断方法大致可分为两类.第一类是精确推断方法，希望能计算出目标变量的边际分布或条件分布的精确值;遗憾的是，一般情形下，此类 算法的计算复杂度随着极大团规模的増长呈指数増长，适用范围有限.第二类 是近似推断方法，希望在较低的时间复杂度下获得原问题的近似解；此类方法 在现实任务中更常用.

本节介绍两种代表性的精确推断方法，下一节介绍近似 推断方法.

## 1变量消去

精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立性 来削减计算目标概率值所需的计算量.变量消去法是最直观的精确推断算法,也是构建其他精确推断算法的基础.

![mark](http://images.iterate.site/blog/image/180701/KcCGK66gIc.png?imageslim)

我们先以图 14.7（a）中的有向图模型为例来介绍其工作流程.

假定推断目标是计算边际概率$P(x_5)$ 。显然，为了完成此目标，只需通过加 法消去变量$\{x_1,x_2,x_3,x_4\}$ 即

![mark](http://images.iterate.site/blog/image/180701/3GG1lcd3DH.png?imageslim)

不难发现，若采用$\{x_1,x_2,x_4,x_3\}$ 的顺序计算加法，则有

![mark](http://images.iterate.site/blog/image/180701/ag8l0ghdFD.png?imageslim)

其中$m_{ij}(x_j)$ 是求加过程的中间结果，下标$i$表示此项是对$x_i$求加的结果，下标 j 表示此项中剩下的其他变量.显然，$m_{ij}(x_j)$ 是关于 $x_j$ 的函数.不断执行此过程可得

![mark](http://images.iterate.site/blog/image/180701/i5EIHaL68B.png?imageslim)


显然，最后的 $m_{35}(x_5)$ 是关于$x_5$ 的函数,仅与变量$x_5$的取值有关.

事实上，上述方法对无向图模型同样适用.不妨忽略图14.7(a)中的箭头，
将其看作一个无向图模型，有

![mark](http://images.iterate.site/blog/image/180701/GA6clggC8H.png?imageslim)

其中$Z$为规范化因子.边际分布$P(x_5)$ 可这样计算：

![mark](http://images.iterate.site/blog/image/180701/e363kcGLa5.png?imageslim)

显然，通过利用乘法对加法的分配律，变量消去法把多个变量的积的求和 问题，转化为对部分变量交替进行求积与求和的问题.这种转化使得每次的求 和与求积运算限制在局部，仅与部分变量有关，从而简化了计算.

变量消去法有一个明显的缺点：若需计算多个边际分布，重复使用变量 消去法将会造成大量的冗余计算.例如在图14.7(a)的贝叶斯网上，假定在计 算$P(x_5)$之外还希望计算$P(x_4)$ ,若采用$\{x_1,x_2,x_5,x_3\}$ 的顺序，则$m_{12}(x_2)$ 和 $m_{23}(x_3)$ 的计算是重复的.

## 2信念传播
亦称Sum-Product算法.


信念传播(Belief Propagation)算法将变量消去法中的求和操作看作一个消 息传递过程，较好地解决了求解多个边际分布时的重复计算问题.具体来说，变 量消去法通过求和操作
![mark](http://images.iterate.site/blog/image/180701/D0K8K5F9g4.png?imageslim)

消去变量$x_i$,其中$n(i)$ 表示结点$x_i$ 的邻接结点.在信念传播算法中，这个操作 被看作从$x_i$向$x_j$传递了一个消息$m_{ij}(x_j)$ .这样，式(14.15)和(l4.16) 所描述的变量消去过程就能描述为图14.7(b)所示的消息传递过程.不难发现，每次消息传递操作仅与变量及其邻接结点直接相关，换言之，消息传递相关的计算被限制在图的局部进行.



在信念传播算法中，一个结点仅在接收到来自其他所有结点的消息后才能 向另一个结点发送消息，且结点的边际分布正比于它所接收的消息的乘积，即

![mark](http://images.iterate.site/blog/image/180701/bhiF7KHCgD.png?imageslim)

例如在图 14.7(b) 中，结点 $x_3$ 要向 $x_5$ 发送消息，必须事先收到来自结点 $x_2$ 和 $x_4$ 的消息，且传递到岛的消息 $m_{35}(x_5)$ 恰为概率 $P(x_5)$ .

若图结构中没有环，则信念传播算法经过两个步骤即可完成所有消息传递， 进而能计算所有变量上的边际分布：

- 指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到 所有邻接结点的消息；

- 从根结点开始向叶结点传递消息，直到所有叶结点均收到消息.

例如在图14.7(a)中，令$x_1$ 为根结点，则 $x_4$ 和 $x_5$ 为叶结点.以上两步消息传递的过程如图14.8所示.此时图的每条边上都有方向不同的两条消息，基于这些消息和式(14.20)即可获得所有变量的边际概率.


![mark](http://images.iterate.site/blog/image/180701/gh8blEafL5.png?imageslim)




## 相关资料
1. 《机器学习》周志华
