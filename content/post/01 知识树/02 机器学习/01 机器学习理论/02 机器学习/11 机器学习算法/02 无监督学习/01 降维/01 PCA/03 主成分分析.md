---
title: 03 主成分分析
toc: true
date: 2018-06-29 11:42:04
---



主成分分析


主成分分析(Principal Component Analysis，简称PCA)是最常用的一种降维方法.在介绍PCA之前，不妨先考虑这样一个问题:

对于正交属性空间中的样本点，如何用一个超平面(直线的高维推广)对所有样本进行恰当的表达? 容易想到，若存在这样的超平面，那么它大概应具有这样的性质：

- 最近重构性：样本点到这个超平面的距离都足够近；
- 最大可分性：样本点在这个超平面上的投影能尽可能分开.

有趣的是，基于最近重构性和最大可分性，能分别得到主成分分析的两种等价推导.我们先从最近重构性来推导.

假定数据样本进行了中心化，即 $\sum_i x_i=0$ ；再假定投影变换后得到的新坐标系为 $\{w_1,w_2,\cdots ,w_d\}$ ,其中 $w_i$ 是标准正交基向量，$||w_i||_2=1$ ,$w_i^Tw_j=0$ $(i\neq j)$ 。若丢弃新坐标系中的部分坐标，即将维度降低到 $d'<d$ ，则样本点 $x_i$ 在低维坐标系中的投影是 $z_i=(z_{i1};z_{i2};\cdots ;z_{id'})$ ，其中 $z_{ij}=w_j^Tx_i$ 是 $x_i$ 在低维坐标系下第 j 维的坐标.若基于 $z_i$ 来重构 $x_i$ ，则会得到 $\hat{x}_i=\sum_{j=1}^{d'}z_{ij}w_j$ 。

考虑整个训练集，原样本点 $x_i$ 与基于投影重构的样本点 $\hat{x}_i$ 之间的距离为

![mark](http://images.iterate.site/blog/image/180629/EdhKmGmLKK.png?imageslim)


根据最近重构性，式(10.14)应被最小化，考虑到 $w_j$ 是标准正交基， $\sum_i x_ix_i^T$ 是协方差矩阵，有

![mark](http://images.iterate.site/blog/image/180629/j19cLld8Lh.png?imageslim)

这就是主成分分析的优化目标.

从最大可分性出发，能得到主成分分析的另一种解释.我们知道，样本点 $x_i$ 在新空间中超平面上的投影是 $W^Tx_i$ ，若所有样本点的投影能尽可能分开, 则应该使投影后样本点的方差最大化，如图10.4所示.

投影后样本点的方差是 $\sum_i W^Tx_ix_i^TW$ ，于是优化目标可写为

![mark](http://images.iterate.site/blog/image/180629/mID21Cbbce.png?imageslim)


![mark](http://images.iterate.site/blog/image/180629/H0k9kHi53E.png?imageslim)

显然，式(10.16)与(10.15)等价。

对式(10.15)或(10.16)使用拉格朗日乘子法可得

![mark](http://images.iterate.site/blog/image/180629/gD9BK2B9ha.png?imageslim)

于是，只需对协方差矩阵 $XX^T$ 进行特征值分解，将求得的特征值排序: $\lambda_1\geq \lambda_2\geq \cdots \geq \lambda_d$ ,再取前 $d'$ ^个特征值对应的特征向量构成 $W=(w_1,w_2,\cdots ,\w_{d'})$ 。这就是主成分分析的解.PCA算法描述如图10.5所示.

![mark](http://images.iterate.site/blog/image/180629/1J55LA54b7.png?imageslim)

降维后低维空间的维数 $d'$ 通常是由用户事先指定，或通过在 $d'$ 值不同的 低维空间中对 k 近邻分类器(或其他开销较小的学习器)进行交叉验证来选取 较好的 $d'$ 值。对PCA,还可从重构的角度设置一个重构阈值,例如 $t=95%$,然 后选取使下式成立的最小，值：


![mark](http://images.iterate.site/blog/image/180629/7gGb3JEf5e.png?imageslim)


PCA 仅需保留 W 与样本的均值向量即可通过简单的向量减法和矩阵-向量乘法将新样本投影至低维空间中.显然，低维空间与原始高维空间必有不同, 因为对应于最小的 $d-d'$ 个特征值的特征向量被舍弃了，这是降维导致的结果。但舍弃这部分信息往往是必要的：
- 一方面，舍弃这部分信息之后能使样本的采样密度增大，这正是降维的重要动机；
- 另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的效果.








## 相关资料
1. 《机器学习》周志华
