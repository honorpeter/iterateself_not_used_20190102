---
title: 04 原型聚类
toc: true
date: 2018-08-21 18:16:23
---






原型聚类

原型聚类亦称“基于原型的聚类”(prototype-based clustering),此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常用。通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式,将产生不同的算法。

下面介绍几种著名的原型聚类算法.

## 1    k均值算法

给定样本集 $D=\{x_1,x_2,\cdots ,x_m\}$  “k均值” (k-means)算法针对聚类所
得簇划分 $\mathcal{C}=\{C_1,C_2,\cdots ,C_k\}$ 最小化平方误差

![mark](http://images.iterate.site/blog/image/180628/6cImEc0l5K.png?imageslim)

其中 $\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x$ 是簇 $C_i$ 的均值向量.直观来看，式(9.24)在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度， E 值越小则簇内样本相似度越高.

最小化式(9.24)并不容易，找到它的最优解需考察样本集 D 所有可能的簇划分，这是一个NP难问题.因此，k均值算法采用了贪心策略，通过迭代优化来近似求解式(9.24).算法流程如图9.2所示，其中第1行对均值向量进行初始化，在第4-8行与第9-16行依次对当前簇划分及均值向量迭代更新，若迭代更新后聚类结果保持不变，则在第18行将当前簇划分结果返回.

![mark](http://images.iterate.site/blog/image/180628/Ebkbfhgaal.png?imageslim)

下面以表9.1的西瓜数据集4.00为例来演示k均值算法的学习过程.为方便叙述，我们将编号为i的样本称为 $x_i$ ，这是一个包含“密度”与“含糖率” 两个属性值的二维向量.

![mark](http://images.iterate.site/blog/image/180628/9G4k5Ce25C.png?imageslim)


假定聚类簇数 k = 3,算法开始时随机选取三个样本 $x_6$,$x_{12}$,$x_{27}$ 作为初始均值向量，即

![mark](http://images.iterate.site/blog/image/180628/AJFEeJ5C16.png?imageslim)

考察样本心 $x_1=(0.697; 0.460)$,它与当前均值向量 $\mu_1$, $\mu_2$, $\mu_3$ 的距离分别为 0.369,0.506,0.166,因此 $x_1$ 将被划入簇 $C_3$ 中.类似的，对数据集中的所有样本考察一遍后，可得当前簇划分为

![mark](http://images.iterate.site/blog/image/180628/C5FGlcaBcJ.png?imageslim)

于是，可从 $C_1$、 $C_2$、 $C_3$分别求出新的均值向量

![mark](http://images.iterate.site/blog/image/180628/Cf36AG9hCe.png?imageslim)

更新当前均值向量后，不断重复上述过程，如图9.3所示，第五轮迭代产生的结果与第四轮迭代相同，于是算法停止，得到最终的簇划分.

![mark](http://images.iterate.site/blog/image/180628/Ldbddlm2kL.png?imageslim)


## 2学习向量量化

与 k均值算法类似，“学习向量量化”(Learning Vector Quantization,简称LVQ)也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类.

给定样本集 $D=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_m,y_m),\}$ ,每个样本 $x_j$ 是由 n 个属性描述的特征向量 $(x_{j1},x_{j2},\cdots ,x_{jn})$ ,$\y_j\in\mathcal{Y}$ 是样本 $x_j$ 的类别标记. LVQ 的目标是学得一组 n 维原型向量 $\{p_1,p_2,\cdots ,p_q\}$ ,每个原型向量代表一个聚类簇，簇标记 $t_i\in \mathcal{Y}$ 。


![mark](http://images.iterate.site/blog/image/180628/gj0efCgKam.png?imageslim)

LVQ 算法描述如图9.4所示.算法第1行先对原型向量进行初始化，例如对第 q 个簇可从类别标记为 $t_q$ 的样本中随机选取一个作为原型向量.算法第2~12行对原型向量进行迭代优化.在每一轮迭代中，算法随机选取一个有标记训练样本，找出与其距离最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新.在第12行中，若算法的停止条件已满足(例如已达到最大迭代轮数，或原型向量更新很小甚至不再更新)，则将当前原型向量作为最终结果返回.

显然，LVQ的关键是第6-10行，即如何更新原型向量，直观上看，对样本 $x_j$ ,若最近的原型向量 $p_{i*}$ 与 $x_j$ 的类别标记相同，则令 $p_{i*}$ 向 $x_j$ 的方向靠拢， 如第7行所示，此时新原型向量为:

![mark](http://images.iterate.site/blog/image/180628/JCaIj9DII9.png?imageslim)

$p'$ 与 $x_j$ 之间的距离为

![mark](http://images.iterate.site/blog/image/180628/fd3FE7681G.png?imageslim)

令学习率 $\eta\in(0,1)$ ，则原型向量 $p_{i*}$ 在更新为 $p'$ 之后将更接近 $x_j$ .


在学得一组原型向量 $\{p_1,p_2,\cdots ,p_q\}$ 后，即可实现对样本空间 $\mathcal{X}$ 的簇划分.对任意样本 x ,它将被划入与其距离最近的原型向量所代表的簇中；换言之，每个原型向量 $p_i$ 仍定义了与之相关的一个区域 $R_i$ ，该区域中每个样本与 $p_i$ 的距离不大于它与其他原型向量 $p_{i'}(i'\neq i)$ 的距离，即

![mark](http://images.iterate.site/blog/image/180628/1J92LG9370.png?imageslim)

由此形成了对样本空间 $\mathcal{X}$ 的簇划分 $\{R_1,R_2,\cdots ,R_q\}$ ，该划分通常称为 “Voronoi剖分” (Voronoi tessellation).

下面我们以表 9.1 的西瓜数据集 4.0 为例来演示LVQ的学习过程.令 9-21 号样本的类别标记为 $c_2$ ,其他样本的类别标记为 $c_1$ .假定 q=5,即学习目标是找到5个原型向量 $p_1$,$p_2$,$p_3$,$p_4$,$p_5$,并假定其对应的类别标记分别为$c_1$,$c_2$,$c_2$,$c_1$,$c_1$。


算法开始时，根据样本的类别标记和簇的预设类别标记对原型向量进行随机初始化，假定初始化为样本 $x_5$ ,$x_{12}$,$x_{18}$,$x_{23}$,$x_{29}$。在第一轮迭代中，假定随机选取的样本为 $x_1$ ，该样本与当前原型向量 $p_1$,$p_2$,$p_3$,$p_4$,$p_5$ 的距离分别为 0.283, 0.506, 0.434, 0.260, 0.032.由于 $p_5$ 与 $x_1$ 距离最近且两者具有相同的类别标记 $c_2$ ,假定学习率 $\eta=0.1$ ，则 LVQ 更新 $p_5$ 得到新原型向量

![mark](http://images.iterate.site/blog/image/180628/JFbDI9diK5.png?imageslim)


将 $p_5$ 更新为 $p'$ 后，不断重复上述过程，不同轮数之后的聚类结果如图9.5所示.

## 3高斯混合聚类

与 k 均值、LVQ用原型向量来刻画聚类结构不同，高斯混合(Mixture-of-Gaussian)聚类采用概率模型来表达聚类原型.

我们先简单回顾一下(多元)高斯分布的定义。对n维样本空间 $\mathcal{X}$ 中的随机 向量 x ,若 x 服从高斯分布，其概率密度函数为.

![mark](http://images.iterate.site/blog/image/180628/IfEBkGe7eL.png?imageslim)

其中 $\mu$ 是 n 维均值向量， $\sum$ 是 $n\times n$ 的协方差矩阵.由式(9.28)可看出，高斯分布完全由均值向量 $\mu$ 和协方差矩阵 $\sum$ 这两个参数确定.为了明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为 $p(x|\mu,\sum)$。

![mark](http://images.iterate.site/blog/image/180628/EgbJ3HGEBi.png?imageslim)

我们可定义高斯混合分布

![mark](http://images.iterate.site/blog/image/180628/9mFcl24KeL.png?imageslim)


该分布共由 k 个混合成分组成，每个混合成分对应一个高斯分布.其中 $\mu_i$ 叫与 $\sum_i$ 是第 i 个高斯混合成分的参数，而 $\alpha_i>0$ 为相应的“混合系数” (mixture coefficient),$\sum_{i=1}^{k}\alpha_i=1$ 。

假设样本的生成过程由高斯混合分布给出：首先，根据 $\alpha_1,\alpha_2,\cdots ,\alpha_k$ 定义的先验分布选择高斯混合成分，其中 $\alpha_i$ 为选择第 i 个混合成分的概率;然后，根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。



若训练集 $D=\{x_1,x_2,\cdots ,x_m\}$ 由上述过程生成，令隨机变量 $x_j\in\{1,2,\cdots ,k\}$ 表示生成样本 $x_j$ 的高斯混合成分，其取值未知.显然，$z_j$ 的先验概率 $P(z_j = i)$ 对应于$\alpha_i(i=1,2,\cdots ,k)$ 。根据贝叶斯定理， $z_j$ 的后验分布对应于

![mark](http://images.iterate.site/blog/image/180628/E3LALm4J3d.png?imageslim)


换言之，$P_{\mathcal{M} }(z_j=i|x_j)$ 给出了样本 $x_j$ 由第 i 个高斯混合成分生成的后验概率.为方便叙述,将其简记为 $\gamma_{ji}(i=1,2,\cdots k)$

当高斯混合分布(9.29)已知时，高斯混合聚类将把样本集 D 划分为 k 个簇 $\mathcal{C}=\{C_1,C_2,\cdots ,C_k\}$ ,每个样本 $x_j$ 的簇标记 $\lambda_j$ 如下确定：

![mark](http://images.iterate.site/blog/image/180628/5D7aDIJGkC.png?imageslim)



因此，从原型聚类的角度来看,高斯混合聚类是采用概率模型(高斯分布)对原型进行刻画，簇划分则由原型对应后验概率确定.

那么，对于式(9.29)，模型参数 $\{(\alpha_i,\mu_i,\sum_i)|1\leq i\leq k\}$ 如何求解呢？显然, 给定样本集 D ,可采用极大似然估计，即最大化(对数)似然

![mark](http://images.iterate.site/blog/image/180628/LmlEA1hA6I.png?imageslim)


常采用EM算法进行迭代优化求解.下面我们做一个简单的推导.

若参数 $\{(\alpha_i,\mu_i\sum_i)|1\leq i\leq k\}$ 能使式(9.32)最大化，则由 $\frac{\partial LL(D)}{\partial \mu_i}=0$ 有

![mark](http://images.iterate.site/blog/image/180628/gDi1hBiLl5.png?imageslim)


由式(9.30)以及 $\gamma_{ji}=p_{\mathcal{M} }(z_j=i|x_j)$ ， 有


![mark](http://images.iterate.site/blog/image/180628/07ebj0FjkF.png?imageslim)

即各混合成分的均值可通过样本加权平均来估计，样本权重是每个样本属于该成分的后验概率.类似的，由 $\frac{\partial LL(D)}{\partial \sum_i}=0$ 可得

![mark](http://images.iterate.site/blog/image/180628/7jhLaFf9aI.png?imageslim)

对于混合系数 $\alpha_i$ ，除了要最大化 $LL(D)$ ，还需满足 $\alpha_i\geq 0,\sum_{i=1}^{k}\alpha_i=1$ 。考虑 $LL(D)$ 的拉格朗日形式

![mark](http://images.iterate.site/blog/image/180628/574ejH15dJ.png?imageslim)


其中 $\lambda$ 为拉格朗日乘子.由式(9.36)对 $\alpha_i$ 的导数为0,有    咖•丨

![mark](http://images.iterate.site/blog/image/180628/A7c0G85G0H.png?imageslim)

两边同乘以 $\alpha_i$ ，对所有样本求和可知 $\lambda =-m$ ，有

![mark](http://images.iterate.site/blog/image/180628/2101CB3hdA.png?imageslim)

即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定.

由上述推导即可获得高斯混合模型的 EM 算法：在每步迭代中，先根据当前参数来计算每个样本属于每个高斯成分的后验概率 $\gamma_{ji}$ (E步)，再根据式（9.34）、（9.35）和（9.38）更新模型参数 $\{(\alpha_i,\mu_i,\sum_i)|1\leq i\leq k\}$ （M步）.

高斯混合聚类算法描述如图 9.6 所示。算法第 1 行对高斯混合分布的模型参数进行初始化。然后，在第 2-12 行基于 EM 算法对模型参数进行迭代更新. 若 EM 算法的停止条件满足（例如已达到最大迭代轮数，或似然函数 LL(D) 増长很少甚至不再增长)，则在第14-17行根据高斯混合分布确定簇划分,在第18 行返回最终结果.

![mark](http://images.iterate.site/blog/image/180628/gA562Lbglk.png?imageslim)

以表9.1的西瓜数据集4.0为例，令高斯混合成分的个数 k = 3。算法开始 时，假定将高斯混合分布的模型参数初始化为：$\alpha_1=\alpha_2=\alpha_3=\frac{1}{3}$ ；$\mu_1=\mu_6=\mu_2=x_{22}$,$\mu_3=x_{27}$;$\sum_1=\sum_2=\sum_3=\begin{pmatrix} 0.1&0.0 \\ 0.0&0.1 \end{pmatrix}$。


在第一轮迭代中，先计算样本由各混合成分生成的后验概率.以 x_1 为例， 由式(9.30)算出后验概率 $\gamma_{11}= 0.219$, $\gamma_{12} = 0.404$,$\gamma_{13}=0.377$.所有样本的后验概率算完后，得到如下新的模型参数：

![mark](http://images.iterate.site/blog/image/180629/L6e6a7HIIE.png?imageslim)

模型参数更新后，不断重复上述过程，不同轮数之后的聚类结果如图9.7所示.

![mark](http://images.iterate.site/blog/image/180629/I5LKJ9kB92.png?imageslim)







## 相关资料
1. 《机器学习》周志华
