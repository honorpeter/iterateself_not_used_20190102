---
title: 聚类
toc: true
date: 2018-07-28 22:29:27
---
---
author: evo
comments: true
date: 2018-03-29 16:42:37+00:00
layout: post
link: http://106.15.37.116/2018/03/30/cluster/
slug: cluster
title: 聚类
wordpress_id: 1466
categories:
- 随想与反思
tags:
- '@todo'
- '@want_to_know'
---

<!-- more -->

**注：非原创，推荐直接看原文**


## 相关资料






  * 七月在线 机器学习


  *



## 需要补充的






  * **将几种聚类的方法拆开来，每个的代码都要写。**


  * **确定下还有没有欠缺的东西**


  * **很多东西理解的不够，比如哪些距离的计算方式，如果自己来写应该怎么写？**


  * **而且各种聚类的方式还是要代码实现一遍的，不然根本不能细致的理解。**


  *



# MOTIVE






  * 对聚类进行总结。





* * *






# 数学基础






  * 实对称阵不同特征值的特征向量正交





###




# 什么是聚类？


当我们面对大量的未知标注的数据集的时候，其实是可以按照数据的内在相似性，将数据集划分为多个类别的，而且这些类别的每个类内部的数据相似度较大，而类别间的数据相似度较小。

这个划分成这些类别的过程就是聚类。

嗯，可见聚类涉及的数据集合的特征是未知的，并且在开始聚类之前，我们并不知道要把数据划分成几类，而且也并不清楚分组的标准。

那么，因此就有两个问题了：




  1. 分组的标准怎么定？即怎么定义类内部的数据的相似度？类之间的数据的相似度？


  2. 怎么知道要把数据划分成几类？





# OK，我们首先来解决第一个问题：如何定义相似性？




## 样本之间的距离


首先，其实不管两个样本到底是什么样本，有什么特征，它们之间总是可以考察距离的，不仅可以考察距离，还有很多考察的方法：


![mark](http://images.iterate.site/blog/image/180728/5D48kHDLH1.png?imageslim)

在聚类这个PPT里面，凡是谈到距离就约等于不相似度，相似度就与等于距离近的程度。

最简单的距离就是欧式空间的欧氏距离，就是第一个式子的p=2的时候。p=1是曼哈顿距离，p=无穷大 的时候，xi和yi的某一对坐标插值最远的那个就是它们之间的距离。

如果X，Y不是坐标点，而是集合呢？那么就是Jaccard 系数，这个在推荐系统中用的非常多。

第三个，就是得到两个坐标点，总是可以求余弦值的，也可以度量它们之间的距离，实际上在讲SVD那个例子的时候，求距离的地方就是用的这个余弦值。计算文本相似的时候经常用余弦相似度。

如果把上面式子中的a和b看作两个随机变量X和Y，那么我就可以求X和Y的协方差以及各自的方差，这样我们就可以求Pearson相关系数 皮尔逊相关系数。

另外，在最大熵模型中探讨多，两个随机分布的距离可以用KL距离，也就是相对熵。。这也是一个度量。

我们知道最大熵模型中的p和q不是对称的，因此我们可以给一个对称化的公式，这个就是Hellinger距离。这个距离中\(\alpha\)趋近于+-1的时候，就是相对熵

总之，在聚类中，可能会用到各种各样的距离的度量，当然最多的还是欧式距离。

**对每个距离的用法和场景再总结下。**


## Hellinger distance



![mark](http://images.iterate.site/blog/image/180728/Aa4I0cak6A.png?imageslim)


## 余弦相似度 与 Pearson相似系数


![mark](http://images.iterate.site/blog/image/180728/2EfkAiDKJG.png?imageslim)

即如果，x的均值是0，y的均值是0，那么pearson相关系数就是我们的夹角的余弦，二者的关系非常紧密。


# 如何选择类别数目呢？


首先我们要有个指标来评价聚类的效果。


![mark](http://images.iterate.site/blog/image/180728/gFI1FBaBjG.png?imageslim)

可见ai越小越好，bi越大越好。


![mark](http://images.iterate.site/blog/image/180728/h7aKGHg744.png?imageslim)

利害，所以选择k的时候，就可以先试试，得出不同的结果的轮廓系数，系数偏1，则就选择那个k。

但是这个运算时间上的确是一个问题。

时间快慢是因为你需要对它做空间的索引，其实你要算一个点到其他店的距离，你需要建一棵树。来方便你去查找它的邻域点来干这个事。**到底是怎么做的？**

**空间索引跟B树，R树有关。要仔细了解下。**




# 聚类的基本思想


![mark](http://images.iterate.site/blog/image/180728/mAEAaIKfKC.png?imageslim)

这个也是一个贪心的思路。






# 主要内容


这部分大多数都是属于无监督分类。最后会介绍半监督问题。**半监督的问题都是怎么解决的？**




  * 掌握K-means聚类的思路和使用条件


  * 了解层次聚类的思路和方法


  * 理解密度聚类并能够应用于实践


    * DBSCAN


    * 密度最大值聚类





  * 掌握谱聚类的算法


    * 考虑谱聚类和PCA的关系







# COMMENT




**感觉这个还是非常好的，要自己从理论到实践全面理解。**
