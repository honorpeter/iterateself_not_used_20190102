---
title: 06 半监督聚类
toc: true
date: 2018-07-01 08:38:05
---

半监督聚类


聚类是一种典型的无监督学习任务，然而在现实聚类任务中我们往往能获得一'些额外的监督信息，于是可通过半监督聚类(semi-supervised clustering)来利用监督信息以获得更好的聚类效果.



聚类任务中获得的监督信息大致有两种类型.
- 第一种类型是“必连” (must-link)与“勿连” (cannot-link)约束，前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇;
- 第二种类型的监督信息则是少量的有标记样本.

约束 k 均值(Constrained k-means)算法是利用第一类监督信息的代表.给定样本集 $D=\{x_1,x_2,\cdots ,x_m\}$  以及“必连”关系

![mark](http://images.iterate.site/blog/image/180701/jIBHLKG1ll.png?imageslim)

集合 $\mathcal{M}$ 和“勿连”关系集合 $\mathcal{C}$。$(x_i,x_j)\in\mathcal{M}$ 表示 $x_i$ 与 $x_j$ 必属于同簇， $(x_i,x_j)\in\mathcal{C}$ 表示 $x_i$ 与 $x_j$ 必不属于同簇.该算法是 k 均值算法的扩展，它在聚类过程中要确保 $\mathcal{M}$ 与 $\mathcal{C}$ 的约束得以满足，否则将返回错误提示，算法如图 13.7所示.

以西瓜数据集4.0为例，令样本 $x_4$与 $x_{25}$， $x_{12}$与 $x_{20}$， $x_{14}$与 $x_{17}$之间存在必连约束， $x_{2}$ 与 $x_{21}$， $x_{13}$与 $x_{23}$， $x_{19}$与 $x_{23}$ 之间存在勿连约束即

![mark](http://images.iterate.site/blog/image/180701/LJmg10g46k.png?imageslim)



![mark](http://images.iterate.site/blog/image/180701/J2EfDHiFEJ.png?imageslim)


设聚类簇数 $k = 3$ ,随机选取样木 $x_6$, $x_12$, $x_27$作为初始均值向量，图13.8显示出约束 k 均值算法在不同迭代轮数后的聚类结果.经5轮迭代后均值向量 不再发生变化(与第4轮迭代相同)，于是得到最终聚类结果

![mark](http://images.iterate.site/blog/image/180701/Dll9LeDEDE.png?imageslim)


第二种监督信息是少量有标记样本。给定样本集 $D=\{x_1,x_2,\cdots,x_m\}$ 假定少量的有标记样本为 $S=\bigcup_{j=1}^{k}S_j\subset D$ , 其中 $S_j\neq \Phi$ 为隶属于第 j 个聚类簇的样本。

这样的监督信息利用起来很容易：直接将它们作为“种子”，用它们初始化 k 均值算法的个聚类中心，并且在聚类簇迭代更新过程中不改变种子样本的簇隶属关系.这样就得到了约束种子k均值(Constrained Seed k-means)算法[Basu et al.，2002],其算法描述如图13.9所示.


![mark](http://images.iterate.site/blog/image/180701/3e7fK1CgBK.png?imageslim)


仍以西瓜数据集4.0为例，假定作为种子的有标记样本为

![mark](http://images.iterate.site/blog/image/180701/bbIL94iI4k.png?imageslim)

以这三组种子样本的平均向量作为初始均值向量，图13.10显示出约束种子 k 均值算法在不同迭代轮数后的聚类结果.经 4 轮迭代后均值向量不再发生变 化（与第3轮迭代相同），于是得到最终聚类结果

![mark](http://images.iterate.site/blog/image/180701/a2HK9Kfdcc.png?imageslim)

![mark](http://images.iterate.site/blog/image/180701/3AGb4jeCli.png?imageslim)





# COMMENT


下面是阅读材料

半监督学习的研究一般认为始于［Shahshahani and Landgrebe, 1994］,该 领域在二十世纪末、二十一世纪初随着现实应用中利用未标记数据的巨大需 求涌现而蓬勃发展.国际机器学习大会(ICML)从2008年开始评选“十年最佳 论文”，在短短6年中，半监督学习四大范型(paradigm)中基于分歧的方法、 半监督SVM、图半监督学习的代表性工作先后于2008年［Blum and Mitchell, 1998］、2009 年［Joachims, 1999］、2013 年［Zhu et al.5 2003］获奖.

生成式半监督学习方法出现最早［Shahshahani and Landgrebe, 1994］.由 于需有充分可靠的领域知识才能确保模型假设不至于太坏，因此该范型后来主 要是在具体的应用领域加以研究.

半监督SVM的目标函数非凸，有不少工作致力于减轻非凸性造成的不 利影响，例如使用连续统(continuation)方法，从优化一个简单的凸目标函数开 始，逐步变形为非凸的S3VM目标函数［Chapelle et al., 2006a］;使用确定性退 火(deterministic annealing)过程，将非凸问题转化为一系列凸优化问题，然后 由易到难地顺序求解［Sindhwani et al.5 2006］;利用CCCP方法优化非凸函数 ［Collobert et al.5 2006］等.

fc近邻图和e近邻图参 见 10.5.1 节.


最早的图半监督学习方法［Blum and Chawla，2001］直接基于聚类假设， 将学习目标看作找出图的最小割(nxixicut；).对此类方法来说，图的质量极为重 要5 13.4节的高斯距离图以及A:近邻图、e近邻图都较为常用，此外已有一些 关于构图的研究［Wang and Zhang, 2006; Jebara et al., 2009］,基于图核(graph kernel)的方法也与此有密切联系［Chapelle et al., 2003］.

许多集成学习研究者认 为：只要能使用多个学习 器即可将弱学习器性能提 升到极高，无须使用未标 记样本；许多半监督学习 研究者认为:只要能使用 未标记样本即可将弱学习 器性能提升到极高，无须 使用多学习器.但这两种 看法都有其局限.


基于分歧的方法起源于协同训练，最初设计是仅选取一个学习器用于预测 ［Blum and Mitchell, 1998］.三体训练(tri-training)使用三个学习器，通过“少 数服从多数”来产生伪标记样本，并将学习器进行集成［Zhou and Li, 2005b］. 后续研究进一步显示出将学习器集成起来更有助于性能提升，并出现了使用更 多学习器的方法.更为重要的是，这将集成学习与半监督学习这两个长期独立 发展的领域联系起来［Zhou, 2009］.此外，这些方法能容易地用于多视图数据， 并可自然地与主动学习进行结合［周志华，2013］.

［Belkin et al., 2006］在半监督学习中提出了流形正则化(manifold regular-ization)框架,直接基于局部光滑性假设对定义在有标记样本上的损失函数进行 正则化，使学得的预测函数具有局部光滑性.

半监督学习在利用未标记样本后并非必然提升泛化性能，在有些情形下甚

这里的“安全”是指利 用未标记样本后，能确保 泛化性能至少不差于仅利 用有标记样本.


至会导致性能下降.对生成式方法，其成因被认为是模型假设不准确［Cozman and Cohen, 2002］,因此需依赖充分可靠的领域知识来设计模型.对半监督 SVM，其成因被认为是训练数据中存在多个“低密度划分”，而学习算法有可 能做出不利的选择；S4VM ［Li and Zhou, 2015］通过优化最坏情形性能来综合 利用多个低密度划分，提升了此类技术的安全性.更一般的“安全”（safe）半监 督学习仍是一个未决问题.

本章主要介绍了半监督分类和聚类，但半监督学习已普遍用于各类机器学 习任务，例如在半监督回归［Zhou and Li, 2005a］ ＞降维［Zhang et al.5 2007］等 方面都有相关研究.更多关于半监督学习的内容可参见［Chapelle etal., 2006b; Zhu5 2006］, ［Zhou and Li, 2010;周志华，2013］专门介绍了基于分歧的方法. ［Settles, 2009］是一个关于主动学习的介绍.




## 相关资料
1. 《机器学习》周志华
