---
title: 05 稀疏表示与字典学习
toc: true
date: 2018-06-29 19:06:09
---



稀疏表示与字典学习


不妨把数据集 D 考虑成一个矩阵，其每行对应于一个样本，每列对应于一个特征.特征选择所考虑的问题是特征具有“稀疏性”，即矩阵中的许多列与 当前学习任务无关，通过特征选择去除这些列，则学习器训练过程仅需在较小的矩阵上进行，学习任务的难度可能有所降低，涉及的计算和存储开销会减少, 学得模型的可解释性也会提高.


现在我们来考虑另一种稀疏性： D 所对应的矩阵中存在很多零元素，但这 些零元素并不是以整列、整行形式存在的.在不少现实应用中我们会遇到这样 的情形，例如在文档分类任务中，通常将每个文档看作一个样本，每个字(词)作为一个特征，字(词)在文档中出现的频率或次数作为特征的取值；换言之，D 所对应的矩阵的每行是一个文档，每列是一个字(词)，行、列交汇处就是某字(词在某文档中出现的频率或次数.那么，这个矩阵有多少列呢？以汉语为例，《康熙字典》中有47035个汉字，这意味着该矩阵可有4万多列，即便仅考 虑《现代汉语常用字表》中的汉字，该矩阵也有3500列.然而，给定一个文档, 相当多的字是不出现在这个文档中的，于是矩阵的每一行都有大量的零元素; 对不同的文档，零元素出现的列往往很不相同。



当样本具有这样的稀疏表达形式时,对学习任务来说会有不少好处，例如线性支持向量机之所以能在文本数据上有很好的性能，恰是由于文本数据在使用上述的字频表示后具有高度的稀疏性,使大多数问题变得线性可分.同时，稀疏样本并不会造成存储上的巨大负担，因为稀疏矩阵已有很多高效的存储方法.

那么，若给定数据集 D 是稠密的，即普通非稀疏数据，能否将其转化为 “稀疏表示” (sparse representation) 形式，从而享有稀疏性所带来的好处呢？ 需注意的是，我们所希望的稀疏表示是“恰当稀疏”，而不是“过度稀疏”。仍以汉语文档为例，基于《现代汉语常用字表》得到的可能是恰当稀疏，即其稀疏性足以让学习任务变得简单可行；而基于《康熙字典》则可能是过度稀疏, 与前者相比，也许并未给学习任务带来更多的好处.


显然，在一般的学习任务中(例如图像分类)并没有《现代汉语常用字表》 可用，我们需学习出这样一个“字典”.为普通稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为 “字典学习”(dictionary learning),亦称“稀疏编 码” (sparse coding)。这两个称谓稍有差别，“字典学习”更侧重于学得字典的 过程，而“稀疏编码”则更侧重于对样本进行稀疏表达的过程.由于两者通常是在同一个优化求解过程中完成的，因此下面我们不做进一步区分，笼统地称为字典学习。

给定数据集 $\{x_1,x_2,\cdots ,x_m\}$ 字典学习最简单的形式为

![mark](http://images.iterate.site/blog/image/180629/2l1Ke7cA1c.png?imageslim)

 其中  $B\in\mathbb{R}^{d\times k}$ 为字典矩阵，k 称为字典的词汇量，通常由用户指定，$\alpha_i\in \mathbb{R}^{k}$ 则是样本 $\x_i\in \mathbb{R}^d$ 的稀疏表示.显然，式(11.15)的第一项是希望 $\alpha_i$ 能很好地重构 $x_i$ ,第二项则是希望 $\alpha_i$ 尽量稀疏.


与LASSO相比，式(11.15)显然麻烦得多，因为除了类似于式(11.7)中 $w$ 的 $\alpha_i$ ，还需学习字典矩阵 B 。不过，受 LASSO 的启发，我们可采用变量交替优化的策略来求解式(11.15).

首先在第一步，我们固定住字典 B ，若将式(11.15)按分量展开，可看出其中 不涉及 $\alpha_i^u\alpha_i^v$$(u\neq v)$ 这样的交叉项，于是可参照 LASSO 的解法求解下式，从而为每个样本 $x_i$ 找到相应的 $\alpha_i$ ：

![mark](http://images.iterate.site/blog/image/180629/0Hh3LdchI5.png?imageslim)

在第二步，我们固定住 $\alpha_i$ 来更新字典B，此时可将式(11.15) 写为

![mark](http://images.iterate.site/blog/image/180629/03fLC8G9ag.png?imageslim)

其中 $X=(x_1,x_2,\cdots ,x_m)\in \mathbb{R}^{d\times m}$ ，$A=(\alpha_1,\alpha_2,\cdots ,\alpha_m)\in \mathbb{R}^{k\times m}$,$||\cdot ||_F$ 是矩阵的 Erobenius 范数。式(11.17)有多种求解方法，常用的有基于逐列更新策略的 KSVD 。令 $b_i$ 表示字典矩阵 B 的第i列，$\alpha^i$ 表示稀疏矩阵 A 的第 i 行，式(11.17)可重写为

![mark](http://images.iterate.site/blog/image/180629/akHEB83g6L.png?imageslim)



在更新字典的第 i 列时，其他各列都是固定的，因此 $E_i=\sum_{j\neq i}b_j\alpha^j$ 是固定的，于是最小化式(11.18)原则上只需对 $E_i$ 进行奇异值分解以取得最大奇异值所对应的正交向量。然而，直接对 $E_i$ 进行奇异值分解会同时修改 $b_i$ 和 $\alpha^i$ ，从而可能破坏 A 的稀疏性.为避免发生这种情况，KSVD 对 $E_i$  和 $\alpha^i$ 进行专门处理: $\alpha^i$ 仅保留非零元素， $E_i$ 则仅保留 $b_i$ 与 $\alpha^i$ 的非零元素的乘积项，然后再进行奇异值分解，这样就保持了第一步所得到的稀疏性.

初始化字典矩阵 B 之后反复迭代上述两步，最终即可求得字典 B 和样本 $x_i$ 的稀疏表示 $\alpha_i$ ,在上述字典学习过程中，用户能通过设置词汇量 k 的大小来控制字典的规模,从而影响到稀疏程度.





## 相关资料
1. 《机器学习》周志华
