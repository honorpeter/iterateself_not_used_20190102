---
title: 03 如何使用偏差和方差来决定该做什么
toc: true
date: 2018-08-12 18:34:19
---
## Chapter 20、Bias and Variance: The two big sources of error

**偏差和方差：错误的两大来源**

假设你的训练集、开发集和测试集都来自相同的分布。那么你应该总是试图去获取更多的训练数据，因为这样能只提高性能，对吗？

尽管有更多的数据是无害的，不幸的是，它并不总是如我们期望的那样有帮助。获取更多的数据可能是浪费时间。所以，你如何决定何时该加数据，何时不用这么麻烦？

机器学习中有两个主要错误来源：偏差和方差。理解它们将有助于你决定添加数据，以及其他提高性能的策略是否能很好地利用时间。

假设你希望构建一个5%错误的猫识别器。目前，你的训练集错误率为15%，并且你的开发集错误率为16%。在这种情况下，添加数据可能不会有太多帮助。你应该关注其他改变。实际上，在你的训练集上添加更多的样本只会让你的算法难以在训练集上做的更好。（我们在后面章节中解释了原因）

如果你在训练集上的错误率是15%（85%的准确率），但是你的目标是5%错误率（95%准确率），那么第一个要解决的问题是提高算法在训练集上的性能。你的开发/测试集上的性能通常比在训练集上要差。所以，如果算法在见过的样本上得到85%的准确率，那么是不可能在没见过的样本上得到95%的准确率的。

假设如上所述你的算法在开发集上有16%的错误率（84%的准确率）。我们将这16%的错误分为两部分：

- 首先，算法在训练集上的错误率。在本例中，它是15%。我们非正式地将此认为是算法的**偏差(bias)**。
- 其次，算法在开发（或测试）集上比训练集上差多少。在本例中，开发集比训练集差1%。我们非正式地将此认为是算法的**方差(variance)**【1】。

学习算法的一些改变能解决错误的第一个组成部分——偏差，并且提高算法在训练集上的性能；一些改变能解决第二个组成部分——方差，并帮助算法从训练集到开发/测试集上更好的泛化【2】。为了选择最有希望的改变，了解这两组错误中哪个更叩待解决是很有用的。

开发对于偏差和方差的良好直觉将帮助你为算法选择有效的改变。



——————————

【1】统计领域有更多关于偏差和方差的正式定义，我们不必担心。粗略地说，当你有一个非常大的训练集时，偏差就是你算法在训练集上的错误率。方差是与此设置中的训练集相比，你在测试集上差多少。当你的误差衡量是均方差(mean squared error)时，你可以卸下指定这两个量的公式，并证明 Total Error = Bias + Variance。但是为了决定如何在ML问题上取得进展的目的，这里给出的偏差和方差的更非正式的定义就足够了。

【2】这里还有一些通过对系统架构做出大的改变的方法，能够同时减少偏差和方差。但是这些方法往往难以识别和实施。



## Chapter 21、Examples of Bias and Variance

**偏差和方差的例子**

考虑我们的猫分类任务。一个“理想的”分类器（比如人类）在这个任务中可能取得近乎完美的表现。

假设你的算法表现如下：

- 训练错误率 = 1%
- 开发错误率 = 11%

它有什么问题？应用前一章的定义，我们估计偏差为1%，方差为10%（=11%-1%）。因此，它有一个很高的方差（**high variance**）。分类器训练误差非常低，但是没能成功泛化到开发集上。这也被叫做过拟合（**overfitting**）。

现在，考虑如下：

- 训练错误率 = 15%
- 开发错误率 = 16%

我们估计偏差为15%，方差为1%。该分类器错误率为15%，不适合训练集，但是它再开发集上的错误几乎没有比在训练集错误更高。因此，该分类器具有较高的偏差（**high bias**），但是较低的方差。我们称该算法是欠拟合（**underfitting**）的。

现在，考虑如下：

- 训练错误率 = 15%
- 开发错误率 = 30%

我们估计偏差为15%，方差为15%。该分类器有高偏差和高方差（**high bias and high variance**）：它再训练集上做的很差，因此有较高的偏差，它再开发集上表现更差，因此具有较高的方差。由于分类器同时过拟合和欠拟合，所以过拟合/欠拟合术语很难应用与此。

最后，考虑如下：

- 训练错误率 = 0.5%
- 开发错误率 = 1%

该分类器做的很好，它具有低偏差和低方差。恭喜取得这么好的表现。



## Chapter 22、Comparing to the optimal error rate

**比较最优错误率**

在我们猫识别的例子中，理想错误率——就是，一个“最优”分类器可达到的——接近0%。如果图片中有猫，人几乎总是可以识别出来。因此，我们希望可以做的一样好的机器。

其他问题更难。例如，假设你正在构建一个语音识别系统，并发现14%的音频片段有太多的背景噪声，或者很难理解，甚至人都不能识别出所说的内容。在这种情况下，即使是“最优”语音识别系统也可能有约为14%的错误。

假设在语音识别问题上，你的算法达到：

- 训练错误率 = 15%
- 开发错误率 = 30%

训练集上的表现以及接近最优的错误率14%。因此，在偏差上或者在训练集的表现上没有太多提升的空间。然而，算法没有很好的泛化到开发集上；因此在由于方差而导致的错误上还有很大的提升空间。

这个例子和前一章第三个例子类似，都有15%的训练错误率和30%的开发错误率。如果最优的错误率是 ~0%，那么15%的训练错误率留下了很大的提升空间。这表明减少偏差的变化可能是有益的。但是，如果最优错误率是14%，那么同样的训练集上的表现告诉我们，在分类器的偏差几乎没有改进的余地。

对于最佳错误率远不为零的问题，这里有一个对算法错误更详细的分解。继续上面我们语音识别的示例，可以将总的30%的开发集错误分解如下（类似的分析可以应用于测试集错误）：

- **最优错误率（“不可避免的偏差”）**：14%。假设我们决定，即使是世界上最好的语音系统，我们仍会遭受14%的错误。我们可以将其认为是学习算法的偏差“不可避免”的部分。
- **可避免的偏差**：1%。通过训练错误率和最优误差率之间的差值来计算【3】。
- **方差**：15%。开发错误和训练错误之间的差值。

为了将这与我们之前的定义联系起来，偏差和可避免的偏差关系如下【4】：

偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差

这个“可避免的偏差”反映了算法在训练集上的表现比“最优分类器”差多少。

方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集总是可以减少方差接近于零。因此，拥有足够大数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免的方差”。

再考虑一个例子，该例子中最优错误率是14%，我们有：

- 训练错误 = 15%
- 开发错误 = 16%

然而在前一章中我们称之为高偏差分类器，现在我们说可避免偏差的误差是1%，方差的误差约为1%。因此，算法已经做的很好了，几乎没有提升的空间。它只比最佳错误率差2%。

从这些例子中我们可以看出，了解最优错误率有利于指导我们的后续步骤。在统计学上，最优错误率也被成为**贝叶斯错误率（Bayes error rate）**，或贝叶斯率。

我们如何才能知道最优错误率是多少呢？对于人类还算擅长的任务，例如识别图片或转录音频剪辑，你可以要求人们提供标签，然后测量人为标签相对于你训练集的准确率。这将给出最优错误率的估计。如果你正在解决甚至人也很难解决的问题（例如预测推荐什么电影，或向用户展示什么广告），这将很难估计最优错误率。

在“与人类表现比较”（第33~35章）这一节中，我将更详细的讨论学习算法的表现和人类表现相比较的过程。

在最后几章中，你学习了如何通过查看训练集和卡法鸡错误率来估计可避免/不可避免的偏差和方差。下一章将讨论如何使用这种分析的见解来优先考虑减少偏差还是减少方差的技术。根据你项目当前的问题是高（可避免的）偏差还是高方差，你应该应用非常不同的技术。请继续阅读。



————————

【3】如果该值是负的，你在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且算法已经过度记忆（over-memorized）训练集。你应该专注于方差减少的方法，而不是进一步减少偏差的方法。

【4】选择这些定义是为了表达如何改进学习算法的见解。这些定义与统计学家定义偏差和方差不同。从技术上来说，我这里定义的“偏差”应该被叫做“我们认为是偏差的错误”；以及“可避免的偏差”应该被称为“我们认为学习算法的偏差超过最优错误率的错误”。


## Chapter 23、Addressing Bias and Variance

**处理偏差和方差**

以下是处理偏差和方差问题最简单的公式：

- 如果具有较高的可避免偏差，那么增加模型的大小（例如，通过添加层/神经元来增加神经网络的大小）。
- 如果具有较高的方差，那么增加训练数据集。

如果你可以增加神经网络的大小，并无限制的增加训练集数据，那么可以在很多学习问题上都做的很好。

在实践中，增加网络的模型终将导致你会遇到计算问题，因为训练大的模型很慢。你也可能会耗尽获取更多训练数据的能力。（即使在网上，也只有有限数量的猫图片）

不同的模型架构（例如，不同的神经网络架构）对于你的问题将有不同的偏差/方差量。最近很多深度学习研究已经开发出很多新的模型架构。所以，如果你在使用神经网络，学术文献可能会是一个很好的灵感来源。github上也有很多好的开源实现。但是尝试新架构的结果要比增加模型大小和添加数据这一简单公式难以预测。

增加模型的大小通常可以减少偏差，但也可能会增加方差和过拟合的风险。然而，这种过拟合的问题通常只在你不使用正则化的时候出现。如果你包含了一个精心设计的正则化方法，那么你通常可以安全的增加模型的大小，而不会增加过拟合。

假设你正在应用深度学习，有L2正则化和dropout，有在开发集上表现最好的正则化参数。如果你增加模型的大小，通常你的表现会保持不变或提升；它不太可能明显的变差。避免使用更大模型的唯一原因就是计算代价变大。


## Chapter 24、Bias vs. Variance tradeoff

**偏差和方差间的权衡**

你可能听过“偏差和方差间的权衡”。你能够对大部分学习算法进行的更改中，有一些能够减少偏差错误，但是是以增加方差为代价的，反之亦然。这就在偏差和方差之间产生了“权衡”。

例如，增加模型的大小（在神经网络中增加神经元/层，或增加输入特征），通常可以减少偏差，但可能会增加方差。另外，增加正则化一般会增加偏差，但是能减少方差。

在现代，我们往往能够获取充足的数据，并且可以使用非常大的神经网络（深度学习）。因此，这种权衡比较少，并且现在有更多的选择可以在不损害方差的情况下减少偏差，反之亦然。

例如，你通常可以增加神经网络的大小，并调整正则化方法去减少偏差，而不会明显的增加方差。通过增加训练数据，你通常也可以在不影响偏差的情况下减少方差。

如果你选择了一个很适合你任务的模型架构，那么你也可以同时减少偏差和方差。选择这样的架构可能是困难的。

在接下来的几个章节中，我们将讨论处理偏差和方差的额外的特定方法。。


## Chapter 25、Techniques for reducing avoidable bias

**减少可避免偏差的方法**

如果你的学习算法遭受高可避免偏差，你可以尝试以下方法：

- **增加模型大小**（如神经元/层的数量）：该方法可以减少偏差，因为它可以让你更好的适应训练集。如果你发现该方法增加了方差，那么使用正则化方法，它通常能够消除方差的增加。
- **基于错误分析的洞察修改输入特征**：假设错误分析启发你去创建额外的特征，以帮助算法消除特定类别的错误。（我们在下一章节进一步讨论）这些新特征可能有助于减少偏差和方差。理论上来说，增加更多的特征可能会增加方差，但如果你发现这种情况，那么久使用正则化方法，它通常能够消除方差的增加。
- **减少或消除正则化**（L2正则化，L1正则化，dropout）：这将减少可避免偏差，但会增加方差。
- **修改模型架构**（如神经网络架构）以便更适合你的问题：这种方法能够影响偏差和方差。

一种没有帮助的方法：

- **增加更多训练数据**：这种方法有助于解决方差问题，但是它通常对偏差没有显著的影响。


## Chapter 26、Error analysis on the training set

**训练集上的错误分析**

你的算法必须在训练集上表现很好，然后才能期望它在开发/测试集上表现很好。

除了之前描述的处理高偏差的方法之外，我有时也对训练数据进行错误分析，遵循类似于在 Eyeball 开发集上错误分析的规则。如果你的算法存在高偏差，也就是说，不能很好的适应训练集，那这可能是有用的。

例如，假设你在为一个app构建语音识别系统，从志愿者那里收集了音频剪辑作为训练集。如果系统在训练集上做的并不好，你可以考虑听一下算法表现很差的 ~100个样本，在这些样本上算法在理解训练集错误的主要类别方面做的很差。和开发集错误分析类似，你可以统计不同类别的错误：

![mark](http://images.iterate.site/blog/image/180812/8lH46De0Kk.png?imageslim)

在这个例子中，你可能会意识到算法在处理有很多背景噪声的训练样本时会特别艰难。因此，你可以专注于使得算法能更好的适应有背景噪声训练样本的方法。

提供你的学习算法相同的输入音频，你也可以再次确认人是否可能转录这些音频剪辑。如果有太多的背景噪声以至于没人可以听出说了什么，那就没有理由去期望任何一个算法能正确识别这些话。我们将在后一章中讨论将算法和人类表现水平相比较的益处。


## Chapter 27、Techniques for reducing variance

**减少方差的方法**

如果你的学习算法遭受高方差，你可以尝试以下方法：

- **添加更多训练数据**：只要你能够获取更多的数据和处理这些数据的充足计算能力，这是处理方差问题最简单也是最可靠的方法。
- **添加正则化**（L2正则化，L1正则化，dropout）：该方法减少了方差，但增加了偏差。
- **添加提前停止（early stopping）**（基于开发集错误提前停止梯度下降）：该方法减少方差但增加了偏差。提前停止的行为很像正则化方法，一些作者称它为正则化方法。


- **选择特征以减少输入特征的数目/类型：**该方法可能有助于解决方差问题，但也可能增加偏差。略微减少特征数量（比如从1000个特征减少到900）不太可能对偏差产生很大影响。显著地减少它（比如从1000减少到100——减少10倍）更可能有显著的影响，只要你没有将太多有用的特征排除在外。在现代深度学习中，当数据丰富时，已经从特征选择转移了出来，现在我们更有可能给算法我们所有的特征，并让算法根据数据分类使用哪些特征。但是当你训练集比较小时，特征选择可能非常有用。

减少模型大小（如神经元/层的数量）：谨慎使用。该方法能够减少方差，但可能增加偏差。但是，我不推荐使用该方法来处理方差。增加正则化通常能得到更好的分类性能。减少模型大小的好处就是降低计算成本，从而加快训练模型的速度。如果加快模型的训练是有用的，那无论如何考虑减少模型的大小。但是如果你的目标是减少方差，并且你不关心计算成本，那么考虑添加正则化替代之。

这里有两个额外的策略，重复上一章处理偏差中的方法：

- **基于错误分析的洞察修改输入特征**：假设错误分析启发你去创建额外的特征，以帮助算法消除特定类别的错误。这些新特征可能有助于减少偏差和方差。理论上来说，增加更多的特征可能会增加方差，但如果你发现这种情况，那么就使用正则化方法，它通常能够消除方差的增加。
- **修改模型架构**（如神经网络架构）以便更适合你的问题：这种方法能够影响偏差和方差。



## 相关资料

- [machine-learning-yearning](https://github.com/xiaqunfeng/machine-learning-yearning/)
- 《Machine Learning Yearning》 by Andrew NG
