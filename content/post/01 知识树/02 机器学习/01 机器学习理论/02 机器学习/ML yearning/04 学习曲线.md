---
title: 04 学习曲线
toc: true
date: 2018-08-12 19:23:39
---
## Chapter 28、Diagnosing bias and variance: Learning curves

**诊断偏差和方差：学习曲线**

我们已经看过一些方法去估计有多少错误可归因于可避免的偏差和方差。我们是通过估计最优错误率，并计算算法的训练集和开发集错误来进行估计的。让我们讨论一个更具信息性的方法：绘制学习曲线。

学习曲线会根据训练样本的数量来绘制开发集错误。为了绘制它，你可以使用不同大小的训练集去运行算法。例如，如果你有1000个样本，你可以在100,200,300,…,1000个样本上单独训练算法副本。然后你就能画出开发集错误如何随着训练集大小而变化的曲线了。如下图所示：

![mark](http://images.iterate.site/blog/image/180812/HH8GLf0098.png?imageslim)

随着训练集大小的增加，开发集错误应该减少。

我们经常会有一些我们希望学习算法最终能达到的“期望错误率”。例如：

- 如果我们希望达到人类水平的表现，那么人类错误率可能就是“期望错误率”。
- 如果我们的学习算法为某些产品提供服务（如提供猫图），我们可能会直观的了解需什么样的水平才能给用户提供出色的体验。
- 如果你长期从事于一个重要应用，那么你可能会有直觉认为在下一个季度/年内能合理取得多大进展。

将期望的表现水平添加到你的学习曲线中：

![mark](http://images.iterate.site/blog/image/180812/lag0fLkgbf.png?imageslim)

你可以直观的看到外推红色的“开发错误”曲线，以此来猜测通过添加更多的数据你能够多接近期望的性能水平。在上面的例子中，通过加倍训练集大小来达到期望的水平看似合理的。

但如果开发错误曲线趋于“稳定”（即变平），那么你可以立刻知道添加更多数据并不能达到你的目标：

![mark](http://images.iterate.site/blog/image/180812/69Dj9lIGF6.png?imageslim)

查看学习曲线可能会帮助你避免花费数月时间来收集两倍多的训练数据，只有意识到这并不管用。

这个过程的一个缺点是，如果你只关注开发错误曲线，如果有更多的数据，你很难推断和准确预测红色的曲线的走向。这里有一个附加的曲线能够帮助你去评估添加更多数据的影响：训练错误。


## Chapter 29、Plotting training error

**绘制训练错误曲线**

你的开发集（和测试集）错误应该随着训练集大小的增长而减少。但随着训练集大小的增加，训练集错误通常会增加。

让我们举例说明这个效果。假设你的训练集有只有两个样本：一张猫图和一张非猫图。学习算法很容易“记住”训练集中这两个样本，并且训练集错误率为0%。即使有一张或两张样本图片都被错误标注，算法仍然很容易记住这两个标签。

现在假设你的训练集有100个样本。可能有一些样本是被错误标记或模棱两可的——一些图非常模糊，甚至人都不能区分是否有猫。或许学习算法仍能“记住”大部分或所有的训练集，但现在很难获得100%的准确率。通过将训练集样本数从2增加到100，你将发现训练集准确率将略有下降。

最后，假设你的训练集有10000个样本。这种情况下算法更难以完全适应10000个样本，特别是有一些样本是模棱两可或错误标注的。因此，你的学习算法在该训练集上将做的更糟。

让我们为之前的曲线（开发错误曲线）添加训练错误曲线：

![mark](http://images.iterate.site/blog/image/180812/dI95EDl4cc.png?imageslim)

你可以看到蓝色的“训练错误”曲线随着训练集大小的增长而增长。而且，算法通常在训练集上表现比在开发集上要好。因此，红色的开发错误曲线通常严格地在蓝色训练错误曲线上方。

下一步我们将讨论如何解释这些曲线。


## Chapter 30、Interpreting learning curves: High bias

**解读学习曲线：高偏差**

假设你的开发错误曲线如下图所示：

![mark](http://images.iterate.site/blog/image/180812/186hiI4hge.png?imageslim)

我们之前说过，如果你的开发错误曲线平稳，仅仅通过添加数据你不可能达到期望的性能。

但是很难确切的知道红色开发错误曲线的外推是什么样的。如果开发集很小，可能会更不确定，因为曲线可能很嘈杂。

假设我们在图片中添加训练错误曲线，如下所示：

![mark](http://images.iterate.site/blog/image/180812/2L9kKiGjmC.png?imageslim)

现在，你可以绝对肯定的说，添加更多的数据本身并不足够。这是为什么？记住我们的两个观察结果：

- 随着我们添加更多的训练数据，训练错误只会变得更差。因此，蓝色的训练错误曲线只会保持不动或变得更高，所以它只会远离期望的性能水平（绿色的线）。
- 红色的开发错误曲线通常要高于蓝色的训练错误曲线。因此，即使训练错误高于期望性能水平，通过添加更多数据来让红色开发错误曲线下降到期望性能水平之下也基本没有可能。

在相同的图中检查开发错误曲线和训练错误曲线可以让我们更加自信地推断开发错误曲线。

为了讨论，假设期望性能是我们对最优错误率的估计。那上面的图片就是一个标准的“教科书”式的例子（具有高可避免偏差的学习曲线是什么样的）：在训练集大小的最大处（大概对应我们的所有训练数据），在训练错误和期望性能之间有大的间隙，表明大的可避免偏差。此外，训练和开发曲线之间的间隙小，表明方差小。

之前，我们只在曲线最右端的点去衡量训练集错误和开发集错误，这对应使用所有的可训练数据。绘制完整的学习曲线给我们提供了更全面图片，绘制了算法在不同训练集大小上的表现。


## Chapter 31、Interpreting learning curves: Other cases

**解释学习曲线：其他情况**

考虑如下学习曲线：

![mark](http://images.iterate.site/blog/image/180812/gDACDhIlA9.png?imageslim)

该曲线表明高偏差、高方差，还是二者？

蓝色的训练错误曲线相对低，红色开发错误曲线远高于蓝色训练错误。因此，偏差较小，但是方差较大。添加更多训练数据可能有助于缩小开发错误和训练错误之间的差距。

现在，考虑如下：

![mark](http://images.iterate.site/blog/image/180812/jL7lg6hFab.png?imageslim)

这次，训练错误较大，远高于期望性能水平。开发错误也比训练错误大很多。因此，具有显著的偏差和方差。你将不得不在算法中去寻找同时减少偏差和方差的方法。


## Chapter 32、Plotting learning curves

**绘制学习曲线**

假设你有一个非常小的训练集，只有100个样本。随机选择10个样本的子集来训练你的算法，然后是20个样本，然后30，直到100，以10个为间隔增加样本数。然后使用这10个数据点绘制学习曲线。你可能会发现曲线在较小的训练集大小下看起来有些嘈杂（意思是这些值比期望的要高/低）。

当只在10个随机选择的样本上训练时，你可能不幸选到了特别“bad”的训练集，例如有很多模棱两可/错误标注的样本。或者，你可能幸运的选到了特别“good”的训练集。小的训练集意味着开发和训练错误可能会随机波动。

如果你的机器学习应用严重偏倚一个类别（如负样本远比正样本多的猫分类任务），或者类别数比较大（如识别100种不同动物类别），那么选择尤其是“不具代表性”或坏的训练集的几率更大。例如，如果80%的样本是负样本（y=0），只有20%是正样本（y=1），那么有可能10个样本的训练集只包含负样本，因此很难让算法学习到有意义的东西。

如果训练曲线里的噪声使得很难看见真实的趋势，这里有两种解决方法：

- 不是仅对10个样本的一个模型进行训练，而是通过从原始100个样本的数据集中通过替换的抽样方法【1】选择几个（如3-10）不同的随机选择的10个样本的训练集。在这些数据集上训练不同的模型，并对每个结果模型计算训练集和开发集错误。计算并绘制平均训练错误和平均开发集错误。
- 如果你的训练集比较倾向一种类别，或有很多类别，从100个训练样本中选择一个“平衡的”子集而不是随机选择的10个训练样本。例如，你可以确保2/10的样本是正样本，8/10为负样本。更为一般的说，你可以确保每个类别的样本部分尽可能的接近原始训练集的整体部分。

我不会为这些技术而烦恼，除非你已经尝试过绘制学习曲线，并且认为曲线太嘈杂以至于看不到潜在的趋势。如果你的训练集较大（比如说超过1000个样本），并且你的类别分布不是很偏，你可能不需要这些技巧。

最后，绘制学习曲线可能花费很高的计算成本：例如，你可能需要训练10个模型，其中分别有1000个样本，然后是2000个，直到10000个。使用小的数据集训练模型比使用大数据集来训练模型要快的多。因此，不像上面那样均匀地将训练集大小按线性范围划分出来，你可以在1000、2000、4000、6000和10000个样本上训练模型。这样应该仍然可以让你清晰的了解学习曲线的趋势。当然，只有在训练所有附加模型的计算成本显著时，该方法才有意义。

————————————

【1】以下是采用替换的抽样方法（sampling with replacement）：你可以从100个里面随机选择10个不同的样本作为第一个训练集。然后你会再选10个样本，形成第二个训练集，不考虑第一个训练集中选了哪些样本。因此，一个特定的样本可能出现在第一和第二个训练集中。相反，如果你的样本没有被替换，第二个训练集将仅在第一次未被选择的90个样本中进行选择。在实践中，有或没有替换的抽样不应该产生很大的差异，但前者是常见的做法。



## 相关资料

- [machine-learning-yearning](https://github.com/xiaqunfeng/machine-learning-yearning/)
- 《Machine Learning Yearning》 by Andrew NG
