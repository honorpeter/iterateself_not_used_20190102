---
title: 从线性分类器开始
toc: true
date: 2018-08-21 18:16:48
---
# 从线性分类器开始


## REF

1. 七月在线 深度学习



# 缘由


实际上这个线性分类器是机器学习的内容，但是由于深度学习在作分类的任务的时候，实际上是相当于把网络抽取的特征输入到线性分类器中，来达到对样本进行分类的目的，因此，线性分类器再介绍一下，实际上看过机器学习的内容的话这个地方是差不多的。




# 线性分类器




## 为什么要讲线性分类器？


因为很多的神经网络的最后一层都是接的一层线性分类器，来达到分类的目的，因此，在学习神经网络之前，先讲一下线性分类器。


## 线性分类器的得分函数


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/HFga00H8FB.png?imageslim)

即希望去根据矩阵判定它对应的每个类别的得分是多少。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Gdfa569Hie.png?imageslim)

输入的矩阵reshape为一个3072*1的一个向量，3072行，1列。W是一个10行，3072列的矩阵。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/H4kdchcfJA.png?imageslim)

假如一张图片只有4个值。为了拿到3个类别的得分向量，那么W就是3*4，b就可以把y=Wx这条直线上下平移，现在这个结果中的cat的得分很低，那么我们要做的就是修正这个W，使得可以在一张图片过来之后，可以在正确的类别上有比较高的得分，在错误的类别上有比较低的得分。

**上面这三张图的确对\(f=Wx+b\)解释的非常好！**




# 有两种方式理解这个线性分类器




## 线性分类器的理解1：空间划分


对于 y=Wx+b 来说， 是W里面的一个行向量乘以 x 再加上 b ，才得到对应于 y 中间的某一个分类的得分，所以 W 中的3个行向量因此就对应了不同的类别。

而把每一行和对应的类别单单拿出来看的时候，这个 \(y_i=W_ix+b_i\)就可以看作是一条直线，输入是x，输出是某一个类别的得分。因此，对于W中的3个行向量来说，不同的行向量就对应了不同的直线 。画图如下：

x每输入一张图片的时候，就能在三条直线上得到3个不同的得分值。**这个地方感觉有些奇怪，这个得分值于线对应的区域有关系吗？箭头指的是什么？而且图片的位置不是应该画在x这条水平线上面吗？而且这个直线是不是有范围的？如果没有范围的话deer岂不是始终取不到了？**


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/86JGbeDBEB.png?imageslim)




## 线性分类器的理解2：模板匹配：


上面说了三个行向量对应三个不同的分类，那么可以把每一行看成对应的类别的模板，而每类得分，实际上就是像素点模板匹配度。只不过我的模板匹配的方式是内积计算而已。**这也行？这个将W的每一行看作一个模板的看法是有特殊的什么作用吗？或者能对这个有一些特别的理解？而且用内积的方式来作为模板的匹配度感觉有些奇怪吧？**

这个是CIFAR-10的**W的每一行reshape为32*32*3的图像的显示**：


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Fbk880cFhe.png?imageslim)

**？上面这个图是什么意思？是W的每一行reshape为32*32*3的图像的显示吗？相知道这个到底是什么？最好自己试验下。**

后面学卷积神经网络，每个神经元可以看作一个filter，它实际上也是一种模板，也是可以做可视化的。**每个神经元也是一个模板吗？每个神经元怎么做可视化？**






# 损失函数


loss function

先给定一个W，就可以由像素映射到类目得分，然后不断调整参数/权重W，目标就是使映射的结果和实际类别吻合。

而用怎么衡量吻合程度呢？是用的损失函数。损失函数的别的称呼：代价函数（cost function） 和客观度（objective）。

其实我们有很多中的损失函数，虽然评判的手段不一样，但是他们都能形容我们的吻合程度，

那么怎么去最小化这个loss呢？


## 损失函数1：合页损失 hinge loss （支持向量机损失） 为什么说是hinge loss？


对于训练集中的第 \(i\) 张图片数据 \(x_i\) ，在W下会有一个得分结果向量 \(f(x_i,W)\) ，第 \(j\) 类的得分为我们记作 \(f(x_i,W)_j\) 。则在该样本上的损失我们由下列公式计算得到：

\[L_i=\sum_{j\neq y_i}^{ }max(0,f(x_i,W)_j-f(x_i,W)_{y_i}+\Delta )\]

\(f(x_i,W)_{y_i}\)是我在正确的类别上的得分。因此这个式子说的是：我想让你在正确的类别上的得分比错误的类别上的得分高出来一段，这一段叫做\(\Delta\)。下面举个例子：**\(\Delta \)是什么？怎么确定 \(\Delta \) 而且为什么要求和？为什么要求max？**

例子：

假如我们现在有三个类别，而得分函数计算某张图片的得分为：\f(x_i,W)=[13,-7,11]\)，而实际的结果是第一类\((y_i=0)\)。假设\(\Delta =10\) 。上面的公式把错误类别\(j\neq y_i\)都遍历了一遍，求值加和：

\[L_i=max(0,-7-13+10)+max(0,11-13+10)\]

对于\(max(0,-7-13+10)\)来说，-7是错误的类别的得分，13是正确的类别的得分，那么13比-7高20，20比10大，那么\(max(0,-7-13+10)\)就是0，表示你在这两类的区分上面已经做的很好了，不用再看这两类之间的差别了。**嗯，原来是这样，这样的值保证了强调了最容易混淆的类的误差。但是为什么要这样计算呢？理论基础是什么？**

继续优化上面的 \(L_i\) ，因为是线性模型，因此可以简化为：

\[L_i=\sum_{j\neq y_i}^{ } max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta )\]


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/bEKfij7iJm.png?imageslim)

嗯 ，上面这个图的意思是：我有很多的类别的得分，然后我设置了一条警戒线，总是比正确类别小\(\Delta\)。如果警戒线之外，说明做得很好了，如果在警戒线之内，越过多少就把多少作为代价。**\(\Delta\)是否要归一化？**

加正则化项后得到：**这个正则化项是哪里来的？ 这个没讲**

\[L=\underset{data\, loss}{\underbrace{\frac{1}{N}\sum_{i}^{ }L_i} }+\underset{regularization\, loss}{\underbrace{\lambda R(W)} }\]

即：

\[L=\frac{1}{N}\sum_{i}^{ }\sum_{j\neq y_i}^{ } [max(0,f(x_i,W)_j-f(x_i,W)_{y_i}+\Delta)]+\lambda \sum_{k}^{ }\sum_{l}^{ }\]

**上面这个式子是怎么得到的？为什么\(L_i\)还要求和？regularization loss是什么？为什么等于\(\lambda \sum_{k}^{ }\sum_{l}^{ }\)？**




## 损失函数2：交叉熵损失 Cross-entropy loss（softmax分类器）


向找到一个办法，来算出属于不同分类的概率是多少，这样也好进行评判。

那么我们怎么计算不同分类的概率呢？我们不是直接根据评分在总分中的占比来做，而是先求exp。**为什么要先求exp呢？因为如果不用exp的值去做归一化，会出现负数。概率是不可能有负数的，对我知道不能出现负数，但是为什么要用exp，而不用别的呢？**

因此对于训练集中的第\(i\)张图片数据\(x_i\)在W下会有一个得分结果向量\(f_{y_i}\)。则损失函数记作：

\[L_i=-log(\frac{e^{f_{y_i} } }{\sum_{j}^{ } e^{f_j} })\]

或者

\[L_i=-f_{y_i}+log\sum_{j}^{ }e^{f_j}\]

那么这个时候我就想知道正确的概率向量和我刚计算出的概率限量之间的距离。两个向量之间的距离，可以从最大似然的角度来理解，也可以从KL散度的角度来理解。**怎么理解的？**最后计算出来的损失叫做交叉熵损失 \(-\sum p\,log(q)\)，它评估的是两个概率向量之间的距离。**交叉熵损失这个地方只是提了一下，要补充一下。**

那么这个时候又有一个问题了：如果类别的评分很大，\(e^{1000}\)结果溢出怎么办？所以，为了保证计算结果的稳定性，实际工程中一般这么算：

\[\frac{e^{f_{y_i} } }{\sum_{j}^{ }e^{f_j} }=\frac{Ce^{f_{y_i} } }{C\sum_{j}^{ }e^{f_j} }=\frac{e^{f_{y_i}+log\, C} }{\sum_{j}^{ }e^{f_j+log\, C} }\]

**为什么交叉熵是这样的函数？上面这个实际工程的公式的 \(C\) 是什么？是多少？**


## 损失函数3：Least squares (L2 loss)


Least squares (L2 loss)    \(L_i=||f-y_i||_2^2\)  **这个为什么没讲？**

**损失函数到底总共有几种？**






## 对两种损失函数的理解：


所以本质上来说：




  * 一个老师是设定了一个界限，然后要求正确的选项的得分至少比错误的选项的得分高\(\Delta\)。


  * 而另一个老师说：去做一个归一化，然后告诉我，你认为某个选项是标准答案的概率是多少？然后我这边是有一份正确的概率向量的，然后我会去评估这两个概率向量之间的距离。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/EmJmHFfB4J.png?imageslim)

**厉害的normalize，这样就sum为1，也就是说\(\sum_{j}^{ } e^{f_j}\)就是0，这样直接算\(-log(e^{f_{y_i} })\)就行。也就是说\(-log(0.353)\)就行，但是为什么是1.04？\(-log(0.353)\)不是0.45吗？**

拿到了loss之后我们有一系列的方式去做最优化，它本质上是一个函数，然后求最小值，求最小值就有很多方法，比如随机梯度下降，牛顿法，拟牛顿法，BFTS。**什么是拟牛顿法和BFTS？**



到现在为止，这个基础叫做线性分类器。




# COMMENT：


**损失函数感觉讲的不是很透彻。是不是只有这两个？还是要在机器学习中仔细补充这方面的。**

**而且还是有几个问题没有明白的。**
