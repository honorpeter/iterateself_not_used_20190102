---
title: 04 朴素贝叶斯法
toc: true
date: 2018-08-21 17:48:45
---
##### 第4章朴素贝叶斯法

朴素贝叶斯(naiveBayes)法是基于贝叶斯定理与特征条件独立假设的分类 方法®.对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合 概率分布；然后基于此模型，对给定的输入;c,利用贝叶斯定理求出后验概率最大 的输出y .朴素贝叶斯法实现简单，学习与预测的效率都很髙，是一种常用的方法.

本章叙述朴素贝叶斯法，包括朴素贝叶斯法的学习与分类、朴素贝叶斯法的 参数估计算法.

###### 4.1朴素贝叶斯法的学习与分类

4.1.1基本方法

设输入空间YeR”为n维向量的集合，输出空间为类标记集合3? = {Cl， c2>'">ck}-输入为特征向量工e/V，输出为类标记(class label) ye y. X是定义 在输入空间;v上的随机向量，r是定义在输出空间;y上的随机变量.pcr，y)是 z和r的联合概率分布.训练数据集

T =xx,yl),(x2,y1},--i{xN,yN)}

由独立同分布产生.

朴素贝叶斯法通过训练数据集学习联合概率分布p(A\y).具体地，学习以 下先验概率分布及条件概率分布.先验概率分布

p(r = CJ，k = \,2,-,K    (4.1)

条件概率分布

P(X = x\Y = ct) = P(XW = xm，--,Xw =    \Y = ck), Jfc = l，2,…，尤(4.2)

于是学习到联合概率分布P(X,Y).

条件概率分布= = 有指数级数量的参数，其估计实际是不可行 的.事实上，假设xw)可取值有久个，J = l，2,7可取值有尺个，那么参数 个数为尺

\>=>

朴素贝叶斯法对条件概率分布作了条件独立性的假设.由于这是一个较强的 假设，朴素贝叶斯法也由此得名.具体地，条件独立性假设是

P(X = XI y = cj = P(Xm = xm，- --,Xw=xw\Y = ck)

==xw |y = cj    (4.3)

朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型.条件独立 假设等于是说用于分类的特征在类确定的条件下都是条件独立的.这一假设使朴 素贝叶斯法变得简单，但有时会牺牲~定的分类准确率.

朴素贝叶斯法分类时，对给定的输入a通过学习到的模型计算后验概率分 布P(r = c*|X = ;c)，将后验概率最大的类作为的类输出.后验概率计算根据贝 叶斯定理进行：

P(Y — ck\X = x) =



/>Cv=xiy=c*)p(y=4)

(4.4)



^kP(X=x\Y = ck}P{Y = Ck)

将式(4.3)代入式(4.4)有

尸(F = Q)n P(Xw=xw) \Y = ck)

P(r = q | 义=x) =    = c，)n，(於＞ =沪,r = cJ，k = l,2,-,K (4.5)

这是朴素贝叶斯法分类的基本公式.于是，朴素贝叶斯分类器可表示为 "、    P(Y = ^Y[.P(X^ =xw)|K = Ci)

(4.6)

注意到，在式(4.6)中分母对所有q都是相同的，所以，

y = argnuxP(r = ct)JJyP(X(7) =x0) |K = c*)    (4.7)

4.1.2后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中.这等价于期望风险最小化.假 设选择0-1损失函数：

z(y,/W)=



1，

0，Y = f(X)



式中/(AT)是分类决策函数.这时，期望风险函数为

期望是对联合分布p(;r，y)取的.由此取条件期望

^cxp(/)= ^E[i(Ct,/(X))]P(Ci|X)

*«=1

为了使期望风险最小化，只需对AT = 逐个极小化，由此得到:

/(x) = aisro3n^L(ct,y)P(_ct\X = x)

K

=argmingP^ ^ck\X = x)

=argmin(l-P(j = ct]X=x)) =argm^tP(j = ck\X = x)

这样一来，根据期望风险最小化准则就得到了后验概率最大化准则:

/(x) = arg max P(ck \ X = x)

«*

即朴素贝叶斯法所采用的原理.

###### 4.2朴素贝叶斯法的参数估计 4.2.1极大似然估计

在朴素贝叶斯法中，学习意味着估计p(r=c*)和p(jv(/)=xw|r=c*).可以 应用极大似然估计法估计相应的概率.先验概率p(r=cj的极大似然估计是

N

£/(>,= C*)

P(Y = ck) = ^—^—, k = l,2,-,K    (4.8)

设第J个特征x(力可能取值的集合为｛a/1，cr'2,…，％ ｝，条件概率P(义⑺=a7；|y = ct) 的极大似然估计是

N

P(妙)=^(y=Ci)=-

X/(V/=c*)

j=i

_/ = 1，2,/ = 1，2,…，Sy k = l,2,--,K    (4.9)

式中，是第f个样本的第/个特征；心是第/个特征可能取的第/个值：/为 指示函数.

4.2.2学习与分类算法

下面给出朴素贝叶斯法的学习与分类算法.

算法4.1 (朴素贝叶斯算法(naive Bayes algorithm))

输入：训练数据7' = {(巧，乃)，(々，八)，一，(〜必)}，其中*«>，栌，一，垆)了， xf是第f个样本的第j•个特征，，％,••■，％}，士是第/•个特征可能取 的第/个值，/• = 1，2/..，《，1 = 1，2,…，Sj，yte {cltc2,---,cK}；实例x:

输出：实例;c的分类.

(1)    计算先验概率及条件概率

N

£/(y,=ct)

p(y=Ct)=J=i———,走=1，2，.•，尺 N

N

£/(x(<y)=oy„yj=c*)

*=i

7 = 1,2,l = l,2,---,Sj； k = l,2,---,K

(2)    对于给定的实例计算

P(r = cti[[P{X^ =^\Y = ct), k = 1,2,…，K

7=1

(3)    确定实例x的类

y = argmax尸(K = cjt)JJP(X(y) =xw) |y = ct)    ■

c,    j=i

例4.1试由表4.1的训练数据学习一个朴素贝叶斯分类器并确定jc = (2,5)t 的类标记jv.表中Xm, Xm为特征，取值的集合分别为岣={1,2,3}.^= {S,M,L}, :T 为类标记，Ke C = {1,-1}.

表4.1训练败据

| 1    | 2    | 3    4    5 | 6      | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |      |
| ---- | ---- | ----------- | ------ | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|      | 1    | 1           | 1 1 1  | 2    | 2    | 2    | 2    | 2    | 3    | 3    | 3    | 3    | 3    |
| xm   | s    | M           | MSS    | S    | M    | M    | L    | L    | L    | M    | M    | L    | L    |
| Y    | -1   | -1          | 1 1 -1 | -1   | -1   | 1    | 1    | 1    | 1    | 1    | 1    | 1    | -1   |

解根据算法4.1，由表4.1，容易计算下列概率:

p(y=i)=^, p(r=-i)=^

p(x(1)=i|y=i)=^, p(x(1)=2|y=i)=^, p(x0)=3|y=i)=^

P{Xm =S\Y = \)=^, P(Xm =M\Y = l) = ^, P(X(2)=£|y = l) = |

091

4.2.3贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况.这时会影响到后 验概率的计算结果，使分类产生偏差.解决这一问题的方法是采用贝叶斯估计.具 体地，条件概率的贝叶斯估计是

N

£/(jcy)=u,=c*)+A

p人於、=a, |r=cj=-«=Hj- (4.io)

1=1

式中A彡0 .等价于在随机变量各个取值的频数上赋予一个正数A>0 .当A = 0时 就是极大似然估计.常取2 = 1，这时称为拉普拉斯平滑(Laplace smoothing).显 然，对任何/ = 1，2,…，Sy, * = 1,2,…，尺，有

Px(Xw=ay;|K = ct)>0

^P^=aj,\Y = ck) = l z=i

表明式(4.10)确为一种概率分布.同样，先验概率的贝叶斯估计是 乂

P^Y = c,) = ^-

(4.11)



例4.2问题同例4.1，按照拉普拉斯平滑估计概率，即取2 = 1.

解 4 =0»2,3}» 為= {S，A/，Z}，C = {1,-1}.按照式(4.10)和式(4.11)计算下

列概率：

p(r=i)=畀，p(r=-i)=^

p(zr(1)=i|7=i)=^,    p(jt(1)=2|k=i)=^,尸(jr(1)=3|y=i)=^|

P{XW =S\Y = Y) = ~^, ,P(Xm =M\Y = X) = ^ , P(X(2)=£|K = 1) = ^ p(a■⑴=i|y=_i)=吾，p(x(1)=2|y=-i)=|, p(^(1)=3|y=-i)=| p(x(2)=s,|r=-i)=^, p(x(2) = a/|k=-i)=|, p(j：(2)=L|r=-i)=|

对于给定的* = (2, S)T计算：

p(y=i)p(x(l) =2|y=l)尸    =sir=i)=—=0.0327

v    1    17 12 12 153

P(Y = -1)P(X(1) =2|K = -1)P(X(2) =S|y = -l) = -^-.|.^ = -^ = 0.0610 *T= 21 y=-i)p(>r(2) = 5( y=-1)所以    ■

[1](#footnote1)

0    0

[2](#footnote2)

   3x(1,+3x<J)+l

0    2x(n+2xm

-1    xm+xm-l

-2    -2

-1    3x(,)+3xw-l

-2    2xin + 2xm~2

-3    x^+xm-3

-3    W>-3





###### 本章概要

\1.    朴素贝叶斯法是典型的生成学习方法.生成方法由训练数据学习联合概 率分布/>cv，y),然后求得后验概率分布p(r|jr).具体来说，利用训练数据学习

I /)和p(y)的估计，得到联合概率分布：

p(^,y)=p(y)p(x|r)

概率估计方法可以是极大似然估计或贝叶斯估计：

\2.    朴素贝叶斯法的基本假设是条件独立性，

P{X = x\Y = ck} = P{Xm =xm，■-,XW =xw\Y = ck)

= nP(X0)=xw)|7 = ct)

'•=1

这是一个较强的假设.由于这--假设，模型包含的条件概率的数量大为减少，朴 素贝叶斯法的学习与预测大为简化.因而朴素贝叶斯法髙效，且易于实现.其缺

点是分类的性能不一定很髙.

3.朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测.

P(y|X)=wn= wwji.

p(x)

r

将输入分到后验概率最大的类y.

y = argmaxP(y = c*)p[P(Ar/ =    | Y = ct)

Ck »

后验概率最大等价于0-1损失函数时的期望风险最小化.

###### 继续阅读

朴素贝叶斯法的介绍可见文献[1，2].朴素贝叶斯法中假设输入变量都是条件 独立的，如果假设它们之间存在概率依存关系，模型就变成了贝叶斯网络，参见 文献[3].

###### 习 题

4.1用极大似然估计法推出朴素贝叶斯法中的概率估计公式(4.8)及公式(4.9). 4.2用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式(4.10)及公式(4.11).

###### 参考文献

[1]    Mitchell TM. Chapter 1: Generative and discriminative classifiers: Naive Bayes and logistic regression.

In: Machine Learning. Draft, 2005. <http://www.cs.cmu.edu/~tom/mlbook/NBayeslogReg.pdf>

[2]    Hastie T, Tibshirani R, Friedman X The Elements of Statistical Learning. Data Mining, Inference, and Prediction. Springer-Verlag, 2001(中译本:统计学习基础——数据挖掘、推理与预测.范 明，柴玉梅，昝红英等译.北京：电子工业出版社，2004)

[3】Bishop C. Pattern Recognition and Machine Learning, Springer, 2006
