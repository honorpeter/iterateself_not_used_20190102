---
title: 05 决策树
toc: true
date: 2018-06-26 19:22:04
---
##### 第5章决策树

决策树(decision tree)是一种基本的分类与回归方法.本章主要讨论用于分类 的决策树.决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类 的过程.它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空 间上的条件概率分布.其主要优点是模型具有可读性，分类速度快.学习时，利用 训练数据，根据损失函数最小化的原则建立决策树模型.预测时，对新的数据，利 用决策树模型进行分类.决策树学习通常包括3个步骤：特征选择、决策树的生成 和决策树的修剪.这些决策树学习的思想主要来源于由Quinlan在1986年提出的ID3 算法和1993年提出的C4.5算法，以及由Breiman等人在1984年提出的CART算法.

本章首先介绍决策树的基本概念，然后通过ID3和C4.5介绍特征的选择、决 策树的生成以及决策树的修剪，最后介绍CART算法.

###### 5.1决策树模型与学习

5.1.1决策树模型

定义5.1 (决策树〉分类决策树模型是一种描述对实例进行分类的树形结 构.决策树由结点(node)和有向边(directed edge)组成.结点有两种类型：内 部结点(internal node)和叶结点(leafnode).内部结点表示一个特征或属性，叶 结点表示一个类.

用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果， 将实例分配到其子结点：这时，每一个子结点对应着该特征的一个取值.如此递 归地对实例进行测试并分配，直至达到叶结点.最后将实例分到叶结点的类中.

图5.1是一个决策树的示意图.图中圆和方框分别表示内部结点和叶结点.

图5.1决策树模型

5.1.2决策树与if-then规则

可以将决策树看成一个if-then规则的集合.将决策树转换成if-then规则的 过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上 内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论.决策树的 路径或其对应的if-then规则集合具有一个重要的性质:互斥并且完备.这就是说， 每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆 盖.这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件.

5.1.3决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布.这一条件概率分布定义在 特征空间的一个划分（partition）上.将特征空间划分为互不相交的单元（cell）或 区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分 布.决策树的一条路径对应于划分中的一个单元.决策树所表示的条件概率分布 由各个单元给定条件下类的条件概率分布组成.假设AT为表示特征的随机变量， F为表示类的随机变量，那么这个条件概率分布可以表示为/＞（7|尤）.I取值于 给定划分下单元的集合，y取值于类的集合.各叶结点（单元）上的条件概率往 往偏向某一个类，即属于某一类的概率较大.决策树分类时将该结点的实例强行 分到条件概率大的那一类去.

图5.2 （a）示意地表示了特征空间的一个划分.图中的大正方形表示特征空 间.这个大正方形被若干个小矩形分割，每个小矩形表示一个单元.特征空间划 分上的单元构成了一个集合，X取值为单元的集合.为简单起见，假设只有两 类：正类和负类，即F取值为+1和-1.小矩形中的数字表示单元的类.图5.2（b） 示意地表示特征空间划分确定时，特征（单元）给定条件下类的条件概率分布. 图5.2 （b）中条件概率分布对应于图5.2 （a）的划分.当某个单元c的条件概率满 足P（r = +l|X = c）＞0.5时，则认为这个单元属于正类，即落在这个单元的实例 都被视为正例■图5.2 （c）为对应于图5.2 （b）中条件概率分布的决策树.

5.1.4决策树学习

决策树学习，假设给定训练数据集

打=｛（巧，乃），（々，乃），• • •，（〜，々）｝

其中，〜=（#，#，…，x;w）T为输入实例（特征向量），„为特征个数，

为类标记，f = l，2,…，况，#为样本容量.学习的目标是根据给定的训练数据集构 建一个决策树模型，使它能够对实例进行正确的分类.

X®-



a3



+1



(b)条件概率分布



(a)特征空间划分



a, 1    x«)

图5.2决策树对应于条件概率分布



决策树学习本质上是从训练数据集中归纳出一组分类规则.与训练数据集不 相矛盾的决策树(即能对训练数据进行正确分类的决策树)可能有多个，也可能 一个也没有.我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的 泛化能力.从另一个角度看，决策树学习是由训练数据集估计条件概率模型.基 于特征空间划分的类的条件概率模型有无穷多个.我们选择的条件概率模型应该 不仅对训练数据有很好的拟合，而且对未知数据有很好的预测.

决策树学习用损失函数表示这一目标.如下所述，决策树学习的损失函数通常 是正则化的极大似然函数.决策树学习的策略是以损失函数为目标函数的最小化.

当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的 问题.因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中 决策树学习算法通常采用启发式方法，近似求解这一最优化问题.这样得到的决 策树是次最优(sub-optimal)的.

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数 据进行分割，使得对各个子数据集有一个最好的分类的过程.这一过程对应着对 特征空间的划分，也对应着决策树的构建.开始，构建根结点，将所有训练数据 都放在根结点.选择一个最优特征，按照这一特征将训练数据集分割成子集，使 得各个子集有一个在当前条件下最好的分类.如果这些子集己经能够被基本正确 分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集 不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割， 构建相应的结点.如此递归地进行下去，直至所有训练数据子集被基本正确分类， 或者没有合适的特征为止.最后每个子集都被分到叶结点上，即都有了明确的 类.这就生成了一棵决策树.

以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试 数据却未必有很好的分类能力，即可能发生过拟合现象.我们需要对已生成的树 自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力.具体地， 就是去掉过于细分的叶结点，使其回退到父结点，甚至更髙的结点，然后将父结 点或更髙的结点改为新的叶结点.

如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只 留下对训练数据有足够分类能力的特征.

可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过 程.由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度 的概率模型.决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全 局选择.决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优.

决策树学习常用的算法有ID3、C4.5与CART,下面结合这些算法分别叙述 决策树学习的特征选择、决策树的生成和剪枝过程.

###### 5.2特征选择

5.2.1特征选择问题

特征选择在于选取对训练数据具有分类能力的特征.这样可以提髙决策树学 习的效率.如果利用~个特征进行分类的结果与随机分类的结果没有很大差别， 则称这个特征是没有分类能力的.经验上扔掉这样的特征对决策树学习的精度影 响不大.通常特征选择的准则是信息増益或信息增益比.

首先通过一个例子来说明特征选择问题.

例5.1®表5.1是一个由15个样本组成的贷款申请训练数据.数据包括贷款 申请人的4个特征（属性）：第1个特征是年龄，有3个可能值：青年，中年，老 年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子， 有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一 般.表的最后一列是类别，是否同意贷款，取2个值：是，否.

①此例取自参考文献[5].

| 表5.1贷款申请样本数据表 |      |        |              |          |      |
| ----------------------- | ---- | ------ | ------------ | -------- | ---- |
| ID                      | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |
| 1                       | 青年 | 否     | 否           | 一般     | 否   |
| 2                       | 青年 | 否     | 否           | 好       | 否   |
| 3                       | 青年 | 是     | 否           | 好       | 是   |
| 4                       | 青年 | 是     | 是           | -般      | 是   |
| 5                       | 青年 | 否     | 否           | 一般     | 否   |
| 6                       | 中年 | 否     | 否           | 一般     | 否   |
| 7                       | 中年 | 否     | 否           | 好       | 否   |
| 8                       | 中年 | 是     | 是           | 好       | 是   |
| 9                       | 中年 | 否     | 是           | 非常好   | 是   |
| 10                      | 中年 | 否     | 是           | 非常好   | 是   |
| 11                      | 老年 | 否     | 是           | 非常好   | 是   |
| 12                      | 老年 | 否     | 是           | 好       | 是   |
| 13                      | 老年 | 是     | 否           | 好       | 是   |
| 14                      | 老年 | 是     | 否           | 非常好   | 是   |
| 15                      | 老年 | 否     | 否           | -般      | 否   |

希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申 请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定 是否批准贷款申请.    ■

特征选择是决定用哪个特征来划分特征空间.

图5.3表示从表5.1数据学习到的两个可能的决策树，分别由两个不同特征 的根结点构成.图5.3 (a)所示的根结点的特征是年龄，有3个取值，对应于不同 的取值有不同的子结点.图5.3 (b)所示的根结点的特征是有工作，有2个取值， 对应于不同的取值有不同的子结点.两个决策树都可以从此延续下去.问题是： 究竟选择哪个特征更好些？这就要求确定选择特征的准则.直观上，如果一个特 征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得 各个子集在当前条件下有最好的分类，那么就更应该选择这个特征.信息增益 (information gain)就能够很好地表示这一直观的准则.

⑻    (b)

图5.3不同特征决定的不同决策树

5.2.2信息増益

为了便于说明，先给出熵与条件熵的定义.

在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量.设 X是一个取有限个值的离散随机变量，其概率分布为

P{X = xi) = pi, i = \,2, -,n

则随机变量AT的熵定义为

= ~^p, \ogpl    (5.1)

i-l

在式(5.1)中，若p,=0,贝IJ定义01og0 = 0.通常，式(5.1)中的对数以2为底或 以e为底(自然对数)，这时熵的单位分别称作比特(bit)或纳特(nat).由定义可 知，熵只依赖于I的分布，而与义的取值无关，所以也可将Z的熵记作2/(p),即

这时，熵H(p)随概率p变化的曲线如图5.4所示(单位为比特).

图5.4分布为贝努利分布时熵与概率的关系

当p = 0或p = l时7/(p) = 0，随机变量完全没有不确定性.当夕=0.5时， H(p) = l,熵取值最大，随机变量不确定性最大.

设有随机变量(％,7),其联合概率分布为

P(X = x„Y = yj) = pv ,    !’ = 1，2,…，n; j = l,2,---,m

条件熵H(Y\X)表示在已知随机变量X的条件下随机变量y的不确定性.随机变 量义给定的条件下随机变量y的条件熵(conditional entropy) H(Y\X),定义为 X给定条件下r的条件概率分布的熵对Z的数学期望

H(Y l^) = Z P々(Y\X = xt)    (5.5)

(=i

这里，pt =P｛X = x,)» »' = 1，2,…，n .

当熵和条件嫡中的概率由数据估计(特别是极大似然估计〉得到时，所对应 的熵与条件熵分别称为经验嫡(empirical entropy)和经验条件熵(empirical conditional entropy).此时，如果有 0 概率，令 01og0 = 0.

信息增益(information gain)表示得知特征又的信息而使得类F的信息的不 确定性减少的程度.

定义5.2 (信息增益)特征J对训练数据集P的信息增益g(D,A),定义为 集合D的经验熵H(D)与特征Z给定条件下D的经验条件熵H(D M)之差，即

g(D,A)^H(D)-H(D\A)    (5.6)

一•般地，熵//(/)与条件熵7/(门义)之差称为互信息(mutual information).决 策树学习中的信息增益等价于训练数据集中类与特征的互信息.

决策树学习应用信息增益准则选择特征.给定训练数据集D和特征X,经验 熵ff(D)表示对数据集D进行分类的不确定性.而经验条件熵H｛D\A｝表示在特 征乂给定的条件下对数据集D进行分类的不确定性.那么它们的差，即信息增益， 就表示由于特征乂而使得对数据集£＞的分类的不确定性减少的程度.显然，对于 数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益.信 息増益大的特征具有更强的分类能力.

根据信息增益准则的特征选择方法是：对训练数据集(或子集)D,计算其 每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征.

设训练数据集为£»，|D|表示其样本容量，即样本个数.设有夂个类C*，女= 1,2,…，A：，为属于类的样本个数，^\Ck\ = \D\.设特征W有n个不同的 取值｛a,，〜•.•，《”｝，根据特征义的取值将分为”个子集，…，/)„，|D,|为 的样本个数，=    记子集马中属于类C*的样本的集合为仏，即

iz^i为仏的样本个数.于是信息増益的算法如下：

算法S.1 (信息增益的算法)

输入：训练数据集£＞和特征

输出：特征J对训练数据集D的信息增益g(23,冯.

(1)    计算数据集2?的经验熵"(D)

，卜微。g2贸    (5-7)

(2)    计算特征W对数据集D的经验条件嫡汉(DM)

![img](2012.4e2a-29.jpg)



(3)计算信息增益

g(ZM) = H{D)~ H{D\A)    (5.9) ■

例5.2对表5.1所给的训练数据集P,根据信息增益准则选择最优特征.

解首先计算经验熵好(£0.

H(D) = -—log, — log,A = 0.971 15 6215 15 5215

然后计算各特征对数据集D的信息增益.分别以J,，A,,    4表示年龄、

有工作、有自己的房子和信贷情况4个特征，则

(1)

g(D,^) = H(D)~

5< 2.    2 3,    3)

iiC5logj?-?log2i)

| (3.  | 3    | 2,    | 2、 5 I | ’ 4    4 1    lYI |
| ---- | ---- | ----- | ------- | ----------------- |
|      | g2?_ | --lo: |         |                   |



= 0.971-

= 0.971-0.888 = 0.083

这里D,, D2, P3分别是D中莩(年龄)取值为青年、中年和老年的样本子集.类 似地，

(2)

(4)

g(D,^) = 0.971- 0.608 = 0.363

最后，比较各特征的信息增益值.由于特征4,(有自己的房子)的信息增益 值最大，所以选择特征嶋作为最优特征.    ■

5.2.3信息増益比

信息增益值的大小是相对于训练数据集而言的，并没有绝对意义.在分类问 题困难时，也就是说在训练数据集的经验熵大的时候，信息増益值会偏大.反之， 信息增益值会偏小.使用信息增益比(information gain ratio)可以对这一问题进 行校正.这是特征选择的另一准则.

定义5.3 (倌息壜益比)特征4对训练数据集D的信息增益比定义 为其信息增益g(D,A)与训练数据集的经验熵H{D)之比：

gR{D,A) =

(5.10)



###### 5.3决策树的生成

本节将介绍决策树学习的生成算法.首先介绍ID3的生成算法，然后再介绍 C4.5中的生成算法.这些都是决策树学习的经典算法.

53.1 ID3 算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地 构建决策树.具体方法是：从根结点(root node)开始，对结点计算所有可能的 特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取 值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的 信息增益均很小或没有特征可以选择为止.最后得到一个决策树.ID3相当于用 极大似然法进行概率模型的选择.

算法5.2 (ID3算法)

输入：训练数据集n,特征集阈值

输出：决策树r.

(1〉若中所有实例属于同一类c*，则r为单结点树，并将类c*作为该结 点的类标记，返回r:

(2)若4 = 0,则r为单结点树，并将£＞中实例数最大的类作为该结点的 类标记，返回r:

(3)    否则，按算法5.1计算4中各特征对£＞的信息増益，选择信息増益最大 的特征么；

(4)    如果 ＜ 的信息增益小于阈值f,则置r为单结点树，并将中实例数最 大的类c*作为该结点的类标记，返回r;

(5)    否则，对4的每一可能值巧，依4=4将D分割为若干非空子集D：，将 忍中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树r，返回r;

(6)    对第/个子结点，以£＞：为训练集，以为特征集，递归地调用

步(1)〜步(5)，得到子树7；,返回7；.    ■

例5.3对表5.1的训练数据集，利用ID3算法建立决策树.

解利用例5.2的结果，由于特征為(有自己的房子)的信息增益值最大，所

以选择特征為作为根结点的特征.它将训练数据集2?划分为两个子集孕(為取 值为“是”)和马(為取值为“否”).由于马只有同一类的样本点，所以它成 为一个叶结点，结点的类标记为“是

对马则需从特征J,(年龄)，為(有工作)和次(信贷情况)中选择新的特 征.计算各个特征的信息增益：

g(D2,^) = ff(D2)-ff(D2\ 4)= 0.918-0.667 = 0.251 g(D2, 乂)= ff(D2)~ ff(D2|4)= 0.918 g(D2,A,) = H(D2)-H(D2[A,) = 0.474

选择信息增益最大的特征J,(有工作)作为结点的特征.由于為有两个可能取 值，从这一结点引出两个子结点：一个对应“是”(有工作)的子结点，包含3 个样本，它们属于同-•类，所以这是一个叶结点，类标记为“是1另一个是对 应“否”(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是 一个叶结点，类标记为“否”.

这样生成一个如图5.5所示的决策树.该决策树只用了两个特征(有两个内 部结点).    ■

ID3算法只有树的生成，所以该算法生成的树容易产生过拟合.

53.2 C4.S的生成算法

C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进.C4.5在生成 的过程中，用信息增益比来选择特征.

算法5.3 (C4.S的生成算法)

输入：训练数据集2),特征集阈值 输出：决策树r.

(1)    如果D中所有实例属于同一类则置r为单结点树，并将C*作为该 结点的类，返回r;

(2)    如果j=0,则置r为单结点树，并将d中实例数最大的类c*作为该结 点的类，返回

(3)    否则，按式(5.10)计算J中各特征对的信息增益比，选择信息增益比 最大的特征

(4)    如果人的信息增益比小于阈值£,则置r为单结点树，并将d中实例数 最大的类c*作为该结点的类，返回r;

(5)    否则，对4的每一可能值叫，依将£>分割为子集若干非空只，将 D,中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树r,返回r:

(6)    对结点*•,以马为训练集，以J-{<}为特征集，递归地调用步(1)〜步(5),

得到子树7；，返回7；.    ■

###### 5.4决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止.这样产生的树 往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即 出现过拟合现象.过拟合的原因在于学习时过多地考虑如何提高对训练数据的正 确分类，从而构建出过于复杂的决策树.解决这个问题的办法是考虑决策树的复 杂度，对已生成的决策树进行简化.

在决策树学习中将已生成的树进行简化的过程称为剪枝(pruning).具体地， 剪枝从已生成的树上裁掉~些子树或叶结点，并将其根结点或父结点作为新的叶 结点，从而简化分类树模型.

本节介绍一种简单的决策树学习的剪枝算法.

决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)或代价 函数(costfunction)来实现.设树r的叶结点个数为|T|, Z是树r的叶结点，该

叶结点有\个样本点，其中*类的样本点有久个，k = l,2, -,K, H,(r)为叶结 点r上的经验熵，a>0为参数，则决策树学习的损失函数可以定义为

ca(r)=^N,/f,(r)+airi    (5.11)

/=i

其中经验熵为





## 聊 sx    (5>2)

在损失函数中，将式(5.11)右端的第I项记作

C(T) =    = Hx log 令    (5.13)

<=i    »-i *=i

这时有

cff(r)=c(r)+«|r|    (5.14)

式(5.14)中，c(r)表示模型对训练数据的预测误差，即模型与训练数据的拟合程 度，|7|表示模型复杂度，参数a>0控制两者之间的影响.较大的《促使选择较 简单的模型(树)，较小的a促使选择较复杂的模型(树).a = 0意味着只考虑 模型与训练数据的拟合程度，不考虑模型的复杂度.

剪枝，就是当or确定时，选择损失函数最小的模型，即损失函数最小的子 树.当a值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度 就越髙；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不 好.损失函数正好表示了对两者的平衡.

可以看出，决策树生成只考虑了通过提髙信息增益(或信息增益比〉对训练 数据进行更好的拟合.而决策树剪枝通过优化损失函数还考虑了减小模型复杂 度.决策树生成学习局部的模型，而决策树剪枝学习整体的模型.

式(5.11)或式(5.14)定义的损失函数的极小化等价于正则化的极大似然估 计.所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模 型选择.

图5.6是决策树剪枝过程的示意图.下面介绍剪枝算法.

算法S.4 (树的剪枝算法)

输入：生成算法产生的整个树r,参数

输出：修剪后的子树

(1)    计算每个结点的经验熵.

(2)    递归地从树的叶结点向上回缩.

设一组叶结点回缩到其父结点之前与之后的整体树分别为7；与7；,其对应 的损失函数值分别是仏(&)与€?„(7；)，如果

Ca(TA)^Ca(TB)    (5.15)

则进行剪枝，即将父结点变为新的叶结点.

(3)返回(2)，直至不能继续为止，得到损失函数最小的子树ra.    ■

注意，式(5.15)只需考虑两个树的损失函数的差，其计算可以在局部进行.所

以，决策树的剪枝算法可以由一种动态规划的算法实现.类似的动态规划算法可 参见文献[10].

###### 5.5 CART 算法

分类与回归树(classification and regression tree, CART)模型由 Breiman 等 人在1984年提出，是应用广泛的决策树学习方法.CART同样由特征选择、树 的生成及剪枝组成，既可以用于分类也可以用于回归.以下将用于分类与回归的 树统称为决策树.

CART是在给定输入随机变量Z条件下输出随机变量F的条件概率分布的 学习方法.CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左 分支是取值为“是”的分支，右分支是取值为“否”的分支.这样的决策树等价于 递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元 上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布.

CART算法由以下两步组成：

(1)决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；

(2)决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这 时用损失函数最小作为剪枝的标准.

5.5.1 CART 生成

决策树的生成就是递归地构建二叉决策树的过程.对回归树用平方误差最小化 准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树.

1.回归树的生成

假设x与y分别为输入和输出变量，并且r是连续变量，给定训练数据集 D = {(xl,y1),(x2>yi)>--,(xN,yN)}

考虑如何生成回归树.

一个回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元上 的输出值.假设己将输入空间划分为M个单元氏凡，…，Rm,并且在每个单元，上 有一个固定的输出值么，于是回归树模型可表示为

u

/W = £cM/(xe7?„)    (5.16)

当输入空间的划分确定时，可以用平方误差之；U-/(xJ)2来表示回归树对

于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出 值.易知，单元圪上的，的最优值<是圪上的所有输入实例七对应的输出只的 均值，即

(5.17)

问题是怎样对输入空间进行划分.这里采用启发式的方法，选择第y个变量 x(/)和它取的值•?，作为切分变量(splittingvariable)和切分点(splittingpoint)， 并定义两个区域，

R,(J，s) = {x\xv^s}和 ^(y,s) = {x|x0)>5}    (5.18)

然后寻找最优切分变量y和最优切分点具体地，求解

min min (^-c^+min 乙(y(-c2)2    (5.19)

J，S L    01 桃(M    」

对固定输入变量J可以找到最优切分点I

$ = ave(yt IX, e    和 c2=ave(y,\x,e(5.20)

遍历所有输入变量，找到最优的切分变量_/，构成一个对0,■?).依此将输入空间 划分为两个区域.接着，对每个区域重复上述划分过程，直到满足停止条件为 止.这样就生成一棵回归树.这样的回归树通常称为最小二乘回归树(least squares regression tree),现将算法叙述如下：

算法5.S (最小二乘回归树生成算法)

输入：训练数据集 输出：回归树/CO.

在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决 定每个子区域上的输出值，构建二叉决策树：

(1)    选择最优切分变量;/与切分点求解

min min L U-c,)2+min    (5.21)

遍历变量对固定的切分变量y扫描切分点j，选择使式(5.21)达到最小 值的对(人j).

(2)    用选定的对C/，s)划分区域并决定相应的输出值：

Rl(j,s) = {x\xu) ^s}, J?z(7» = {x|x0) >5}

Z xeRm' w=1，2

(3)    继续对两个子区域调用步骤(1), (2),直至满足停止条件.

(4)    将输入空间划分为A/个区域代，氏，…，心，生成决策树：

M

⑽氏)    ■

2.分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点.

定义5.4 (基尼指数)分类问题中，假设有个类，样本点属于第A类的概

率为A，则概率分布的基尼指数定义为

Gini(p) = 2 A (1 - A ) = 1 - A2    (5.22)

*=i    *=i

对于二类分类问题，若样本点属于第1个类的概率是p,则概率分布的基尼指数为 Gini(p) = 2p(l-p)    (5.23)

对于给定的样本集合£»，其基尼指数为

Gini(D) = l-g(贸)    (5.24)

这里，C*是Z)中属于第t类的样本子集，尺是类的个数.

如果样本集合D根据特征j是否取某一可能值a被分割成和D2两部分，即 D' = {(x, j) e D | A(x) = a}, D2 =D-DX

则在特征W的条件下，集合D的基尼指数定义为

Gini(2M) = l^Gmi(D1)+l^Gini(D2)    (5.25)

基尼指数Gini(Z))表示集合的不确定性，基尼指数Gini(D,>4)表示经A = a分割 后集合D的不确定性.基尼指数值越大，样本集合的不确定性也就越大，这一点 与熵相似.

图5.7显示二类分类问题中基尼指数Gini(p)、熵(单位比特)之半和

分类误差率的关系.横坐标表示概率p，纵坐标表示损失.可以看出基尼指数和 熵之半的曲线很接近，都可以近似地代表分类误差率.

图5.7二类分类中基尼指数、熵之半和分类误差率的关系

算法5.6 (CART生成算法)

输入：训练数据集£>,停止计算的条件；

输出：CART决策树.

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二 叉决策树：

(1)    设结点的训练数据集为2),计算现有特征对该数据集的基尼指数.此时， 对每一个特征J，对其可能取的每个值a，根据样本点对W = 的测试为“是”或 “否”将2)分割成马和马两部分，利用式(5.25)计算J = «时的基尼指数.

(2)    在所有可能的特征J以及它们所有可能的切分点a中，选择基尼指数最 小的特征及其对应的切分点作为最优特征与最优切分点.依最优特征与最优切分 点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去.

(3)    对两个子结点递归地调用(1), (2)，直至满足停止条件.

(4)    生成CART决策树.    ■

算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指

数小于预定阈值(样本基本属于同一类)，或者没有更多特征.

例5.4根据表5.1所给训练数据集’应用CART算法生成决策树.

解首先计算各特征的基尼指数，选择最优特征以及其最优切分点.仍采用

例5.2的记号，分别以4,    A,, 表示年龄、有工作、有自己的房子和信贷

情况4个特征，并以1, 2, 3表示年龄的值为青年、中年和老年，以1，2表示 有工作和有自己的房子的值为是和否，以1, 2, 3表示信贷情况的值为非常好、好 和一般.

求特征的基尼指数：

咖叫4 •釙+〔4))+徵2+〔,-却=。.、4

Gini(Z>，4=2) = 0.48 Gini(D，4 =3) = 0.44

由于Gini(D，J,=l)和Gini(D，4=3)相等，且最小，所以矣=1和=3都可 以选作忒的最优切分点.

求特征 < 和必的基尼指数：

0如(£),次=1) = 0.32 Gioi(Z)，為=1) = 0.27

由于4和斗只有一个切分点，所以它们就是最优切分点.

求特征么的基尼指数：

Gini(O,4 =1) = 0.36 Gini(£), ^=2) = 0.47 Gmi(£>, A4=3) = 0.32

Gini(D,4, = 3)最小，所以屿=3为么的最优切分点.

在4,為，4,馮几个特征中，Gini(D，為=1) = 0.27最小，所以选择特征為为

最优特征，為=1为其最优切分点.于是根结点生成两个子结点,一个是叶结点.对 另一个结点继续使用以上方法在4,為，4中选择最优特征及其最优切分点，结 果是次=1.依此计算得知，所得结点都是叶结点.    ■

对于本问题，按照CART算法所生成的决策树与按照ID3算法所生成的决策 树完全•一致.

5.5.2 CART 剪枝

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小 (模型变简单)，从而能够对未知数据有更准确的预测.CART剪枝算法由两步组 成：首先从生成算法产生的决策树7；底端开始不断剪枝，直到rQ的根结点，形成 一个子树序列;然后通过交叉验证法在独立的验证数据集上对子树 序列进行测试，从中选择最优子树.

1.剪枝，形成一个子树序列

在剪枝过程中，计算子树的损失函数：

Co(D = C(r)+a|r|    (5.26)

其中，r为任意子树，c(r)为对训练数据的预测误差(如基尼指数)，|r|为子 树的叶结点个数，为参数，c„(r)为参数是a时的子树r的整体损失.参 数a权衡训练数据的拟合程度与模型的复杂度.

对固定的《，一定存在使损失函数cB(r)最小的子树，将其表示为在 损失函数(7„(7)最小的意义下是最优的.容易验证这样的最优子树是唯一的.当 «大的时候，最优子树ra偏小；当a小的时候，最优子树偏大.极端情况，当 a = 0时，整体树是最优的.当时，根结点组成的单结点树是最优的.

Breiman等人证明：可以用递归的方法对树进行剪枝.将a从小增大，0 = a0<a1<••-<an<-¥<»,产生一系列的区间吹，^)，/。。，!^…，”；剪枝得到的子树 序列对应着区间ae[a,，aw)，/ = 0，1，“.，《的最优子树序列｛7；，7；，".,7；｝，序列中 的子树是嵌套的.

具体地，从整体树rfl开始剪枝.对rQ的任意内部结点以/为单结点树的 损失函数是

Ca(t) = C(t)+a    (5.27)

以Z为根结点的子树7；的损失函数是

Ca(T,) = C(Tl)+a\Tl\    (5.28)

当a = 0及a充分小时，有不等式

Ca(T,)<Ca(t)    (5.29)

当a增大时，在某一 a有

Ca(T,) = Ca(.t)    (5.30)

当a再增大时，不等式(5.29)反向.只要,〒)，7；与/有相同的损 1乃卜1

失函数值，而f的结点少，因此f比7；更可取，对7；进行剪枝.

为此，对7；中每一内部结点f，计算 它表示剪枝后整体损失函数减少的程度.在ra中剪去沙)最小的7；，将得到的子 树作为7；，同时将最小的g(r)设为叫.7；为区间［a,，a2)的最优子树.

g(0 =



17； |-1



(5.31)



如此剪枝下去，直至得到根结点.在这一过程中，不断地增加a的值，产生 新的区间.

2.在剪枝得到的子树序列T0,Tir-,T„中通过交叉验证选取最优子树7；

具体地，利用独立的验证数据集，测试子树序列ra，?;，...，r„中各棵子树的平 方误差或基尼指数.平方误差或基尼指数最小的决策树被认为是最优的决策 树.在子树序列中，每棵子树?;，r2，…，都对应于一个参数叫，叫所以，当 最优子树7；确定时，对应的叫也确定了，即得到最优决策树r„.

现在写出CART剪枝算法.

算法5.7 (CART剪枝算法)

输入：CART算法生成的决策树r0;

输出：最优决策树r„.

(1)    设灸=0，T = T0.

(2)    设ar=+<» •

(3)    自下而上地对各内部结点f计算C(7；), |r，|以及

g(0 =



c⑺一哪

W-1



ar=min(a,g(0)



这里，2；表示以f为根结点的子树，C(7；)是对训练数据的预测误差，|7；|是7；的 叶结点个数.

(4)    自上而下地访问内部结点n如果有g(f) = a，进行剪枝，并对叶结点f 以多数表决法决定其类，得到树T.

(5)    设女=灸+ 1，ak—a, Tk=T.

(6)    如果r不是由根结点单独构成的树，则回到步骤(4).

(7)    采用交叉验证法在子树序列^，…，rn中选取最优子树r„.    ■

###### 本章概要

1.分类决策树模型是表示基于特征对实例进行分类的树形结构.决策树可 以转换成一个if-then规则的集合，也可以看作是定义在特征空间划分上的类的条

件概率分布.

\2.    决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策 树.因为从可能的决策树中直接选取最优决策树是NP完全问题.现实中采用启 发式方法学习次优的决策树.

决策树学习算法包括3部分：特征选择、树的生成和树的剪枝.常用的算法 有 ID3、C4.5 和 CART.

\3.    特征选择的目的在于选取对训练数据能够分类的特征.特征选择的关键 是其准则.常用的准则如下：

(1)    样本集合£＞对特征4的信息增益(ID3)

g(D,A) = H(D)-H(D\A)

## 雌,1。喵

1=1 \u\

其中，//(£))是数据集£＞的熵，77(D,)是数据集O,的熵，是数据集D对 特征4的条件熵.马是D中特征X取第f个值的样本子集，C*是D中属于第 类的样本子集.》是特征4取值的个数，是类的个数.

(2)    样本集合£＞对特征J的信息增益比(C4.5)

其中，g(Z),⑷是信息增益，//(Z?)是数据集Z)的熵.

(3)样本集合Z)的基尼指数(CART)

•卜聽)

特征J条件下集合D的基尼指数：

Gini(£U) = ^Gini(D,) + ^Gini(D2) lDl    lDl

\4.    决策树的生成.通常使用信息増益最大、信息增益比最大或基尼指数最 小作为特征选择的准则.决策树的生成往往通过计算信息增益或其他指标，从根 结点开始，递归地产生决策树.这相当于用信息增益或其他准则不断地选取局部 最优的特征，或将训练集分割为能够基本正确分类的子集.

\5.    决策树的剪枝.由于生成的决策树存在过拟合问题，需要对它进行剪枝，以 简化学到的决策树.决策树的剪枝，往往从己生成的树上剪掉一些叶结点或叶结 点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树.

###### 继续阅读

介绍决策树学习方法的文献很多，关于ED3可见文献［1］，C4.5可见文献［2］， CART可见文献［3,4］.决策树学习一般性介绍可见文献［5〜7］.与决策树类似的 分类方法还有决策列表(decisionlist).决策列表与决策树可以相互转换［8］，决策 列表的学习方法可参见文献［9］.

习 题

5.1根据表5.1所给的训练数据集，利用信息增益比(C4.5算法)生成决策树. 5.2己知如表5.2所示的训练数据，试用平方误差损失准则生成一个二叉回归树.

表5.2训练数据表

| xt   | i    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| y,   | 4.50 | 4.75 | 4.91 | 5.34 | 5.80 | 7.05 | 7.90 | 8.23 | 8.70 | 9.00 |

5.3证明CART剪枝算法中，当a确定时，存在唯一的最小子树7；使损失函数 最小.

5.4证明CART剪枝算法中求出的子树序列｛r0,r„-,rB｝分别是区间ae ［aitaM) 的最优子树L，这里/ = 0,1，.",《，0 = a。＜叫.

###### 参考文献

J•- ^2 - 3- 4 5[671



Quinlan JR. Induction of decision trees. Machine Learning, 1986,1(1): 81-106 Quinlan JR. C4. 5: Programs for Machine Learning. Morgan Kaufinann, 1992 Breiman L, Friedman J, Stone C. Classification and Regression Trees. Wadsworth, 1984 Ripley B. Pattern Recognition and Neural Networks. Cambridge University Press, 1996 Liu B. Web Data Mining: Exploring Hyperlinks, Contents and Usage Data. Springer-Verlag, 2006 Hyafil L, Rivest RL. Constructing Optimal Binary Decision Trees is NP-complete. Information Processing Letters, 1976,5(1): 15-17

HastieT, Tibshirani R, Friedman JH. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer-Verlag, 2001

[8]    Yamanishi K. A learning criterion for stochastic rules. Machine Learning, 1992

[9]    Li H, Yamanishi K. Text classification using ESC-based stochastic decision lists. Information Processing & Management, 2002,38(3): 343-361

[10]    Li H, Abe N. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 1998,24(2): 217-244
