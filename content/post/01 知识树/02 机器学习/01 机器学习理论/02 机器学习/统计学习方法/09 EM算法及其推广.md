---
title: 09 EM算法及其推广
toc: true
date: 2018-06-26 19:22:02
---
##### 第9章EM算法及其推广

EM算法是一种迭代算法，1977年由Dempster等人总结提出，用十甘有隐 变量(hidden variable)的概率模型参数的极大似然估计，或极大后验概率估计. EM算法的每次迭代由两步组成：E步，求期望(expectation); M步，求极大 (maximization).所以这一算法称为期望极大算法(expectation maximization algorithm),简称EM算法.本章首先叙述EM算法，然后讨论EM算法的收敛性： 作为EM算法的应用，介绍髙斯混合模型的学习；最后叙述EM算法的推广—— GEM算法.

###### 9.1 EM算法的引入

概率模运有时既含有观测变量(observable variable),又含有隐变量或潜在变 量(latent variable).如果概率模型的变量都是观测变量，那么给定数据，可以直 接用极大似然估计法，或贝叶斯估计法估计模型参数.但是，当模型含有隐变量 时，就不能简单地使用这些估计方法.EM算法就是含有隐变量的概率模型参数 的极大似然估计法，或极大后验概率估计法.我们仅讨论极大似然估计，极大后 验概率估计与其类似.

9.1.1 EM 算法

首先介绍一个使用EM算法的例子.

例9.1 (三硬币模型)假设有3枚硬币，分别记作A, B，C.这些硬币正面 出现的概率分别是疋，p和分.进行如下掷硬币试验：先掷硬币A，根据其结果 选出硬币B或硬币C,正面选硬币B,反面选硬币C;然后掷选出的硬币，掷硬币 的结果，出现正面记作1，出现反面记作0:独立地重复《次试验(这里，《 = 10)， 观测结果如下：

1，1，0，1，0,0，1，0，1，1

假设只能观测到掷硬币的结果，不能观测掷硬币的过程.问如何估计三硬币正面 出现的概率，即三硬币模型的参数.

解三硬币模型可以写作

P{y\e)=\e)=^P{z\e)P{y\z,0}

=npW- py-y + (1 _ jr)qy(y- q)'-y    (9.1)

这里，随机变量是观测变量，表示一次试验观测的结果是1或0;随机变量z 是隐变量，表示未观测到的掷硬币A的结果：0 =    是模型参数.这一模

型是以上数据的生成模型.注意，随机变量jv的数据可以观测，随机变量z的数 据不可观测.

将观测数据表示为r = («，••■,1；)T,未观测数据表示为Z = (ZPZ2，…，Za)T, 则观测数据的似然函数为

P(y\0) = ^P(.Z\0)P(X\Z,0)    (9.2)

Z

即

P(Y\0) = fjpry (1 - p)1^ +(1-疋)(1 - q)^}

(9.3)



/=>

考虑求模型参数0 =    幻的极大似然估计，即

0 = aig max log P(Y i 0}



(9.4)



这个问题没有解析解，只有通过迭代的方法求解.EM算法就是可以用于求解 这个问题的一种迭代算法.下面给出针对以上问题的EM算法，其推导过程省略.

EM算法首先选取参数的初值，记作沪然后通过下面的 步骤迭代计算参数的估计值，直至收敛为止.第f次迭代参数的估计值为

EM算法的第扞1次迭代如下.

E步：计算在模型参数下观测数据J；.来自掷硬币B的概率

#### _wr(i-，y-

w (1 _ ,)'-々+(1 -龙《)(，)广(1-

M步：计算模型参数的新估计值

n 7=1

j=i

###### t(i-O

gd+n -    _



(9.6)



(9.7)



(9.8)



进行数字计算.假设模型参数的初值取为

\#)=0.5, pm=0.5, qm=0.5

由式(9.5)，对巧=1与乃=0均有0.5.

利用迭代公式(9.6)〜(9.8),得到

?ra)=0.5, p(l)=0.6, qm =0.6

由式(9.5),

Af)=0.5, 7 = 1,2,-.10

继续迭代，得

=0.5, pm =0.6 , qm=Q.6

于是得到模型参数6的极大似然估计，

\# = 0.5 , p = 0.6 » 夺= 0.6

冗=0.5表示硬币A是均匀的，这一结果容易理解.

如果取初值；r(a) =0.4, pm =0.6，=0.7，那么得到的模型参数的极大似 然估计是彦= 0.4064，p = 0.5368, ? = 0.6432.这就是说，EM算法与初值的选 择有关，选择不同的初值可能得到不同的参数估计值.    ■

—般地，用r表示观测随机变量的数据，z表示隐随机变量的数据.r和z 连在一起称为完全数据(complete-data),观测数据r又称为不完全数据 (incomplete-data).假设给定观测数据I7,其概率分布是P(F|的，其中<9是需要 估计的模型参数，那么不完全数据y的似然函数是p(y|的，对数似然函数 £(的=iogp(r|的；假设y和z的联合概率分布是p(y,z\0),那么完全数据的对 数似然函数是iogp(y，z |的.

EM算法通过迭代求£(6») = logP(/|<9)的极大似然估计.每次迭代包含两步： E步，求期望；M步，求极大化.下面来介绍EM算法.

算法9.1 (EM算法)

输入：观测变量数据F，隐变量数据Z,联合分布P(r，Z|<9)，条件分布 P(Z\Y,^；

输出：模型参数沒.

(1)    选择参数的初值(9^，开始迭代；

(2)    E步：记W为第z•次迭代参数6•的估计值，在第f + 1次迭代的E步，计算

0(^^°) = Ez[logP(Y,Z\ff)\ Y,^]

= ^logP(r,Z| 0)P(Z\ r,^°)    (9.9)

这里，P(z IY,^)是在给定观测数据y和当前的参数估计於> 下隐变量数据Z的 条件概率分布；

(3 ) M步：求使2(氏沪))极大化的0，确定第i +1次迭代的参数的估计值妒+u 沪+|) =argmax Q(0, ^°)    (9.10)

(4)重复第(2)步和第(3)步，直到收敛.    ■

式(9.9)的函数0(<9,沪))是EM算法的核心，称为0函数(0 function).

定义9.1 (e函数)完全数据的对数似然函数logP(y,Z|6>)关于在给定观测

数据r和当前参数^«下对未观测数据2的条件概率分布P(z I y,^0)的期望称 为2函数，即

e(^,^(0) = Ez[loSP(Y,Z\0)\ Y,^]    (9.11)

下面关于EM算法作几点说明：

步骤(1)参数的初值可以任意选择，但需注意EM算法对初值是敏感的. 步骤(2) E步求2(氏沪>).2函数式中Z是未观测数据，7是观测数据•注

意，00?，分f))的第1个变元表示要极大化的参数，第2个变元表示参数的当前估 计值.每次迭代实际在求0函数及其极大.

步骤(3 ) M步求Q(d, 6^ )的极大化，得到沪+1>，完成一次迭代W 4沪+1>. 后面将证明每次迭代使似然函数增大或达到局部极值.

步骤(4)给出停止迭代的条件，一般是对较小的正数£pf2,若满足 || 沪+1)-妒 ||<& 或 |1⑽，妒)-Q(.0(n,0in)\\<£2

则停止迭代.

9.1.2 EM算法的导出

上面叙述了 EM算法.为什么EM算法能近似实现对观测数据的极大似然估 计呢？下面通过近似求解观测数据的对数似然函数的极大化问题来导出EM算 法，由此可以清楚地看出EM算法的作用.

我们面对一个含有隐变量的概率模型，目标是极大化观测数据(不完全数据) r关于参数的对数似然函数，即极大化

L(0) = logP(Z|^) =

z

(9.12)



= iogfyp(y|z^)p(z|^

注意到这一极大化的主要困难是式(9.12)中有未观测数据并有包含和(或积分) 的对数.

事实上，EM算法是通过迭代逐步近似极大化Z(6»)的.假设在第/次迭代后 的估计值是沪>.我们希望新估计值0能使Z(的增加，即£(的>£(<?«),并逐步

达到极大僮.为此，考虑两者的差： Z(^)-Z(^(o) =



、?p(y\z,e)p{z\e)yiogp(y|(9w)

利用Jensen不等式(Jensen inequality〉得到其下界：

■ - z(W)=iog(jp(r    - iogp(j^«)

_ iogp(r，)

= yp(z|r,^)iog-p<nzj)^^)

V    P(Z\Y,^W\0^)

令

峨咋 i(ew)+£p(z| y,»(‘>gp(fzO芯 gf,)    (9.13)

则

L ⑼各 B{fi，，、、    (9.14)

即函数5(氏W)是£(的的一个下界，而且由式(9.13)可知，

=    (9.15)

因此，任何可以使5(况沪》)增大的沒，也可以使£(的增大.为了使1(的有尽可 能大的增长，选择分*+1)使5(么妒)达到极大，即

沪+1>=argn^xS(/9,妒))    (9.16)

现在求#+|)的表达式.省去对0的极大化而言是常数的项，由式(9.16)、式(9.13) 及式(9.10)，有

r:叫，卜，+?松时，賴嚴詞

=argmax^P(Zl F,0w)log(P(y | Z,0)P(Z|0))']

=argmax^P(Z| Y,0w')logP(Y,Z\0')^

=argmaxg(沒，沒(0)

式(9.17)等价于EM算法的一次迭代，即求0函数及其极大化.EM算法是通过

不断求解下界的极大化逼近求解对数似然函数极大化的算法•

图9.1给出EM算法的直观解释.图中上方曲线为£(6>)，下方曲线为

B(0,0W).由式(9.14)，5    为对数似然函数1(的的下界.由式(9.15)，两个

函数在点<9 =俨处相等.由式(9.16)和式(9.17), EM算法找到下一个点W+1)使 函数SW(i))极大化，也使函数0W(i))极大化.这时由于，函 数5(况妒)的增加，保证对数似然函数i(的在每次迭代中也是增加的.EM算法 在点决/+1)重新计算2函数值，进行下一次迭代.在这个过程中，对数似然函数 L｛9)不断增大.从图可以推断出EM算法不能保证找到全局最优值.

9.1.3 EM算法在非监督学习中的应用

监督学习是由训练数据学习条件概率分布 P(F|幻或决策函数y=/OQ作为模型，用于分类、回归、标注等任务.这时训 练数据中的每个样本点由输入和输出对组成.

有时训练数据只有输入没有对应的输出｛以,，)，^^.)，…，^#.)｝，从这样的数 据学习模型称为非监督学习问题.EM算法可以用于生成模型的非监督学习.生 成模型由联合概率分布PGT，y)表示，可以认为非监督学习训练数据是联合概率 分布产生的数据.又为观测数据，r为未观测数据.

###### 9.2 EM算法的收敛性

EM算法提供一种近似计算含有隐变量概率模型的极大似然估计的方法.EM 算法的最大优点是简单性和普适性.我们很自然地要问：EM算法得到的估计序 列是否收敛？如果收敛，是否收敛到全局最大值或局部极大值？下面给出关于 EM算法收敛性的两个定理.

定理9.1设/VI的为观测数据的似然函数，俨(f = l，2,…)为EM算法得到 的参数估计序列，…)为对应的似然函数序列，则p(r|W)是单 调递增的，即

p(y|^+,)>>p(r|^(,))    (9.18)

证明由于



尸(嘲=



尸(y，z|g) P(Z\Y,ff)



取对数有

由式(9.11)



iogp(y I 的=iogP(Y,z\ff)~ iogp(z| Y,e)



Q(e,ew)=2；iogp(y,z| e)p{z\ y,0w)

z

W,<?(，)) = ^l0gP(Z| Y,e)P(Z\ Y,0W) z

于是对数似然函数可以写成

iogp(y|<9)=Q(e,^

(9.19)



(9.20)



在式(9.20)中分别取沒为少f)和沪+1>并相减，有 iogp(y| 妒+1)) - iogp(y| 0W)

=[e(^,+l),^°)-    -    (9.2i)

为证式(9.18),只需证式(9.21)右端是非负的.式(9.21)右端的第1项，由 于6»('+1)使Q(0,^y达到极大，所以有

) - 0(沪)，妒)彡 0    (9.22)

其第2项，由式(9.19)可得:

这里的不等号由Jensen不等式得到.

尸(z!y，g⑽))

尸(Z! V)) J



^log



P(z|y,<9(0)



=iogp(z|y,^i+l))=o



(9.23)



由式(9.22)和式(9.23)即知式(9.21)右端是非负的.    ■

定理9.2设Z(£0 = logP(F|6»)为观测数据的对数似然函数，=

为EM算法得到的参数估计序列，(» = 1,2,-)为对应的对数似然函数序列.

(1)    如果P(y|的有上界，则Z⑺(*))=吨/^|沪))收敛到某一值1；;

(2)    在函数0(<9，的与Z(的满足一定条件下，由EM算法得到的参数估计序 列沪> 的收敛值矿是£(的的稳定点.

证明(1)由£(灼的单调性及P(y|的的有界性立即得到.

(2)证明从略，参阅文献[6].    ■

定理9.2关于函数0(况的与L(0)的条件在大多数情况下都是满足的.EM算 法的收敛性包含关于对数似然函数序列£(^°)的收敛性和关于参数估计序列於> 的收敛性两层意思，前者并不蕴涵后者.此外，定理只能保证参数估计序列收敛 到对数似然函数序列的稳定点，不能保证收敛到极大值点.所以在应用中，初值 的选择变得非常重要，常用的办法是选取几个不同的初值进行迭代，然后对得到 的各个估计值加以比较，从中选择最好的.

###### 9.3 EM算法在高斯混合模型学习中的应用

EM算法的一个重要应用是髙斯混合模型的参数估计.髙斯混合模型应用广 泛，在许多情况下，EM算法是学习髙斯混合模型(Gaussian misture model)的有 效方法.

93.1高斯混合模型

定义9.2 (高斯混合模型)髙斯混合模型是指具有如下形式的概率分布模型： P(y\^ = Xak<^y\0k)    (9.24)

*=i

其中，珥是系数，ak^Q, Xak=l；兴y|么)是高斯分布密度，0,吻»

*=i

-穿)    ⑽

称为第*个分模型.

一般混合模型可以由任意概率分布密度代替式(9.25)中的髙斯分布密度，我 们只介绍最常用的髙斯混合模型.

9.3.2高斯混合模型参数估计的EM算法

假设观测数据…，.外由髙斯混合模型生成，

p(y\e)=^a^yW    (9.26)

*=i

其中，沒= («i，a2，…，％此氏，…A).我们用EM算法估计髙斯混合模型的参数权.

1.明确隐变量，写出完全数据的对数似然函数

可以设想观测数据&，J = l，2,…，况，是这样产生的：首先依概率珥选择第 k个髙斯分布分模型0aI色)：然后依第个分模型的概率分布＜KyW生成观测 数据巧.这时观测数据＞7, 7=1,2,…，JV，是已知的；反映观测数据人来自第女 个分模型的数据是未知的，fc = l，2,…，尺，以隐变量表示，其定义如下：

= fl,    第_/个观测来自第*个分模型

^"{o, 否则

; = 1,2，."，JV: k = l,2,-,K    (9.27)

/#是0-1随机变量.

有了观测数据 及未观测数据& ,那么完全数据是

于是，可以写出完全数据的似然函数：

P(y,r\o')= YlP(yj，rji：rj2,…，rJK\6

7=1

kn    rjt

##### =nn[Mo,j i^*)]

##### M >1

##### 式中，nk    * Zw*=Ar-

y-i    k=\

那么，完全数据的对数似然函数为 log/V，Zl的=2X loga*    log(

-loga*-



(9.28)



\2. EM算法的E步：确定G函数

Q(0,^) = E[\ogP(y，r\0)\y,^]

= E\t,nk +Ez#pog •



-logtr* -



-11



= ZlL(£Jy*)loSa* +Z(£Z/J lQg -7z=



这里需要计算£(z#| y,0),记为么.

?#=^(z#|y^)=p(zA=i]j^)

尸(?>=i，巧I的

i=l

_尸(巧1?>=1，戰^=11的

Zp^Iza=i,Wz#=1|6»)

*=1

---，7 = 1,2, --,M k = l,2,- -,K

i-l

7Jk是在当前模型参数下第y个观测数据来自第/t个分模型的概率，称为分模型女 对观测数据A的响应度.

N

将^ = £〜及R = ^ErJk代入式(9.28)即得 7=1

e(^^°) = E/I*1°8a* +^&flog(去)-l0ga*    -Ai)2J (9.29)

3.确定EM算法的M步

迭代的M步是求函数!3(0，於))对0的极大值，即求新~轮迭代的模型参数： 6»(/+1)=argmaxe(0,^f))

用A , ＜及咚，A = l，2，"•，尤，表示tf(w＞的各参数.求成，巧只需将式(9.29) 分别对从，＜ 求偏导数并令其为0,即可得到；求么是在fat=l条件下求偏 导数并令其为0得到的.结果如下：

N

Zw

A=^4?——，k = l,2f-,K    (9.30)

\>1

N

^l=J=L-^--k = l,2,-,K    (9.31)

名=贵=乂，k = l,2,---,K    (9.32)

重复以上计算，直到对数似然函数值不再有明显的变化为止.

现将估计髙斯混合模型参数的EM算法总结如下：

算法9.2 (高斯混合模型参数估计的EM算法)

输入：观测数据乃，凡，…，:以，髙斯混合模型；

输出：髙斯混合模型参数.

(1)    取参数的初始值开始迭代

(2)    E步：依据当前模型参数，计算分模型it对观测数据巧的响应度

7Jk =~k-,    J = l,2,---,N-, k = \,2,- -,K

(3)M步：计算新一轮迭代的模型参数

N

Ew

A =~^i-» k = l,2,---,K

\- a)2

ZZz*



k = l,2,-,K



ak=^-,    k=l,2,-,K

(4)重复第(2)步和第(3)步，直到收敛.

###### 9.4 EM算法的推广

EM算法还可以解释为F函数(Ffunction)的极大-极大算法(maximization-maximization algorithm).基于这个解释有若干变形与推广，如广义期望极大 (generalized expectation maximization, GEM)算法.下面予以介绍.

9.4.1尸函数的极大-极大算法 首先引进F函数并讨论其性质.

定义(F函数)假设隐变量数据Z的概率分布为戶(Z)，定义分布P与 参数沒的函数戶，的如下：

F(P,^) = Ef[\ogP(Y,Z\ 0)] + H(P)    (9.33)

称为F函数.式中H(P) = -Ef, !ogP(Z)是分布P(Z)的熵.

在定义9.3中，通常假设P(y，Z|的是夕的连续函数，因而F(戶，的是P和沒的 连续函数.函数F(户，的还有以下重要性质：

引理9.1对于固定的存在唯一的分布总极大化F(戶，的，这时&由下式 给出：

Pe{Z) = P(Z\Yt0)    (9.34)

并且巧随权连续变化.

证明对于固定的沒，可以求得使F(户，的达到极大的分布戶6(2).为此，引 进拉格朗日乘子2,拉格朗日函数为

L = E-P log 尸(r，Z\0)-EP log 戶(2) +    (9.35)

将其对戶求偏导数，

g^j = log P(K,Z|^)-log P{Z)-1-2 令偏导数等于0,得出

义=logP(y,Z|6»)- log 芎(Z)-1 由此推出A (Z)与P(Y, ZI的成比例

p(y,z\e)^e+x

to

再从约束条件=1得式(9.34).

由假设P(Y,Z |的是沒的连续函数，得到巧是没的连续函数.    ■

引理9.2若巧(Z) = P(Zf)，则

F(P,0) = \oSP(Y\0)    (9.36)

证明作为习题，留给读者.

由以上引理，可以得到关于EM算法用F函数的极大-极大算法的解释.

定理9J设1(的= logP(y|的为观测数据的对数似然函数，俨，f = l,2,…，

为EM算法得到的参数估计序列，函数F(A的由式(9.33)定义.如果F(户，的在戶* 和矿有局部极大值，那么£(的也在矿有局部极大值.类似地，如果F(户，的在r 和矿达到全局最大值，那么L⑼也在矿达到全局最大值.

证明由引理9.1和引理9.2可知，M的= iogP(r|的：厂仿,的对任意夕成 立.特别地，对于使F(戶，的达到极大的参数矿，有

L(0') = F(Ptr,^) = F(P',ff,)    (9.37)

为了证明矿是£(的的极大点，需要证明不存在接近矿的点矿使1(矿*)>£(矿). 假如存在这样的点矿*,那么应有F(#M，f*)>F(尹，女)，这里户但因& 是随沒连续变化的，戶“应接近尹，这与尹和矿是F(戶，的的局部极大点的假设 矛盾.

类似可以证明关于全局最大值的结论.    ■

定理9.4 EM算法的一次迭代可由F函数的极大-极大算法实现.

设W为第f次迭代参数没的估计，P为第i次迭代函数戶的估计.在第<• +1

次迭代的两步为

(1)    对固定的妒\求户<+1)使F(户，沪^极大化：

(2)    对固定的戶(i+1>：求#+1>使F(戶°+1)，的极大化.

证明(1)由引理9.1,对于固定的^«,

PiM\Z) = P^(Z) = P(Z\Y,^)

使尺足沪”极大化.此时，

f 〜的=£戸[iogP(y,z|^)]+斤(戸)

=[log 尸(y，z 16>)p(z| r,^°)+h(尹,+1))

7.

由0(^,^°)的定义式(9.11)有

F(PiM\ff) = e( W)) + 7/(^,+1))

(2)固定P°+1>,求沪+|)使厂(？°+|)，的极大化.得到

= aigmax.F(P(M),d) = argm严 0(沒>0)

通过以上两步完成了 EM算法的一次迭代.由此可知，由EM算法与尸函数的极 大-极大算法得到的参数估计序列少0，f = l,2,…，是一致的.    ■

这样，就有EM算法的推广.

9.4.2 GEM 算法

算法9.3 (GEM算法1)

输入：观测数据，尸函数；

输出：模型参数.

(1)    初始化参数开始迭代

(2)    第Z + 1次迭代，第1步:记妒为参数的估计值，胂为函数P的估计.求 使P极大化F(戶，

(3)    第2步：求妒+1>使^(户极大化

(4)    重复(2)和(3)，直到收敛.    ■

在GEM算法1中，有时求0(况W)的极大化是很困难的.下面介绍的GEM

算法2和GEM算法3并不是直接求沪+1)使达到极大的0，而是找一个 沪+1)使得0(沪+1)，沪.

算法9.4 (GEM算法2)

输入：观测数据，0函数；

输出：模型参数.

(1)    初始化参数开始迭代

(2)    第f + 1次迭代，第1步：记^w为参数的估计值，计算

Q(0,0W)=£z[iogP(y,z|(9)| y,0(o]

= ^P{Z\Y,e^)\ogP{Y,Z\ff)

(3)第2步：求分'+1>使

e(^i+i)>o)>e(^°,^°)

(4)重复(2)和(3),直到收敛.    ■

当参数0的维数为d (d>2)时，可采用~种特殊的GEM算法，它将EM

算法的M步分解为次条件极大化，每次只改变参数向量的一个分量，其余分 量不改变.

算法9.5 (GEM算法3)

输入：观测数据，0函数；

输出：模型参数.

(1)    初始化参数，=(6T，6f，•••,#>)，开始迭代

(2)    第f+1次迭代，第1步:记妒=(时0，却)，…，巧))为参数0 =汝,色,…，么) 的估计值，计算

Q(W) = Ez[\OgP{Y,Z\0)\ Y,0W]

= ^P(Z|y,6»w)logP(y,Z|6>)

z

(3)    第2步：进行</次条件极大化：

首先，在保持不变的条件下求使，沪达到极大的<+1);

然后，在今=碎|+1)，0j=^, </ = 3,4，"必的条件下求使2(沒，於))达到极大

的磅i+1〉；

如此继续，经过£/次条件极大化，得到#+1>«+1)，咚+1>，…，巧+1>)使得

(4)    重复(2)和⑶，直到收敛.    ■



###### 本章概要

I. EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭 代算法.含有隐变量的概率模型的数据表示为p(y，zi的.这里，y是观测变量的 数据，Z是隐变量的数据，0是模型参数.EM算法通过迭代求解观测数据的对 数似然函数L(0) = logP(F|0)的极大化，实现极大似然估计.每次迭代包括两步： E步，求期望，即求logP(y，Z|灼关于)的期望：

=^iogP(y,z| e)p(z\ r>°) z

称为2函数，这里少G是参数的现估计值；M步，求极大，即极大化g函数得 到参数的新估计值：

^+I)=argmaxe(^^(0)

在构建具体的EM算法时，重要的是定义0函数.每次迭代中，EM算法通 过极大化0函数来增大对数似然函数Z(的.

\2. EM算法在每次迭代后均提髙观测数据的似然函数值，即

在一般条件下EM算法是收敛的，但不能保证收敛到全局最优.

\3.    EM算法应用极其广泛，主要应用于含有隐变量的概率模型的学习.髙斯 混合模型的参数估计是EM算法的一个重要应用，下一章将要介绍的隐马尔可夫 模型的非监督学习也是EM算法的一个重要应用.

\4.    EM算法还可以解释为f函数的极大-极大算法.EM算法有许多变形， 如GEM算法.GEM算法的特点是每次迭代增加F函数值（并不一定是极大化F 函数），从而増加似然函数值.

###### 继续阅读

EM算法由Dempster等人总结提出[1].类似的算法之前已被提出，如Baum 与Welch算法，但是都没有EM算法那么广泛.EM算法的介绍可参见文献[2〜4]. EM算法收敛性定理的有关证明见文献[5]. GEM是由Neal与Hinton提出的[6].

习 题

9.1如例9.1的三硬币模型.假设观测数据不变，试选择不同的初值，例如，；=

0.46, p（0）=0.55, 9w=0.67,求模型参数沒= （;r,p，g）的极大似然估计.

9.2证明引理9.2.

9.3已知观测数据

-67, -48, 6, 8, 14, 16, 23, 24, 28, 29, 41, 49, 56, 60, 75

试估计两个分量的髙斯混合模型的5个参数.

9.4 EM算法可以用到朴素贝叶斯法的非监督学习.试写出其算法.

###### 参考文献

[1]    Dempster AP, Laird NM, Rubin DB. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B., 1977, 39

[2]    Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springcr-Verlag, 2001（中译本:统计学习基础•"一数据挖掘、推理与预測.范 明，柴玉梅，昝红英等译.北京：电子工业出版社，2004）

[3]    McLachlan Q Krishnan T. The EM Algorithm and Extensions. New Yoric John Wley & Sons, 1996

[4]    茆诗松，王静龙，濮晓龙.麻等数理统计.北京：髙等教育出版社；海登堡：斯普林格出 版社，1998

[5]    Wu CEL On the convergence properties of the EM algorithm. The Annals of Statistics, 1983, 11:95-103

[6]    Radford N, Geoffrey H, Jordan MI. A view of the EM algorithm that justifies incremental, sparse, and other variants. In: Learning in Graphical Models. Cambridge, MA: MIT Press, 1999, 355-368
