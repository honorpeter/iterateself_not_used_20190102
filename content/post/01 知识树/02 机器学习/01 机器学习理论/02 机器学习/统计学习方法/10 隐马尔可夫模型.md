---
title: 10 隐马尔可夫模型
toc: true
date: 2018-06-26 19:22:01
---
##### 第10章隐马尔可夫模型

隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型.本 章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计 算算法、学习算法以及预测算法.隐马尔可夫模型在语音识别、自然语言处理、 生物信息、模式识别等领域有着广泛的应用.•

###### 10.1隐马尔可夫模型的基本概念

10.1.1隐马尔可夫模型的定义

定义10.1 （隐马尔可夫模型）隐马尔可夫模型是关于时序的概率模型，描 述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生 成一个观测而产生观测随机序列的过程.隐藏的马尔可夫链随机生成的状态的序 列，称为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测 的随机序列，称为观测序列（observation sequence）.序列的每一个位置又可以看 作是一个时刻.

隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确 定.隐马尔可夫模型的形式定义如下：

设（2是所有可能的状态的集合，r是所有可能的观测的集合.

G = {?1，?2,    «    ^ = {V1»V2» —，VAf）

其中，是可能的状态数，似是可能的观测数.

I是长度为r的状态序列，o是对应的观测序列.

Z = （“，•••，&），0==（°i，o2,…，0r）

W是状态转移概率矩阵：

吨］脚    （瓜1）

其中，

〜=叫+1=川卜9：），    / =    7=1,2,-,^    （10.2）

是在时刻f处于状态9,的条件下在时刻f +1转移到状态*的概率.

B是观测概率矩阵:

(io.3)

其中，

bj(k) = P(ot =vk\il=qj),    j =    (10.4)

是在时刻f处于状态a的条件下生成观测vt的概率.

龙是初始状态概率向量：

龙=沐)    (10.5)

其中，

^1=^1 =?,)» *' = 1，2,…，7V    (10.6)

是时刻f = l处于状态9,的概率.

隐马尔可夫模型由初始状态概率向量/T、状态转移概率矩阵4和观测概率矩 阵5决定.*和3决定状态序列，5决定观测序列.因此，隐马尔可夫模型又可 以用三元符号表示，即

A = (A,B,7T)    (10.7)

次况疋称为隐马尔可夫模型的三要素.

状态转移概率矩阵X与初始状态概率向量;r确定了隐藏的马尔可夫链，生成 不可观测的状态序列.观测概率矩阵5确定了如何从状态生成观测，与状态序列 综合确定了如何产生观测序列.

从定义可知，隐马尔可夫模型作了两个基本假设：

(1)    齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻T的状态只 依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻f无关.

/ = l,2,-,r    (10.8)

(2)    现测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链 的状态，与其他观测及状态无关.

P(o, |/r,Qr,»^o,) = P(o, 10    (10.9)

隐马尔可夫模型可以用于标注，这时状态对应着标记.标注问题是给定观测 的序列预测其对应的标记序列.可以假设标注问题的数据是由隐马尔可夫模型生 成的.这样我们可以利用隐马尔可夫模型的学习与预测算法进行标注.

下面看一个隐马尔可夫模型的例子.

例10.1 （盒子和球模型）假设有4个盒子，每个盒子里都装有红白两种颜 色的球，盒子里的红白球数由表10.1列出，

表10.1各盒子的红白球数

| 盒子   | 1 2  | 3    | 4    |      |
| ------ | ---- | ---- | ---- | ---- |
| 红球数 | 5    | 3    | 6    | 8    |
| 白球数 | 5    | 7    | 4    | 2    |

按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里 以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放 回；然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1， 那么下一盒子一定是盒子2,如果当前是盒子2或3,那么分别以概率0.4和0.6 转移到左边或右边的盒子，如果当前是盒子4,那么各以0.5的概率停留在盒子4 或转移到盒子3•:确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其 颜色，放回；如此下去，重复进行5次，得到一个球的颜色的观测序列：

0 = ｛紅，红，白，白，红｝

在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取 出的，即观测不到盒子的序列.

在这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球 的颜色的观测序列（观测序列）.前者是隐藏的，只有后者是可观测的.这是一 个隐马尔可夫模型的例子，根据所给条件，可以明确状态集合、观测集合、序列 长度以及模型的三要素.

盒子对应状态，状态的集合是

e = ｛盒子1,盒子2,盒子3,盒子4｝，AT = 4

球的颜色对应观测.观测的集合是

r = ｛红，白M = 2

状态序列和观测序列长度T = 5.

初始概率分布为

^ = （0.25,0.25,0.25,0.25）T

状态转移概率分布为

观测概率分布为

| ■0.5 | 0.5' |
| ---- | ---- |
| 0.3  | 0.7  |
| 0.6  | 0.4  |
| 0.8  | 0.2  |

10.1.2观測序列的生成过程

根据隐马尔可夫模型定义，可以将一个长度为r的观测序列o=(O|，o2，…，Of) 的生成过程描述如下，

算法10.1 (观测序列的生成)

输入：隐马尔可夫模型A=(45，幻，观测序列长度 输出：观测序列O = (opo2,…，or) •

(1)    按照初始状态分布;T产生状态4

(2)    令f = l

(3)    按照状态f,的观测概率分布久(Jt)生成o,

(4〉按照状态f,的状态转移概率分布｛<?^｝产生状态f,+1, /<+1=l,2,-,JV (5)令f = / + l:如果f<r,转步(3): 则，终止    ■

10.13隐马尔可夫模型的3个基本问题

隐马尔可夫模型有3个基本问题：

(1)    概率计算问题.给定模型2 = G4W)和观测序列6? = (OpO2，…，Or)，计 算在模型A下观测序列O出现的概率14 .

(2)    学习问题.己知观测序列(? = (o,，o2，…，or),估计模型2 = 04,5,龙)参数， 使得在该模型下观测序列概率P(O | A)最大.即用极大似然估计的方法估计参数.

(3)    预测问题，也称为解码(decoding)问题.已知模型A = (J,5，;r)和观测

序列O = (ot,o2, -,oT),求对给定观测序列条件概率P(门6?)最大的状态序列 / =    即给定观测序列，求最有可能的对应的状态序列.

下面各节将逐一介绍这些基本问题的解法.

###### 10.2概率计算算法

本节介绍计算观测序列概率P((9|2)的前向(forward)与后向(backward) 算法.先介绍概念上可行但计算上不可行的直接计算法.

10.2.1直接计算法

给定模型又=04,足龙)和观测序列O = (O)，Oj，…，or)，计算观测序列0出现的 概率P(O|2).最直接的方法是按概率公式直接计算.通过列举所有可能的长度 为r的状态序列/=(^2，…劣)，求各个状态序列J与观测序列o=(q，02，…，&) 的联合概率P(o,/1A),然后对所有可能的状态序列求和，得到P(O|2).

状态序列J=(^2，一,y的概率是

(10.10)

对固定的状态序列，观测序列O = (O|,o2，…，七)的概率是 P{O\1,X),

P(Q | /，乂)=《峨(o2)-\(or)    (10.11)

<?和/同时出现的联合概率为

P(C?，/|A) = P(O|/，A)P(/| 乂)

(o2)."awA (°r)    (10.12)

然后，对所有可能的状态序列/求和，得到观测序列O的概率P(O|A)，即

P(O|2) = JP(O|/,A)P(/|A)

1

=L nKbh    (Or)    (10.13)

H…，

但是，利用公式(10.13)计算量很大，是O(77\TT)阶的，这种算法不可行.

下面介绍计算观测序列概率/>(O|A)的有效算法：前向-后向算法(forward-backward algorittim).

10.2.2前向算法

首先定义前向概率.

定义10J (前向概率)给定隐马尔可夫模型定义到时刻/部分观测序列 为o,，o2，…，o,且状态为弘的概率为前向概率，记作

^l(i) = P(oi,o2,--,o„il =qt\X)    (10.14)

可以递推地求得前向概率«,(/)及观测序列概率| A).

算法10.2 (观测序列概率的前向算法)

输入：隐马尔可夫模型A ,观测序列O;

输出：观测序列概率P(O|A).

(1)初值



al(i) = ^ib,(ol),    i=l，2,…，2V



(10.15)



(2)递推对z=i，2,…，r-i，



«(+,0)=

6'(o'+i)， j = 1，D



(10.16)



(3)终止



P(O|2) = £ar(f)

(10.17) ■



前向算法，步骤(1)初始化前向概率，是初始时刻的状态/,= 9,和观测o,的 联合概率.步骤(2)是前向概率的递推公式，计算到时刻Z + 1部分观测序列为 01，02,"‘，0,，0,+1且在时刻< + 1处于状态见的前向概率，如图10.1所示.在式(10.16) 的方括弧里，既然a,(j)是到时刻/观测到并在时刻/处于状态*的前 向概率，那么乘积a,(»a>(就是到时刻r观测到o,，o2,.• 并在时刻f处于状态a 而在时刻f +1到达状态的联合概率.对这个乘积在时刻/的所有可能的AT个状 态％求和，其结果就是到时刻/观测为Ol,o2,-,o,并在时刻f +1处于状态&的联 合概率.方括弧里的值与观测概率6,(o,+l)的乘积恰好是到时刻f + 1观测到 0、，02，…，o,，ol+l并在时刻* + 1处于状态必的前向概率a,+1⑺.步骤(3)给出P(O|2) 的计算公式.因为

Oj.(i) = P(Oi ,o2,--,oT,iT=qi\X)

N

p(oia)=2^.(o

t

a,(J)

r+l

a川0)



图10.1前向概率的递推公式

如图10.2所示，前向算法实际是基于“状态序列的路径结构”递推计算 的算法.前向算法髙效的关键是其局部计算前向概率，然后利用路径结构将前向 概率“递推”到全局，得到P(<9|A).具体地，在时刻f = l,计算《,(<•)的W个值

(i = l，2,…，JV);在各个时刻f = l，2,…，r-1，计算a<+1(f)的 TV■个值G = l»2，…，AT), 而且每个a,+1(0的计算利用前一时刻况个珥(/).减少计算量的原因在于每一次 计算直接引用前一个时刻的计算结果，避免重复计算.这样，利用前向概率计算 P(O\Xj的计算量是O(N2T)阶的，而不是直接计算的阶.

12    3    T

图10.2观测序列路径结构

例10.2考虑盒子和球模型又=,状态集合0 = {1,2,3}.观测集合K = {红，白},

| A =                   | •0.5 0.2 0.3-0.3 0.5 0.20.2 0.3 0.5 | ，B = | 0.5 0.5' 0.4 0.60.7 0,3 | ，^ = (0.2,0.4,0.4)t |
| --------------------- | ----------------------------------- | ----- | ----------------------- | -------------------- |
| 设r=3, 0=(红，白，红) | 试用前向算法计算P((91义).           |       |                         |                      |

解按照算法10.2

(1)    计算初值

«i(i)=^A(0i)=o.io

«i(2) = ^262(0,) = 0.16 叫⑶= ^3^(°i)= 0.28

(2)    递推计算

| «2(1)=  | ./=!            | ^(<^) = 0.154x0^=0.077         |
| ------- | --------------- | ------------------------------ |
| «2(2)=  | 3./-l    .      | Z»2(o2) = 0.184x0.6 = 0.1104   |
| 巧⑶=    | .1-1    .       | i>3(o2) = 0.202 x 0.3 = 0.0606 |
| a3(l) = | 32X⑺％_(-i    • | il(o3) = 0.04187               |
| 叫(2)=  | 31X(加j         | *2(^3) = 0-03551               |

^(3)= [Za20X^3(o3) = 0.05284

(3)终止



3

P(O I 义)=2X(:) = 0.13022

i=i

10.2.3后向算法

定义1(L3 (后向概率)给定隐马尔可夫模型Z,定义在时刻状态为g,的 条件下，从/+I到7的部分观测序列为心，o，+2，…，外的概率为后向概率，记作

P,(t) = P(oM,oM,—,oT\i, =q„X)    (10.18)

可以用递推的方法求得后向概率肩0及观测序列概率P(O\X).

算法10.3 (观测序列概率的后向算法)

输入：隐马尔可夫模型；I，观测序列O:

输出：观测序列概率P(O|A).

(1)

A_(f) = l，» = 1,2,-,2V    (10.19)

⑵ 5^/=r-i,r-2,-,i

N .

⑽=1沙i = l,2,-,N    (10.20)

7=1

(3)

(10.21) ■ i=l

步骤(1)初始化后向概率，对最终时刻的所有状态9,•规定疼(:-)=1.步骤(2) 是后向概率的递推公式.如图10.3所示，为了计算在时刻f状态为弘条件下时刻 t + 1之后的观测序列为Ol+I,Ol+2,-,Or的后向概率爲(0,只需考虑在时刻f +1所有

'    t+1

A(0    ^i(J)

图10.3后向概率递推公式



可能的个状态％的转移概率（即~项），以及在此状态下的观测0,+1的观测概 率（即6/0,+1）项），然后考虑状态％之后的观测序列的后向概率（即戊+1（力项）.步 骤（3）求的思路与步骤（2〉一致，只是初始概率^代替转移概率.

利用前向概率和后向概率的定义可以将观测序列概率P（O| 乂）统一写成

N N

P（OIX} =    （°<+i）A+>（7）-，= 1，2,…，r-1    （10.22）

i=l j=\

此式当r=1和r = r-1时分别为式（10.17）和式（10.21）.

10.2.4 —些概率与期望值的计算

利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式.

1.给定模型A和观测O,在时刻f处于状态％的概率.记

Z，（0 = P（d|O,2）    （10.23）

可以通过前向后向概率计算.事实上，

r,（i） = P（i, =?,|0,4）=气冗广）

由前向概率a,⑺和后向概率戍（f）定义可知：

«,（0A0'）=P0,=?/,o|A）

于是得到:



/,(/)=



«,佩(j)

N

###### 2>，(•/卿



(10.24)



2.给定模型义和观测（?，在时刻，处于状态孓且在时刻/ + 1处于状态受.的概 率.记

^(iJ) = P(.it= =    0,又)    (10.25)

可以通过前向后向概率计算：



尸(6=劣，,什1 =4,0|义)

= ?(»»/+! =?y,O|A)

<=l 7=1

p0«= 9,，i,+i = g},O\X) = a,OX^(o,+1)A+i(y)

所以

^(M)=    -    (10.26)

ZSa/ (加A (A+爲1 (•/) i-l 7=1

3.将以0和s(/，y)对各个时刻求和，可以得到一些有用的期望值：

(1)    在观测O下状态f出现的期望值

Ez,(0    (10.27)

h»l

(2)    在观测O下由状态f转移的期望值

r-i

(10.28)

<=i

(3)    在观测O下由状态转移到状态/的期望值

(10.29)

###### 10.3学习算法

隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还 是只有观测序列，可以分别由监督学习与非监督学习实现.本节首先介绍监督学 习算法，而后介绍非监督学习算法——Baum-Weich算法(也就是EM算法).

10.3.1监督学习方法

假设已给训练数据包含S个长度相同的观测序列和对应的状态序列 {(O,，A)，(O2 J2),"s(Os，心)}，那么可以利用极大似然估计法来估计隐马尔可夫模 型的参数.具体方法如下.

\1.    转移概率％的估计

设样本中时刻Z处于状态Z•时刻f +1转移到状态y的频数为4 ,那么状态转 移概率％的估计是

-» i = \,2,---,N ■, j =    (10.30)

j=i

\2.    观测概率的估计

设样本中状态为/并观测为4的频数是那么状态为y观测为的概率

b人k、的估计是

咖，7 = 1，2,…，V: k = \,2, -,M    （10.31）

3.初始状态概率;r,的估计弋为5个样本中初始状态为孓的频率

由于监督学习需要使用训练数据，而人工标注训练数据往往代价很髙，有时

就会利用非监督学习的方法.

10.3.2 Baum-Welch 算法

假设给定训练数据只包含s个长度为r的观测序列｛o„o2>--,os｝而没有对 应的状态序列，目标是学习隐马尔可夫模型2 =    的参数.我们将观测序

列数据看作观测数据0,状态序列数据看作不可观测的隐数据/，那么隐马尔可 夫模型事实上是一个含有隐变量的概率模型

P（O| A） = Yp（P\1,X）P（I\X）    （10.32）

它的参数学习可以由EM算法实现.

\1.    确定完全数据的对数似然函数

所有观测数据写成O = （Ol，02，…，外），所有隐数据写成/«乂，…，&），完全 数据是（0，/） = （01，02，"，，七，/|，4，"4）.完全数据的对数似然函数是108戸（0,/|2）.

\2.    EM算法的E步：求2函数2（又，X）®

0（人 J） =    | MP（O,I\1）    （10.33）

i

其中，I是隐马尔可夫模型参数的当前估计值，A是要极大化的隐马尔可夫模型 参数.

P（O，I\ 又）：W （hW 于是函数eoU）可以写成：

1

\+    y（OJ\X） + Z（Zlog 久（o,）y（O,/|I）

(10.34)



式中求和都是对所有训练数据的序列总长度r进行的.

①按照12函数的定义    _

e（A,J） = £/[logP（O,/|A）|Oj]

式（10.33）略去了对A而言的常数因子1/P（O|Z）.

\3. EM算法的M步：极大化2函数2GU)求模型参数迭况疋 由于要极大化的参数在式(10.34)中单独地出现在3个项中，所以只需对各

项分别极大化.

(1)式(10.34)的第1项可以写成：

_ N

£iog^p(o,^ =/|X)

/    i=l

N

注意到％满足约束条件£^=i,利用拉格朗日乘子法，写出拉格朗日函数：

(=i

Xlog^P(O,^ =/| J) +

1=1    V<=i )

对其求偏导数并令结果为0

去［写 log^P叫=||X) +    -1^ = 0

(10.35)



(10.36)



得

P(O，ij =/| J) + ^rf =0

对z‘求和得到/

Z=-P(O|J)

代入式(10.35)即得

1 P(O\I)

(2)式(10.34)的第2项可以写成

f    A    _ n n r-i

Z    \P(O,I I J) = 2：22：log a9p(o,i, = /,/f+I =7|J)

! \»=1    J    /=1 }=\ t=\

N

类似第1项，应用具有约束条件=1的拉格朗日乘子法可以求出

y=l

tp(o，i, =“+i =7)^)

av=J^~n-Z—    (10.37)

(3〉式(10.34)的第3项为

Z|Zlog^ (o,)W/| ^)=ZElogdy(O<)P(O,/, =y|X)

/、'=i    J    j=i(=i

M

同样用拉格朗日乘子法，约束条件是艺~(幻=1.注意，只有在0,=%时6/0，)对

*=i

6/幻的偏导数才不为0,以/(o,=v*)表示.求得

^p(o,i(=y|Xy(o, =v*)

bj(k) = ^-T-=——    (10.38)

(-1

10.3.3 Baum-Welch模型参数估计公式

将式(10.36)〜式(10.38)中的各概率分别用X(0，么G，7)表示，则可将相应 的公式写成：

%=丰、——    (10.39)

!>('•)

(-1

S X/O)

bj(k) = ^^——    (10.40)

'Lr.(j)

^ = zi(0

(10.41)



其中,《(:•，力分别由式(10.24)及式(10.26)给出.式(10.39)〜式(10.41)就 是Baum-Welch算法(Baum-Welch algorithm),它是EM算法在隐马尔可夫模型 学习中的具体实现，由Baum和Welch提出.

算法 10.4 (Baum-Welch 算法)

输入：观测数据^)：^©!^，…^);

输出：隐马尔可夫模型参数.

(1)    初始化

对》= 0,选取〜w, bj(k)m, <)，得到模型

(2)    递推.对”二口，…，

av



### I沾) '■1



IL r'G )

——

1＞(力

z=i

龙” W)

右端各值按观测^^久么…:…和模型沪^^^^^^计算.式中y,(i), 4；(/,y)由式(10.24)和式(10.26)给出.

(3)终止.得到模型参数义"^=04°^，沪■

###### 10.4预测算法

下面介绍隐马尔可夫模型预测的两种算法：近似算法与维特比算法(VKerbi algorithm).

10.4.1近似算法

近似算法的想法是，在每个时刻f选择在该时刻最有可能出现的状态＜，从 而得到一个状态序列尸=(«，…＜)，将它作为预测的结果.

给定隐马尔可夫模型A和观测序列0，在时刻/处于状态g,的概率z,G)是

⑽ A(')

N

7-1



A(砌 尸(0|1)



M') =



(10.42)



在每一时刻f最有可能的状态/；是

»；= argmax[r(0')]» t = l,2,-,r    (10.43)

从而得到状态序列/• =(«，•••，/；).

近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有 可能的状态序列，因为预测的状态序列可能有实际不发生的部分.事实上，上述 方法得到的状态序列中有可能存在转移概率为0的相邻状态，即对某些 %=0时.尽管如此，近似算法仍然是有用的.

10.4.2维特比算法

维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划 (dynamic programming)求概率最大路径(最优路径).这时一条路径对应着一个 状态序列.

根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻/通过 结点＜，那么这一路径从结点＜到终点与的部分路径，对于从＜到$的所有可能

的部分路径来说，必须是最优的.因为假如不是这样，那么从(到i；就有另一条 更好的部分路径存在，如果把它和从＜到达I了的部分路径连接起来，就会形成一 条比原来的路径更优的路径，这是矛盾的.依据这一原理，我们只需从时刻f = l开 始，递推地计算在时刻f状态为/的各条部分路径的最大概率，直至得到时刻f=r 状态为/的各条路径的最大概率.时刻f=r的最大概率即为最优路径的概率P*, 最优路径的终结点$也同时得到.之后，为了找出最优路径的各个结点，从终结 点芩开始，由后向前逐步求得结点么，…，/；，得到最优路径尸=(«，…，//).这 就是维特比算法.

首先导入两个变量J和定义在时刻/状态为I•的所有单个路径 中概率最大值为

^,(0 = 6=    I    i = l,2,—,N    (10.44)

由定义可得变量多的递推公式：

=    z = I,2,-,M f = l，2,…，T-l (10.45)

定义在时刻f状态为/的所有单个路径(以，…，U)中概率最大的路径的第 t-l个结点为

V, (0 = arg    (J)aji 1»    »' = 1,2,-,JV    (10.46)

下面介绍维特比算法.

算法10.5 (维特比算法)

输入：模型义二队丑⑻和观测^二以，^^，…^);

输出：最优路径/•=(«，•••<).

⑴初始化

(0 = ^(0,) , i = l,2,---,N ^(i) = 0,    i = l,2,---,N

(2)    递推.对f = 2,3,…，r

仙)=    久(A)， i = \2,-,N

K⑺=arg    (j)ayj], i = \,2,—,N

(3)    终止

h =argmax[^r(0]

(4)最优路径回溯.对r = r-l，r-2,…，1

< =^/+i(Ci)

求得最优路径r=(K, •••，/；).

下面通过一个例子来说明维特比算法.

^■ = (0.2,0.4,0.4)t



已知观测序列0 =(红，白，红)，试求最优状态序列，即最优路径广

解如图10.4所示，要在所有可能的路径中选择一条最优路径，按照以下步

骤处理：

(1)初始化.在/ = 1时，对每一个状态；，/ = 1,2,3»求状态为/观测o,为红 的概率，记此概率为戎(0,则

相) = 物)=林(红)，/ = 1,2,3

代入实际数据

4(1) = 0.10, <55(2) = 0.16, <55(3) = 0.28 记的(i) = 0， i = l，2,3.

状态

0.28    0.042    0.0147



3

2

2    3    时间

图10.4求最优路径

(2)在f = 2时，对每个状态i = l,2,3-求在r = l时状态为j•观测为红并在 t = 2时状态为f观测o2为白的路径的最大概率，记此最大概率为J2(f)，则

=    [^iC/)ayJ6,(o2)

同时，对每个状态:•，i = 1,2,3,记录概率最大路径的前一个状态 ^2(0 = argm^ lAG)%]，i = 1，2,3

计算:

么(1)=氏贸OXJ^CoJ

=max{0.10 x 0.5,0.16 x 0.3,0.28 x 0.2} x 0.5 = 0.028

妁(1) = 3

式(2) = 0.0504,的(2) = 3 式(3) = 0.042，％(3) = 3

同样，在/ = 3时，

^(0 = max[^2 ⑺…,]恤)

^3(i) = argmax[S2 (j)^]

戎(1) = 0.00756，(I) = 2 么(2) = 0.01008，叭(2) = 2 戎(3) = 0.0147，(3) = 3

(3)以产表示最优路径的概率，则

P* = mot J3 (/) = 0.0147

最优路径的终点是f3*:

C =argmax [么(f)] = 3

(4)由最优路径的终点    逆向找到« :

在 ^ = 2 时，= ⑶=3 在/ = 1 时，= K ⑶=3

于是求得最优路径.即最优状态序列广=««) = (3,3,3).    ■

###### 本章概要

1.隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链 随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的 序列的过程.

隐马尔可夫模型由初始状态概率向量;T、状态转移概率矩阵和观测概率矩 阵5决定.因此，隐马尔可夫模型可以写成Z =（忒5,疋）.

隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但 是状态序列是隐藏的，不可观测的.

隐马尔可夫模型可以用于标注，这时状态对应着标记.标注问题是给定观测 序列预测其对应的标记序列.

\2.    概率计算问题.给定模型4 = G4，B，;r）和观测序列O = （0|，o2，…，叫），计算 在模型/I下观测序列O出现的概率1 zl）.前向-后向算法是通过递推地计算前 向-后向概率可以髙效地进行隐马尔可夫模型的概率计算.

\3.    学习问题.已知观测序列o=（Ol，o2，…，or）,估计模型a=（y）参数， 使得在该模型下观测序列概率最大.即用极大似然估计的方法估计参 数.Baum-Welch算法，也就是EM算法可以髙效地对隐马尔可夫模型进行训练.它 是一种非监督学习算法.

\4.    预测问题.已知模型2 = （45，龙）和观测序列＜? = （01，02，...，叫），求对给定 观测序列条件概率P（/10）最大的状态序列/ =认人，…人）.维特比算法应用动态 规划髙效地求解最优路径，即概率最大的状态序列.

###### 继续阅读

隐马尔可夫模型的介绍可见文献［1,2］,特别地，文献［1］是经典的介绍性论文. 关于Baum-Welch算法可见文献［3,4］.可以认为概率上下文无关文法（probabilistic context-free grammar）是隐马尔可夫模型的~种推广，隐马尔可夫模型的不可观 测数据是状态序列，而概率上下文无关文法的不可观测数据是上下文无关文法树［5】. 动态贝叶斯网络（dynamic Bayesian network）是定义在时序数据上的贝叶斯网络， 它包含隐马尔可夫模型，是一种特例［6］.

10.1给定盒子和球组成的隐马尔可夫模型;l =    其中，

|      | 0.5 0.2 0.3n |       | 0.5 0.5' |
| ---- | ------------ | ----- | -------- |
| A =  | 0.3 0.5 0.2  | ，B = | 0.4 0.6  |
|      | 0.2 0.3 0.5  |       | 0.7 0.3  |

龙= (0.2,0.4,0.4)t

设r=4，o=（红，白，红，白），试用后向算法计算p（o|X）.

10.2考虑盒子和球组成的隐马尔可夫模型2 = （X，5，；r）,其中，

|      | 0.5 0.1 0.4" |       | '0.5 0.5' |
| ---- | ------------ | ----- | --------- |
| A =  | 0.3 0.5 0.2  | ，B = | 0.4 0.6   |
|      | 0.2 0.2 0.6  |       | 0.7 0.3   |



^ = （0.2,03,0.5/

设r=8，0=（红，白，红,红，白,红,白，白），用前向后向概率计算叩4=込|＜9，乂）. 10.3在习题lo.i中，试用维特比算法求最优路径r =（＜,«＜）.

10.4试用前向概率和后向概率推导

N N

Wla）=2S«；0XA（o«+.）A*.（7）. /=i,2,-,r-i

10.5比较维特比算法中变景5的计算和前向算法中变量a的计算的主要区别.



###### 参考文献

[1]    Rabiner L, Juang B. An introduction to hidden markov Models. IEEE ASSP Magazine, January 1986

[2]    Rabiner L. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of IEEE, 1989

[3]    Baum L, et al. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics, 1970,41: 164-171

[4]    Bilmes JA. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models, <http://ssli.ee.washington.edu/~bilmes/mypubs/> bilmes1997-em.pdf

[5]    Lari K, Young SJ. Applications of stochastic context-free grammars using the Inside-Outside algorithm, Computer Speech & Language, 1991, 5(3): 237-257

[6]    Ghahramani Z. Learning Dynamic Bayesian Networks. Lecture Notes in Computer Science, Vol. 1387,1997,168-197
