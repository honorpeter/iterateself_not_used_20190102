---
title: 深度学习的前世今生
toc: true
date: 2018-08-29
---



## 神经科学的研究

深度学习并不是什么复杂神秘的新方法，本节将带领大家一起去了解深度学习的前世 今生。



控制论，诺伯特•维纳(Norbert Wiener)提出了著名的控制论。<span style="color:red;">控制论也要学习的。</span>


基于符号系统进行推理和演算的研究，艾伦•纽厄尔(Allen Newell)和赫伯特•西蒙(Herbert Simon)在当时是这个领域的活跃研究者，他们于1955 年开发的程序证明了《数学原理》中前 52 个定理中的 38 个。<span style="color:red;">基于符号系统的推理和演算是怎么实现的？这个程序现在还有吗？是怎么实现的？想知道。</span>


神经元学说：1903年，西班牙科学家圣地亚哥•拉蒙•卡哈尔(Santiago Ramony Cajal)改良了高尔基染色法，首次观察到了神经细胞里更细微的结构，并于1904年发表 著作确立了神经元学说。其中提出的神经活动的基本原则，成了后来启发人工神经网络理论的仿生基础。<span style="color:red;">想知道现在的神经元学说发展到什么程度了？以及到底与现在的深度神经网络有哪些差别？</span>


神经元的结构示意图：

![mark](http://images.iterate.site/blog/image/180829/2C7be4f4lF.png?imageslim)


一个神经元中在细胞周围会有多个呈放射状、杂乱而粗短的突起结构，称为树突。每个神经元会有一个细长而均匀的突起结构，称为轴突。轴突的末端有许多分 支，被称为轴突末梢。每个神经元从树突接收输入的信号，当某一时刻累计的信号 超过某一个阈值时，通过轴突释放传出。轴突末梢连接其他神经元的树突，形成一 个复杂的网络。


## 神经网络的兴起


M-P 模型：

![mark](http://images.iterate.site/blog/image/180829/AKf37fCcCD.png?imageslim)


1943年，美国神经生理学家沃伦•麦克洛奇 (Warren McCulloch)和逻辑学家沃尔特•匹茨(Walter Pitts) 一起提出了一种简单的计算模型来模拟神经元， 我们把这种模型称为M-P模型。<span style="color:red;">以前我好像没听说 M-P 模型。还是说我遗漏了这个？好像都是从感知机直接开始说的。</span>

M-P模型的论文是人类历史上第一次对人工神经网络的系统性研究。在M-P模型中，所有的值都是 0 和1，所有输入节点模拟神经元的树突，输入信号的求和通过一个阈值 0 比较决定输出值来模拟信号的输出。M-P模型表达成公式的形式如下：

![mark](http://images.iterate.site/blog/image/180829/CKL7b57le2.png?imageslim)

可以看出，M-P模型中所有的值都是0和1,可以用来模拟最基本的二进制逻辑。然 而这也成了 M-P模型的局限性，即只有用于模拟信号电平的二值(0和1)，并且每个输入信号的权重都是一样的。1949年，加拿大心理学家唐纳德•赫布(Donald Hebb)提出了赫布学习(Hebbian Learning)规则。大意是神经系统响应一个信号时，同时被激活的神经元之间的联系会被强化。以这种规律为基础，赫布将神经元之间连接的强弱以权值的方式引入了计算模型，并且提出了一套修改权值的规则让模型可以“学习”。<span style="color:red;">这个赫布学习规则想详细的了解下，是不是真的有这个规则？以及现在的激活函数对这个规则的模拟到了什么程度？以及想知道这样的规则是怎么在神经系统中形成的？</span>

在 M-P 模型的基础上和赫布学习规则的启发下，1956年，美国心理学家弗兰克•罗森布拉特(Frank Rosenblatt)提出了著名的感知机(Perceptron)。这种模型不仅在 M-P 模型基础上加入了权值，并且跳出了二值的限制，最重要的是，罗森布拉特给出了这种模型的权值修改方法：通过公式记录感知器对训练数据的正确率，然后根据正确率对权值进行更新。这样就相当 于感知机进行了针对某个数据任务的“训练”。不过总体而言这种“训练”是半手动的，非常低效。感知机提出后，罗森布拉特用硬件电路做了简单的实现，并且完成了一些在那个年代非常激动人心的任务，如通过光传感器阵列来模拟视网膜并识别一些简单的字母。<span style="color:red;">是呀，的确是激动人心的。</span>

感知机的结构非常简单，相比起M-P模型更加灵活，后来成为了人工神经网络的基础。

## 神经网络的第一次寒冬

当时在人工智能领域除了以罗森布拉特为首的山寨大脑神经元结构的连接主义 (connectionism)外，还有另一个流派，他们推崇的是从逻辑和思维的角度出发，把思维分解成一个个可以用符号操作表示的单元，这一派称为符号主义(Symbolism)，更偏向于推理和逻辑。<span style="color:red;">感觉上两个派都有道理，一个是相当于让他自发产生，一个是严格的控制它产生。嗯，不一定对，想详细的了解下这两个派别。</span>

有人的地方，就有江湖。作为当时人工智能领域最大的两个流派，他们并没有和平相处。符号派的代表人物是人工智能的奠基人之一，也是前面提到过的达特茅斯 会议的组织者之一马文•明斯基。明斯基事实上最早是连接主义学派的，1954年他在普林 斯顿的博士毕业论文题目是《Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain Model Problem》。在第一次达特茅斯会议上他也是连接主义的支持 者，后来才转向了符号主义，并成为了领袖级的人物。明斯基和罗森布拉特在高中时期就 是师兄弟，不知是不是那个时候就结下了 “梁子”，在他们的研究生涯中，这两人总是互相怀有敌意。

这两派的斗争在1969年暂时分出了高下，明斯基在那一年出版了一本《感知机：计算 几何学》书(Perceptrons: An Introduction to Computational Geometry),这本书看标题是讲 感知机，但是真正产生影响的内容是书中对感知机限制的描述。在那个年代，机器的计算能力还非常有限，所以多层的感知机网络(如图1-4所示)的计算是不实际的。而单层感 知机的表达能力又非常有限，明斯基在书中用单层感知机不能解决异或逻辑的例子指出了 感知机在实用层面的局限性，并断定感知机的研究没有前途。除了对感知机局限的理性论 述，这本书中还用一些非常刻薄的语句来攻击相关研究，如在第一版序言中所写“大部分 关于感知机的研究都是没有科学价值的”(Perceptrons have been widely publicized as ''pattern recognition” or ’’learning” machines and as such have been discussed in a large number of books journal articles and voluminous ’’reports”. Most of this writing... is without scientific value )。

1969年是个非常微妙的时间点，距感知机的大红大紫已经过去了近10年，却并没有出现之前预期中令人激动的成果，相关研究的进展也越来越慢。明斯基作为人工智能领域的泰斗之一，他对神经网络了如指掌，在这个时间点发布了这样一部直指连接主义缺陷的著作， 这对连接主义学派而言无疑是一个致命的打击。随着明斯基著作的发表，许多学者纷纷离 开神经网络的研究领域，很多政府资助也被停止。神经网络的研究走入低谷，迎来了第一 次寒冬。事实上，研究者和资金的撤离不仅仅限于神经网络方向，而是整个人工智能领域。 因为无论是人工智能领域的哪个学派，在人工智能刚开始蓬勃发展的时候都对这个领域做 出了过于乐观的预期，而随着时间的推移，研究的进展并没有达到研究者和媒体渲染的那 样。伴随神经网络寒冬一同陨落的还有罗森布拉特本人。1971年7月11日，罗森布拉特 在43岁生日的当天，在美国东海岸的切萨皮克湾(Chesapeake Bay)泛舟时不幸遭遇事故 丧生。



## 神经网络的第一次复兴

在沉寂了近10年之后，神经网络的研究又开始慢慢复苏。1982年，加州理工的生物 物理学家约翰•霍普菲尔德(John Hopfield)提出了一种反馈型神经网络，即Hopfield网 络。和Hebbian学习规则一样，Hopfield网络提出的最初目的是偏向于神经学科，用于研 究记忆的机制，而结果是这种网络成功地解决了一些识别和约束优化的问题，引起了很大的反响。这让神经网络领域的连接主义研究者们非常振奋，渐渐地许多研究者又将目光再 次移向了神经网络。同时一些新的研究中心建立起来，并开始吸引许多老一辈的神经网络 研究者和后起之秀。这其中就有后来的深度学习“祖师爷”之一的杰弗里•辛顿(Geoffrey Hinton) 。

在第一次神经网络的寒冬开始时，辛顿刚刚开始读研究生。凭借着对神经网络的浓厚 兴趣，他并没有在这段寒冬岁月放弃对神经网络的研究。到了 1978年，他在爱丁堡大学完 成了博士学习，然后历经辗转，于1980年到了美国加州大学圣迭戈分校(UCSD)攻读博士后。那时UCSD正走在连接主义学派复兴的前沿，其中两位代表人物是两位心理学家大 卫•鲁梅哈特(David Rumelhart)和詹姆斯•麦克利兰德(James McClelland)。在那里辛 顿加入了 UCSD 认知科学中心的研究小组，并与鲁梅哈特和麦克利兰德建立了良好的合作。

两年后辛顿到了卡耐基梅隆大学，于1986年和鲁梅哈特合作发表了论文《Learning Representations by Back-Propagating Errors》，提出了后向传播算法(Back Propogation，简 称BP算法)。BP算法解决的正是明斯基 在《感知机：计算几何学》中指出的双层网络难以训练的问题。BP算法使一层以上的神经 网络进入实用阶段，开启了第二轮神经网络的研究热潮。1987年，鲁梅哈特转到了斯坦福 大学当教授，在这里他培养出了后来的机器学习大神，继承了他图论造诣的迈克尔•乔丹 (Micheal Jordan)。而乔丹后来的学生里又出现了两个深度学习的代表人物，一个是博士生吴恩达(Andrew Ng)，还有一个是博士后，即后来深度学习创始人之一约书亚•本吉奥(YoshuaBengio)。<span style="color:red;">厉害了，BP 算法是真的厉害。</span>

再来看辛顿，在祭出了复兴神经网络的“大杀器” BP算法后，又几经辗转，于 1987 年到了多伦达大学当教授。在这里他又遇到了后来深度学习的又一个始祖级人物 YannLeCun，YannLeCun 出生于法国，并在那里完成了博士的所有学业。就在辛顿到了多 伦多的同一年，他也来到了多伦多大学，成为辛顿组里的一名博士后。在那里他做了一些 关于 BP 算法的理论研究，并在一年后加入了贝尔实验室(AT&T Bell Lab)。在贝尔实验室，基于后向传播算法，杨乐昆提出了后来名满天下，第一个真正意义上的深度学习，也是目前深度学习中应用最广的神经网络结构 - 卷积神经网络(Convolutional Neural Networks)。卷积神经网络在当时就对手写字母达到了很高的识别率，并被广泛应用于欧美许多大银行的自动手写识别系统。<span style="color:red;">厉害了，原来每个大神都开创过一个新的大杀器，并且非常厉害。</span>

1989年，美国应用数学家乔治•塞班克(George Cybenko)证明了神经网络可以被看作是一个通用逼近函数，一个隐藏层的网络可以逼近任意连续函数，两个隐藏层的网络可 以逼近任意函数。这个理论在后来又被奥地利数学家科特•霍尼克(Kurt Homik)完善，也就是说神经网络的拟合能力是接近无限强的，任意复杂的分类决策边界都可以被逼近， 至此神经网络的研究又进入了一个新的高潮。<span style="color:red;">想知道这个是怎么证明的，想详细了解下这个过程。</span>

## 神经网络的第二次寒冬

虽然BP算法将神经网络带入了实用阶段，可是当时的神经网络仍然存在一些缺陷。首先是神经网络的层数，随着研究的深入，人们发现BP算法的一个缺点是梯度计算存在 不稳定的问题。简单来说就是越远离输出层的参数越难以被训练，要么会不变，要么会变化过于剧烈，这被称为梯度消失/爆炸问题，而且层数越多，这个问题越明显。

说到这里又要引出一个深度学习的始祖级人物，现居瑞士的德国计算机科学家尤尔根•施米德休 (Jurgen Schmidhuber)，梯度传播中的消失/爆炸问题正是由他的第一个学生赛普•霍克 莱特(Sepp Hochreiter)在毕业论文中第一次正式提出并讨论。不知道是不是因为住的离美国太远，提到深度学习的时候人们往往提到的是和他贡献差不多的另外三巨头辛顿、杨乐昆和本吉奥。而实际上在深度学习领域施米德休也是举足轻重的，他从20世纪90年代 就开始研究深度神经网络，从时间上来看他和杨乐昆应该是最早研究深度学习的两个人。 他提出的长短期记忆网络(Long-Short Term Memory, LSTM)，在语音识别和自然语言处理领域产生了巨大的影响。2016年初靠着 AlphaGo 大红大紫的公司 DeepMind 里也有两个创始人都是他的学生。<span style="color:red;">LSTM 也是厉害的</span>

言归正传，一方面 BP 算法只对浅层网络有效，另一方面两三层的神经网络就已经有了足够强的拟合能力，所以当时应用层面的研究很多都集中在浅层神经网络，对深层网络的了解总体而言知之甚少。

除了浅层的限制，参数过多也成了神经网络被诟病的问题之一。针对一些识别问题， 因为底层细节的不直观和过于强大的拟合能力，再加之那个年代很多问题的训练数据量都不大，神经网络的泛化能力确实有隐患，所以很多人认为神经网络就是某种程度上的过拟合，甚至今天很多人还是对深度学习抱有类似的观点。神经网络的调参也是个巨大的问题， 因为神经网络的结构让使用者并不关注细节，而只需要关注输入输出，选择诸如隐层数、 单元数等参数往往会显得有些无迹可寻，相关的理论研究也乏善可陈，所以调参往往成了一项经验活。另外相对应大量的参数也对计算机的算力提出了要求。除了参数相关的问题, 因为BP算法本身依赖于梯度，所以训练陷入局部最小值也成了神经网络的一个大问题。<span style="color:red;">隐层数、 单元数等参数 现在对这方面的研究怎么样了？</span>

进入20世纪90年代中期，神经网络的发展又进入了一个相对缓慢的阶段，而那时，一个机器学习领域内的“大神”杀了出来，他就是支持向量机(Support Vector Machine, SVM)的提出者，统计学家弗拉基米尔•万普尼克(Vladimir Vapnik)。<span style="color:red;">厉害了，SVM，尤其是各种 kernel </span>

万普尼克也是第 一批从事人工智能的老一辈研究者，在感知机风靡的年代，他也在那股热潮下做了很多研究，并于1963年提出了原始的SVM。不过那个时候他的 SVM 和感知机差别并不大，再加上他人在莫斯科，没有产生大的影响。在随后的岁月里，万普尼克一直在莫斯科控制科学研究所潜心研究偏理论的方向，一直做到了研究所计算机部门的老大。1990年，前苏联 动荡，万普尼克移居美国加入了贝尔实验室，成了杨乐昆的同事。1995年，万普尼克正式 提出了统计学习理论，并将该方法应用到了 SVM。虽然广义上来讲，SVM 也是个浅层网络，但相比当时其他浅层神经网络，SVM 拥有全局最优、调参简单、泛化能力强等优点， 并且还有完善的理论支撑。更重要的是，SVM 诞生后，在当时的一些诸如手写体识别的问题上一举击败了其他各种浅层神经网络，迅速成了研究的主流。就这样，神经网络进入了第二次寒冬。<span style="color:red;">厉害了。</span>

## 2006年——深度学习的起点

在科研领域总是有这么一些人，无论热点和资金在哪，他们都不会动摇。<span style="color:red;">是的。</span>

在第二次神经网络的寒冬到来时，辛顿、杨乐昆、本吉奥和尤尔根等仍然坚持着神经网络的研究。到 了 21世纪初的时候，随着随机森林(Random Forest) 和 AdaBoost 等方法的兴起，神经网络研究的处境更是雪上加霜，根据辛顿和杨乐昆的回忆，那个时候他们的学生完成的和神 经网络沾边的文章被拒是家常便饭。其实和SVM等方法比起来，神经网络有一个非常大的优点即分布式表征(Distributed Representation)。<span style="color:red;">什么是分布式表征？之前好像没听说过？</span>

在浅层神经网络中，虽然这个优点也有，但并不容易体现出来，所以一个重要的努力方向是克服BP算法的误差传播问题，将网络变得更深。1997年，尤尔根提出了 LSTM,在一定程度上解决了 BP 算法中的误差传播问题，形成了一定的影响，但受到计算能力等因素的影响，当时并没有流行起来。

到了 2004 年，神经网络方向的研究进入了最低谷，这年辛顿的重要资金来源加拿大高等研究所(Canadian Institute For Advanced Research, CIFAR)，又到了评审项目资助的时候。因为神经网络的颓势，CIFAR在当时 已经是极少数还愿意支持神经网络相关研究的机构，当时除了辛顿以外所有其他的受资助 人都在做其他方向的研究。在这种举步维艰的情况下，辛顿终于还是成功说服 CIFAR 的评 审委员，拿到了一笔不大的资助让研究得以维持。在这笔资助下，经过两年的卧薪尝胆， 辛顿在 2006 年发表了一篇突破性的文章《A Fast Learning Algorithm for Deep Belief Nets》， 这篇论文里辛顿介绍了一种成功训练多层神经网络的办法，他将这种神经网络称为深度信念网络。深度信念网络的基本思想是用一种叫受限波尔兹曼机(Restricted Boltzmann Machine, RBM)的结构得到生成模型(Generative Model)。受限波尔兹曼机的结构如 图1-5所示。<span style="color:red;">这个就是 受限波尔兹曼机 吗？什么是受限玻尔兹曼机？什么是深度信念网络？为什么我之前对这个好像没什么印象？</span>

![mark](http://images.iterate.site/blog/image/180829/2i1IJd2g8L.png?imageslim)

要注意的是，和常见的神经网络不同，RBM的结构是无向图，本质上讲是一个马尔科夫随机场。简单来理解，可以把 RBM 看作一个输入+单隐层的网络，层内无连接，层间是全连接。其中图1-5中的单元 名称为 v 的是输入层，对应观测(visible)到的数据，h 的一层对应于隐(hidden)变量。训练 RBM 的效果是可以让网络学习及产生输入数据，也就是 v 的分布。<span style="color:red;">为什么这么重要的东西我现在才注意到？之前看到过几次提到 深度信念网络的，但是现在才真正注意到。</span>

对于深度神经网络而言，RBM 的原理可以简单理解如下：首先学习大量无标注数据， 然后在隐藏层提取出数据的特征，而这个特征就可以重现数据的分布。如果隐层单元数量 远小于可见单元的数量，则相当于降维，如果隐层单元大部分都是0,则相当于学习到了稀疏的表示。而当这样的结构学习到之后，就有了每个连接的权重，这时候把 RBM 的结构当作有向图来使用，就相当于得到了一个最简单的神经网络，并且学到了一层特征。<span style="color:red;">什么意思？</span>

而 如果接下来继续这个过程，把学到的特征作为另一个RBM的输入进行学习，则相当于又学到了更加高层的表示，于是一层层堆起来就成了一个深层的网络，也就是文章标题里提到的深度信念网络(DBN)。

这样一个网络已经有了学习到的权重，可以看作是对网络一个非常好的初始化，这个过程叫做预训练(pre-train)。然后再按照传统的BP算法，结合标注的数据对这个网络进行训练，此时预训练的权重相当于把网络初始化到了一个很好的起点上，所以BP算法只需要进行局部的搜索就可以收敛到一个不错的结果，这个BP算法训练过程也叫做微调(finetune)。<span style="color:red;">哎，这个训练的方式，现在都是这么训练的吗？好像没有看到提别着重强调的？</span>

无论是RBM，还是这种一层层训练的办法，在今天的大部分深度学习应用中看来已经没什么特别的优势。<span style="color:red;">现在是已经不使用这种初始化的方式了吗？</span>

但当时这个办法颠覆了之前被学术界大部分人默认的深度网络不能被 训练的观点。另外，还有可以利用无标注数据的优点。更重要的是，DBN一出，立刻在效 果上打败了风光已久的SVM，这让许多研究者的目光重新回到了神经网络。与辛顿发表论 文的同一年，本吉奥和杨乐昆也在之后的神经信息处理系统会议(Conference on Neural Information Processing Systems, NIPS)上发表了两篇深度网络的论文。一篇探讨了辛顿的方法并比较了基于自编码机(auto-encoder)的深度网络，一篇讨论和辛顿方法类似的办法， 用于初始化卷积神经网络，并在手写数字识别上达到了当时最好的效果。这三篇论文算是重振神经网络的开山之作，而这三位研究者现在也是领域内公认的三巨头。可能是之前神经网络不招人待见的经历让辛顿郁闷了太久，所以当初他拿到 CIFAR 资助的时候就给接下来的研究想了一个新的名字 Deep Learning，于是深度学习正式登场。<span style="color:red;">嗯，原来是这样的。</span>

Deep Learning 并不是2006年才出现的新词，最早正式在学术刊物里出现是1986 年。加州大学尔湾分校(University of California，Irvine )计算机系的教授丽娜•德 科特(Rina Dechter)在发表的论文 «Learning While Searching in Constraint-Satisfaction Problems》里第一次提出了 Deep Learning这个词。




## 相关资料

- 《深度学习与计算机视觉》
