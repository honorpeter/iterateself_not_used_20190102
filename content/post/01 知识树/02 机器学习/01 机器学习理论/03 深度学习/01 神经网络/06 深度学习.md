---
title: 06 深度学习
toc: true
date: 2018-06-27 12:39:27
---





深度学习


理论上来说，参数越多的模型复杂度越高、“容量”(capacity)越大，这意味着它能完成更复杂的学习任务。但一般情形下，复杂模型的训练效率低，易陷入过拟合，因此难以受到人们青睐。而随着云计算、大数据时代的到来，计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过故合风险, 因此，以 “深度学习” (deep learning) 为代表的复杂模型开始受到人们的关注。

典型的深度学习模型就是很深层的神经网络。显然，对神经网络模型，提高容量的一个简单办法是増加隐层的数目。隐层多了，相应的神经元连接权、阈值等参数就会更多。模型复杂度也可通过单纯增加隐层神经元的数目来实现，前面我们谈到过，单隐层的多层前馈网络已具有很强大的学习能力；但从增加模型复杂度的角度来看，増加隐层的数目显然比增加隐层神经元的数目更有效， 因为增加隐层数不仅增加了拥有激活函数的神经元数目，还増加了激活函数嵌套的层数。

然而，多隐层神经网络难以直接用经典算法(例如标准BP算法)进行训练，因为误差在多隐层内逆传播时，往往会 “发散”(diverge) 而不能收敛到稳定状态。

无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入，这称为 “预训练” (pre-training)<span style="color:red;">这个地方有一点非常想知道：怎么知道这层输出的是不是我想要的？即这层的输出怎么判断误差是多少？</span>；在预训练全部完成后，再对整个网络进行 “微调”(fine-tuning) 训练。例如，在深度信念网络(deep belief network,简称DBN) [Hinton et al., 2006]中<span style="color:red;">什么是深度信念网络？</span>，每层都是一个受限 Boltzmann 机，即整个网络可视为若干个 RBM 堆叠而得。在使用无监督逐层训练时，首先训练第一层，这是关于训练样本的RBM模型，可按标准的RBM训练；然后，将第一层预训练好的隐结点视为第二层的输入结点，对第二层进行预训练 <span style="color:red;">怎么对第二层进行训练的？第二层的输出的误差怎么得到？</span>；... 各层预训练完成后，再利用 BP 算法等对整个网络进行训练。

事实上，“预训练+微调” 的做法可视为将大量参数分组，对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优. 这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销。<span style="color:red;">嗯，但是这个局部最优值怎么算出来的？</span>

另一种节省训练开销的策略是 “权共享” (weight sharing)，即让一组神经元使用相同的连接权。这个策略在卷积神经网络(Convolutional Neural Network,简称 CNN) [LeCun and Bengio, 1995; LeCun et al., 1998]中发挥了重要作用。以CNN进行手写数字识别任务为例[LeCun et al., 1998],如图 5.15

![mark](http://images.iterate.site/blog/image/180627/jC9J8aFl3E.png?imageslim)


所示，网络输入是一个 32x32 的手写数字图像，输出是其识别结果，CNN 复合多个 “卷积层” 和 “采样层”对输入信号进行加工，然后在连接层实现与输出目标之间的映射。每个卷积层都包含多个特征映射(feature map)，每个特征映射是一个由多个神经元构成的 “平面”，通过一种卷积滤波器提取输入的一种特征.例如，图5.15中第一个卷积层由6个特征映射构成，每个特征映射是一个 28x28 的神经元阵列，其中每个神经元负责从 5x5 的区域通过卷积滤波器提取局部特征.采样层亦称为 “汇合”(pooling) 层，其作用是基于局部相关性原理进行亚采样，从而在减少数据量的同时保留有用信息。例如图5.15中第一个采样层有6个14x14的特征映射，其中每个神经元与上一层中对应特征映射的2x2邻域相连，并据此计算输出。通过复合卷积层和采样层，图5.15中的 CNN 将原始图像映射成120维特征向量，最后通过一个由84个神经元构成的 连接层和输出层连接完成识别任务。CNN 可用 BP 算法进行训练，但在训练中，无论是卷积层还是采样层，其每一组神经元 (即图5.15中的每个“平面”) 都是用相同的连接权，从而大幅减少了需要训练的参数数目。



我们可以从另一个角度来理解深度学习.无论是 DBN 还是 CNN，其多隐层堆叠、每层对上一层的输出进行处理的机制，可看作是在对输入信号进行逐层加工，从而把初始的、与输出目标之间联系不太密切的输入表示，转化成与输出目标联系更密切的表示，使得原来仅基于最后一层输出映射难以完成的任务成为可能。

换言之，通过多层处理，逐渐将初始的 “低层” 特征表示转化为“高层”特征表示后，用 “简单模型” 即可完成复杂的分类等学习任务.由此可将深度学习理解为进行 “特征学习” (feature learning)或 “表示学 习” (representation learning)。<span style="color:red;">嗯，是的</span>

以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这称为 “特征工程” (feature engineering)。众所周知，特征的好坏对泛化性能有至关重要的影响，人类专家设计出好特征也并非易事；特征学习则通过机器学习技术自身来产生好特征，这使机器学习向 “全自动数据分析” 又前进了一步。<span style="color:red;">是呀。不知道现在的进展怎么样了。</span>





# COMMENT


下面是补充阅读的内容

2012年前的名称是 IEEE Transactions on Neural Networks.

近来NIPS更偏重于机 器学习.


LMS 亦称 Widrow-Hoff 规则或(5规则.


［Haykin，1998］是很好的神经网络教科书，［Bishop, 1995］则偏重于机器学 习和模式识别.神经网络领域的主流学术期刊有Neural Computation、Neural Networks、 IEEE Transactions on Neural Networks and Learning Systems; 主要国际学术会议有国际神经信息处理系统会议(NIPS)和国际神经网络联合 会议(IJCNN)，区域性国际会议主要有欧洲神经网络会议(ICANN)和亚太神经 网络会议(ICONIP).

M-P神经元模型使用最为广泛，但还有一些神经元模型也受到关注，如考 虑了电位脉冲发放时间而不仅是累积电位的脉冲神经元(spiking neuron)模型 ［Gerstner and Kistler, 2002］.

BP 算法由［Werbos, 1974］首先提出，此后［Rumelhart et al.5 1986a，b］重新 发明.BP算法实质是LMS (Least Mean Square)算法的推广.LMS试图使网 络的输出均方误差最小化，可用于神经元激活函数可微的感知机学习；将LMS 推广到由非线性可微神经元组成的多层前馈网络，就得到BP算法，因此BP算 法亦称广义 S 规则［Chauvin and Rumelhart, 1995］.

［MacKay, 1992］在贝叶斯框架下提出了自动确定神经网络正则化参数的 方法.［Gori and Tesi，1992］对BP网络的局部极小问题进行了详细讨论.［Yao, 1999］综述了利用以遗传算法为代表的演化计算(evolutionary computation)技 术来生成神经网络的研究工作.对BP算法的改进有大量研究，例如为了提速， 可在训练过程中自适应缩小学习率，即先使用较大的学习率然后逐步缩小，更 多“窍门” (trick)可参阅［Reed and Marks, 1998; Orr and Muller, 1998］.

关于 RBF 网络训练过程可参阅［Schwenker et al., 2001］. ［Carpenter and Grossberg，1991］介绍了 ART族算法.SOM网络在聚类、高维数据可视化、 图像分割等方面有广泛应用，可参阅［Kohonen，2001］, ［Benglo et al., 2013］综 述了深度学习方面的研究进展.

神经网络是一种难解释的“黑箱模型”，但已有一些工作尝试改善神经 网络的可解释性，主要途径是从神经网络中抽取易于理解的符号规则，可参阅 ［Tickle et al.，1998; Zhou, 2004］.




## 相关资料
1. 《机器学习》周志华
