---
title: DL 卷积神经网络经典结构
toc: true
date: 2018-06-12 22:19:01
---



## 相关资料






  1. 《解析卷积神经网络》魏秀参




## 需要补充的






  * aaa





* * *





# INTRODUCTION






  * aaa










卷积神经网

激活函数、全 模型就是这些基本部件的按序层叠,可 “有机组合”才能让模型工作、发挥效 三个重要概念,随之以四类典型的卷积

3.1

要概念
(receptive filed)原指听觉、视觉等神经系统中一些神经元的特性，即 只接受其所支配的刺激区域内的信号。在视觉神经系统中,视觉皮层中 胞的输出依赖于视网膜上的光感受器。当光感受器受刺激兴奋时,会将

动信号传导至视觉皮层。不过需指出并不是所有神经皮层中的神经元都 这些信号。如第1.1节我们提到的一样，正是由于感受野等功能结构在猫 中枢中的发现,催生了福岛邦彦的带卷积和子采样操作的多层神经网络。 代卷积神经网络中的感受野又是怎样一回事？我们慢慢道来。先以单层

3.1. CNN网络结构中的重要概念

卷积操作为例（图3.1a）,如图是一个7 x 7,步长为1的卷积操作，对后层的 每一个输出神经元（如紫色区域）来说,它的前层感受野即为黄色区域,可以 发现这与神经系统的感受野定义大同小异。不过,由于现代卷积神经网络拥有

（a）单层卷积中后层神经元对应的前层感受野 （b）多层卷积中后层神经元对应的前层感受野

（黄色区域）。图中卷积核大小为7 X 7,步长为 （黄色区域）。图中卷积核大小为3 X 3,步长为

1。 1。

图3.1:填充（padding）操作示例。

也就是说，小卷积核（如3 x 3）通过多层叠加可取得与大卷积核（如7 x 7） 同等规模的感受野,此外采用小卷积核同时可带来其余两个优势：第一,由于 小卷积核需多层叠加，加深了网络深度进而增强了网络容量（model capacity） 和复杂度（model complexity）;第二，増强网络容量的同时减少了参数个数。

若假设上述示例中卷积核对应的输人输出特征张量的深度均为C,则7 x 7卷 积核对应参数有C x (7 x 7 x C) = 49C11个。而三层3 x 3卷积核堆叠只需三倍 单层3 x 3卷积核个数的参数，即3 x [C x (3 x 3 x C)] = 27C11,远小于7 x 7 卷积核的参数个数。

此外，需指出的是，目前已有不少研究工作为提升模型预测能力通过改造现

有卷积操作试图扩大原有卷积核在前层的感受野大小，或使原始感受野不再是

矩形区域而是更自由可变的形状，对以上内容感兴趣的读者可参考“扩张卷积

操作”(dilated convolution) [93]和“可变卷积网络”(deformable convolutional networks) [12]。

3.1.2分布式表示

众所周知，深度学习相比之前机器学习方法的独到之处是其表示学习部分。

但仍需强调，深度学习只是表示学习(representation learning)的一种方式。 深度学习兴起之前，就有不少关于表示学习的研究，其中在计算机视觉中 比较著名的就是“词包”模型(bag-of-word model)。词包模型源自自然语 言处理领域，在计算机视觉中，人们通常将图像局部特征作为一个视觉单词

(visual word),将所有图像的局部特征作为词典(vocabulary),那么一张图 像就可以用它的视觉单词来描述，而这些视觉单词又可以通过词典的映射形

成一条表示向量(representation vector)。很显然，这样的表示是离散式表示 (distributional representation),其表示向量的每个维度可以对应一个明确的视 觉模式(pattern)或概念(concept)。词包模型示意图如图3+2所示。

不同的是，在深度学习中，深度卷积神经网络呈现“分布式表示” (distributed representation) [4, 37]的特性。神经网络中的“分布式表示”指“语义概 念” (concept)到神经元(neuron)是一个多对多映射，直观来讲，即每个语义 概念由许多分布在不同神经元中被激活的模式(pattern)表示；而每个神经元 又可以参与到许多不同语义概念的表示中去。

举个例子，如图3+3所示，将一些物体为中心的图像(object-centric images) 送人在ImageNet数据集[73]上预训练(pre-train)的卷积网络1，若输人图像

3.1. CNN网络结构中的重要概念

图 3.2:词包模型(bag-of-word model)示意

分辨率为224 x 224,则最后一层汇合层（pool5）可得7 x 7 x 512大小的响应 张量（activation tensor）,其中“512”对应了最后一层卷积核的个数，512个 卷积核对应了 512个不同的卷积结果（512个特征图或称“通道”）。可视化时, 对于“鸟”或“狗”这组图像对,我们分别从512张7x7的特征图（feature map）中随机选取相同的4张，并将特征图与对应原图叠加，即可得到有高亮 部分的可视化结果。从图中可明显发现并证实神经网络中的分布式表示特性。 以鸟类这组图像为例，对上下两张“鸟”的图像，即使是同一卷积核（第108 个卷积核）但在不同原图中响应（activate）的区域可谓大相径庭：对上图，其 响应在鸟爪部位；对下图,其响应却在三个角落即背景区域。关于第三个随机 选取的特征图（对应第375个卷积核），对上图其响应位于头部区域，对下图则 响应在躯干部位。更有甚者，同一卷积核（第284个卷积核）对下图响应在躯 干,而对上图却毫无响应。这也就证实了：对于某个模式,如鸟的躯干,会有 不同卷积核（其实就是神经元）产生响应；同时对于某个卷积核（神经元）,会 在不同模式上产生响应,如躯干和头部。另外,需指出的是,除了分布式表示 特性，还可从图中发现神经网络响应的区域多呈现“稀疏”（sparse）特性，即

The 108-th channel The 468-th channel The 375-th channel

The 163-th channel

图3+3:卷积神经网络的分布式表示特性。

3.1.3深度特征的层次性

上节我们讨论了在同一层的神经元的特性，本节介绍不同层神经元的表示特点, 即深度特征的层次性：之前提到，卷积操作可获取图像区域不同类型特征，而 汇合等操作可对这些特征进行融合和抽象，随着若干卷积、汇合等操作的堆叠, 各层得到的深度特征逐渐从泛化特征（如边缘、纹理等）过度到高层语义表示 （躯干、头部等模式）。

2014年，Zeiler和Fergus ［96］曾利用反卷积技术［97］对卷积神经网络（［97］ 中以Alex-Net ［52］为例）特征进行可视化，洞察了卷积网络的诸多特性，其中 之一即层次性。如图3+4,可以发现，浅层卷积核学到的是基本模式，如第一层 中的边缘、方向和第二层的纹理等特征表示。随着网络的加深，较深层例如从第 三层除了一些泛化模式外，也开始出现了一些高层语义模式，如“车轮” “文字” 和“人脸”形状的模式。直到第五层，更具有分辨能力的模式被卷积网络所捕 获……以上的这些观察就是深度网络中特征的层次性。值得一提的是，目前深 度特征的层次性已成为深度学习领域的一个共识，也正是由于Zeiler和Fergus 的贡献，该工作［97］被授予欧洲计算机视觉大会ECCV 20142最佳论文提名奖,

:计算机视觉领域有三大顶级国际会议：“国际计算机视觉和模式识别大会"（Computer Vision

短短几年间引用已逾1700次。另外，得益于卷积网络特征的层次特性使得不 同层特征可信息互补，因此对单个网络模型而言“多层特征融合”(multi-layer ensemble)往往是一种很直接且有效的网络集成技术，对于提高网络精度通常 有较好表现，详细内容可参见本书第13.2.1节。

3.2经典网络案例分析
本节将以 Alex-Net [52], VGG-Nets [74], Network-In-Network [67]和深度残差 网络[36] (residual network)为例，分析几类经典的卷积神经网络案例。在此 需请读者注意,此处的分析比较并不是不同网络模型精度的“较量”,而是希望 读者体会卷积神经网络自始至今的发展脉络和趋势,这样会更有利于对卷积神

经网络的理解,进而方可举一反三提高解决真实问题的能力。

3.2.1 Alex-Net 网络模型

Alex-Net [52]是计算机视觉中首个被广泛关注、使用的卷积神经网络，特别是 Alex-Net在2012年ImageNet竞赛[73]中以超越第二名10+9个百分点的优异 成绩一举夺冠,从而打响了卷积神经网络、乃至深度学习在计算机视觉领域中 研究热潮的“第一枪”。

Alex-Net 由加拿大多伦多大学的 Alex Krizhevsky, Ilya Sutskever (G. E. Hinton的两位博士生)和Geoffrey E. Hinton提出，网络名“Alex-Net”即取自 第一作者。关于Alex-Net还有一则八卦：由于Alex-Net划时代的意义，开启了 深度学习在工业界的应用，2015年Alex和Ilya两位作者连同“半个” Hinton 被Google重金(据传高达3500万美金)收买。但为何说“半个” Hinton?只

and Patt

ion,简称 CVPR)、“国际计算机视觉大会” (International Conference on 衣 ICCV)和“欧洲计算机视觉大会”(European Conference on Computer 。其中CVPR每年一届，ICCV和ECCV两年一届交替举办。另外， S 顶级期干1J 包括：IEEE Transactions on Pattern Analysis and Machine .International Journal of Computer Vision (IJCV), IEEE Transactions TIP)等。

夹|

Layer 5

Layer 1

I', 'o -

Layer 2

Layer 3

h网络深度特征的层次特性［96］。该图中，由于第一层卷积核相 一层学到的卷积核直接可视化。而深层的卷积核往往很小，直

接可视化效果不佳，因此对第2至5层的可视化作以下处理：在验证集图像中, 将响应最大的前9个卷积核利用反卷积技术将其投影到像素空间，以此完成后 面深层卷积层参数的可视化。

因当时Hinton只是花费一半时间在Google工作，而另一半时间仍然留在多伦 多大学。

图3+5是Alex-Net的网络结构，共含五层卷积层和三层全连接层。Alex-Net 的上下两支是为方便同时使用两片GPU并行训练，不过在第三层卷积和全连 接层处上下两支信息可交互。由于两支网络完全一致,在此仅对其中一支进 行分析。表3.1列出了 Alex-Net网络的架构及具体参数。对比第1.1节提到的

不过仍需指出Alex-Net的几点重大贡献，正因如此，Alex-Net方可在整个 卷积神经网络甚至连接主义机器学习发展进程中占据里程碑式的地位。

1. Alex-Nee首次将卷积神经网络应用于计算机视觉领域的海量图像数据集 ImageNee [733 （该数据集共计1000类图像，图像总数约12S多万张），揭 示了卷积神经网络拥有强大的学习能力和表示能力。另一方面，海量数据 同时也使卷积神经网络免于过拟合。可以说二者相辅相成，缺一不可。自 此便引发了深度学习，特别是卷积神经网络在计算机视觉中“井喷”式的 研究。

2. 利用GPU实现网络训练。在上一轮神经网络研究热潮中，由于计算资源 发展受限，研究者无法借助更加高效的计算手段（如GPU），这也较大 程度阻碍了当时神经网络的研究进程。“工欲善其事，必先利其器”在 Alex-Net中，研究者借助GPU从而将原本需数周甚至数月的网络训练过

程大大缩短至五到六天。在揭示卷积神经网络强大能力的同时,无疑也大

大缩短了深度网络和大型网络模型开发研究的周期与时间成本。缩短了迭

代周期,数量繁多、立意新颖的网络模型和应用才能像雨后春笋一般层出

不穷。

3. —些训练技巧的引人使“不可为”变“可为”，甚至是“大有可为”。如 ReLU激活函数、局部响应规范化操作、为防止过拟合而采取的数据增广 (data augmentation)和随机失活(dropout)等，这些训练技巧不仅保证 了模型性能,更重要的是为后续深度卷积神经网络的构建提供了范本。实 际上,此后的卷积神经网络大体都是遵循这一网络构建的基本思路。

有关Alex-Net中涉及的训练技巧，第二篇对应章节会有系统性介绍。在此仅对 局部响应规范化做一解释。

局部响

djacent depth)

的卷积结果做规范化。假设adj为第d个通道的卷积核在(i,j)位置处的输出 结果(即响应)，随后经过ReLU激活函数作用。其局部响应规范化的结果bdj 可表示为：

(N-1,d+n/2)

E (aj

(3+1)

nax(0,d—n/2)

其中，n指定了作用LRN的相邻深度卷积核数目，N为该层所有卷积核数目。 k, n, a, (3等为超参数，需通过验证集进行选择，在原始Alex-Net中这些参数 的具体赋值为表3+1所示。使用LRN后，在ImageNet数据集上Alex-Net性能 分别在top-1和top-5错误率上降低了 1.4%和1.2%;此外，一个四层的卷积 神经网络使用LRN后，在CIFAR-10数据上的错误率也从13%降至11% [52]。

LRN目前已经作为各个深度学习工具箱的标准配置，将k, n, a, 3等超 参数稍作改变即可实现其他经典规范化操作。如，当“k = 0, n = N, a =1, 3 = 0.5”时便是经典的12规范化：

bi,j=ai,j/

/EH/.

(3+2)

3.2.2 VGG-Nets 网络模型

VGG-Nets ［74］由英国牛津大学著名研究组VGG (Visual Geome 提出，是2014年ImageNet竞赛定位任务(localization task)第-任务第二名做法中的基础网络。由于VGG-Nets具备良好的泛化， ImageNet数据集上的预训练模型(pre-trained model)被广泛应用' 的特征抽取(feature extractor) ［7,20］外的诸多问题：如物体候选 proposal)生成［26］、细粒度图像定位与检索(fine-grained object and image retrieval) ［84］、图像协同定位(co-localization) ［85］等。

以VGG-Nets中的代表VGG-16为例，表3+2列出了其每层具体 可以发现，相比Alex-Net, VGG-Nets中普遍使用了小卷积核以及第 到的“保持输入大小”等技巧，为的是在增加网络深度(即网络复杂 各层输入大小随深度增加而不极具减小。同时，网络卷积层的通道数 也是从3 4 64 4 128 4 256 4 512逐渐増加。

3.2.3 Networl

Network-In-Network

统卷积神经网络的一类

多层感知机(多层全连

的线性卷积层。我们知

射也只能将上层特征

了复杂度

一种新可

多复杂性

Inceptic

是由新加坡国立大学LV实验室提出的异于传 模型，它与其他卷积神经网络的最大差异是用 线性函数的组合)替代了先前卷积网络中简单 积层的复杂度有限，利用线性卷积进行层间映 f单”的线性组合形成下层特征。而NIN采用 层间映射形式，一方面提供了网络层间映射的 络卷积层的非线性能力，使得上层特征可有更 ,这样的想法也被后期出现的残差网络［36］和

模型的另一个重大突破是摒弃了全连接层作为分类层的传 ［合操作(global average pooling),如图 3.7。NIN 最后一 feature map)对应分类任务的C个类别。全局汇合操作分

，最后以汇合结果映射到样本真实标记。可以发现，在这

图3+6:传统卷积模块（a）与NIN网络卷积模块（b）对比［67］y

样的标记映射关系下，C张特征图上的响应将很自然的分别对应到C个不同的 样本类别，这也是相对先前卷积网络来讲，NIN在模型可解释性上的一个优势。

图3+7: NIN网络模型整体结构［67］。此例中的NIN堆叠了三个多层感知机卷 积层模块和一个全局汇合操作层作为分类层。

3.2.4残差网络模型

理论和实验已经表明，神经网络的深度WdepUhn和宽度（width）是表征网络 复杂度的两个核心因素，不过深度相比宽度在増加网络的复杂性方面更加有效， 这也正是为何VGG网络想方设法增加网络深度的一个原因。然而，随着深度 的增加，训练会变得愈加困难。这主要因为在基于随机梯度下降的网络训练过 程中，误差信号的多层反向传播非常容易引发梯度“弥散”（梯度过小会使回传 的训练误差极其微弱）或者“爆炸”（梯度过大会导致模型训练出现“NaN”） 现象。目前，一些特殊的权重初始化策略（参见第7章），以及批规范化（batch normalizationf策略［46］等方法使这个问题得到极大的改善——网络可以正常

训练了！但是，实际情形仍不容乐观。当深度网络收敛时，另外的问题又随之而

来：随着继续增加网络的深度，训练数据的训练误差没有降低反而升高[36,79], 这种现象如图3.8所示。这一观察与直觉极其不符，因为如果一个浅层神经网 络可以被训练优化求解到某一个很好的解，那么它对应的深层网络至少也可以 而不是更差。这一现象在一段时间内困扰着更深层卷积神经网络的设计、训练 和应用。

不过很快，该方面便涌现出一个优秀的网络模型，这便是著名的残差网络 (residual network) [36]。由于残差网络很好的解决了网络深度带来的训练困难， 它的网络性能(完成任务的准确度和精度)远超传统网络模型，曾在ILSVRC 20153和COCO 20154竞赛的检测、定位和分割任务中纷纷斩获第一，同时发表 残差网络的论文也获得了计算机视觉与模式识别领域国际顶级会议CVPR 2016 的最佳论文奖。残差网络模型的出现不仅备受学界业界瞩目，同时也拓宽了卷 积神经网络研究的道路。介绍残差网络前，不得不提到另一个该方面的代表模 型 高速公路网络(highway network)。

20

柵彤眾蓐

0123456 0123456

迭代輸数x104 (iteration) 选代辕数x104 (iteration)

图3+8: 20层和56层的常规网络在CIFAR-10数据集上的训练错误率(左图) 和测试错误率(右图)[36]。

'http:/ / image-net .org/chsllenges/LSVRC/2015/

:http://mscoco.Org/dataset/#detections-challenge2015

高速公路网络

为克服深度増加带来的训练困难，Srivastava等［12］受长 short term memory network) ［41］中门(gate)机制［25

的前馈神经网络修正以至于信息能够在多个神经网络层之间高效流动，也正因 如此，这种修改后网络称为“高速公路网络”(Highway network)。

假设某常规卷积神经网络有L层，其中第i层(i G 1, 2, + + + ,L)的输人为 xS参数为W，该层的输出yf = xi+1。为了表述的简单，我们忽略层数和偏 置，它们之间的关系可表示如下：

(3+3)

其中，F为非线性激活函数，参数Wf的下标表明该操作对应于F。对于高速 公路网络而言，y的计算定义如下：

,Wt) + x • C(x, Wc) +

(3+4)

同式3.3中类似，T(x,Wt)和C(x,Wc)是两个非线性变换，分别称作“变换门” 和“携带门”。变换门负责控制变换的强度，携带门则负责控制原输人信号的保 留强度。换句话说，y是F(x,Wf)和x的加权组合，其中T和C分别控制了 两项对应的权重。为了简化模型，在高速公路网络中，设置C = 1-T,因此公 式3.4可表示为：

+ X • (1 - T(x, Wt)) +

(3+5)

由于增加了恢复原始输人的可能，这种改进后的网络层(式3.5)要比常规网络

层(式3+3)更加灵活。特别地，对于特定的变换门，我们可以得到不同的输出:

if T (x, wt) = 0

其实不难发现，当变换门为恒等映射13时，高速公路网络则退化为常规网络 深度残差网络

言归正传，现在请出本节主角-残差网络(residual network)。其实，He等

人［36］提出的深度残差网络与高速公路网络的出发点极其相似，甚至残差网络 可以看作是高速公路网络的一种特殊情况。在高速公路网络中的携带门和变换 门都为恒等映射时，公式3.4可表示为：

y = F(x, w) + x . (3.7)

对式3+7做简单的变形，可得

(3+8)

也就是说，网络需要学习的函数F实际上是式3.7右端的残差项y- x,称为 “残差函数”。如图3.9所示，残差学习模块有两个分支，其一是左侧的残差函数, 其二是右侧的对输人的恒等映射。这两个分支经过一个简单整合(对应元素的 相加)后，再经过一个非线性的变换ReLU激活函数，从而形成整个残差学习 模块。由多个残差模块堆叠而成的网络结构称作“残差网络”。

x

identity

图3.9:残差学习模块［36］。

图3.10展示了两种不同形式的残差模块。左图为刚才提到的常规残差模块, 由两个3x3卷积堆叠而成，但是随着网络深度的进一步增加，这种残差函数 在实践中并不是十分有效。右图所示为“瓶颈残差模块” (bottleneck residual block),依次由3个1 x 1, 3 x 3和1 x 1的卷积层构成，这里1 x 1卷积能够 起降维或者升维的作用，从而令3 x 3的卷积可以在相对较低维度的输人上进 行，以达到提高计算效率的目的。在非常深的网络中，“瓶颈残差模块”可大量 减少计算代价。

图3+10:两种不同的残差学习模块［36］。左图为常规的残差模块，右图为“瓶 颈”残差模块。

和“高速公路”网络相比，残差网络与其的不同点在于残差模块中的近路连 接(short cut)可直接通过简单的恒等映射完成，而不需要复杂的携带门和变 换门去实现。因此，在残差函数输人输出维度一致的情况下，残差网络不需要 引人额外的参数和计算的负担。与高速公路网络相同的是，通过这种近路连接 的方式，即使面对特别深层的网络，也可以通过反向传播端到端的学习，使用 简单的随机梯度下降的方法就可以训练。这主要受益于近路连接使梯度信息可 以在多个神经网络层之间有效传播。

此外，将残差网络与传统的VGG网络模型对比(图3.11)可以发现，若 无近路连接，残差网络实际上就是更深的VGG网络，只不过残差网络以全 局平均汇合层(global average pooling layer)替代了 VGG网络结构中的全连 接层，一方面使得参数大大减少，另一方面减少了过拟合风险。同时需指出, 这样“利用全局平均汇合操作替代全连接层”的设计理念早在2015年提出的 GoogLeNet。80］中就已经被使用。

3.3.小结

3.3小结
§ 本章介绍了深度卷积神经网络中三个重要概念：神经元感受野、特征分布 式表示和与网络深度相关的特征层次性；

§以Alex-Net、VGG-Nets、NIN和残差神经网络四种经典卷积神经网络为 例，介绍了深度学习中卷积神经网络结构自2012年至今的发展变化。同 时需要指出，上述模型包括其他目前应用较多的卷积网络模型的结构仍需 依赖人工设计，到底何种架构才是最优模型结构尚不可知，不过已有一些 研究开始着力于自动化的深度网络结构学习。相信不久的将来，机器自己 设计的深层网络终将打败人类精心设计的网络结构。

表3.1: Alex-Net网络架构及参数。其中，f”为卷积 步长，“d”为该层卷积核个数（通道数），“p”为填充参 失活率，“C”为分类任务类别数（如在ImageNet数据 a, f3” 为局部响应规范化（Local Response Normalizat

LRN）操作的

不同而略有差

参数。（注：各层输出数据维度可能因所使用深度学习开

异。）

操作类型

参数信息

输人数据维度

输出数据维度

1

卷积操作

f = 11; s = 4; d = 96

227 x 227 x 3

55 x 55 x 96

2

ReLU

/

55 x 55 x 96

55 x 55 x 96

3

最大值汇合操作

f = 3; s = 2

55 x 55 x 96

27 x 27 x 96

4

LRN规范化

k = 2; n = 5; a = 10-4; ^ = 0.75

27 x 27 x 96

27 x 27 x 96

5

卷积操作

f = 5; p = 2; s = 1; d = 256

27 x 27 x 96

27 x 27 x 256

6

ReLU

-f

27 x 27 x 256

27 x 27 x 256

7

最大值汇合操作

f = 3; s = 2

27 x 27 x 256

13 x 13 x 256

8

LRN规范化

k = 2; n = 5; a = 10-4; ^ = 0.75

13 x 13 x 256

13 x 13 x 256

9

卷积操作

f = 3; p = 1; s = 1; d = 384

13 x 13 x 256

13 x 13 x 384

10

ReLU

-

13 x 13 x 384

13 x 13 x 384

11

卷积操作

f = 3; p = 1; s = 1; d = 384

13 x 13 x 384

13 x 13 x 384

12

ReLU

13 x 13 x 384

13 x 13 x 384

13

卷积操作

f = 3; p = 1; s = 1; d = 256

13 x 13 x 384

13 x 13 x 256

14

ReLU

-

13 x 13 x 256

13 x 13 x 256

15

最大值汇合操作

f = 3; s = 2

13 x 13 x 256

6 x 6 x 256

16

全连接层

f = 6; s = 1; d = 4096

6 x 6 x 256

1 x 1 x 4096

17

ReLU

-

1 x 1 x 4096

1 x 1 x 4096

18

随机失活

J = 0.5

1 x 1 x 4096

1 x 1 x 4096

19

全连接层

f = 1; s = 1; d = 4096

1 x 1 x 4096

1 x 1 x 4096

20

ReLU

-

1 x 1 x 4096

1 x 1 x 4096

21

随机失活

J = 0.5

1 x 1 x 4096

1 x 1 x 4096

22

全连接层

f = 1; s = 1; d = 4096

1 x 1 x 4096

1 x 1 x C

23

损失函数层

Softmax loss

1 x 1 x C

-

3.3.小结

表3+2: VGG-16网络架构及参数。其中，f”为卷积核/汇合核大小 步长，“d”为该层卷积核个数（通道数），“p”为填充参数，“C”为分 别数（在ImageNet数据集上为1000）。（注：各层输出数据维度可能 深度学习开发工具的不同而略有差异。）

操作类型

参数信息

输人数据维度

输出数据维度

1

卷积操作

f = 3; p = 1; s = 1; d = 64

224 x 224 x 3

224 x 224 x 64

2

ReLU

-

224 x 224 x 64

224 x 224 x 64

3

卷积操作

f = 3; p = 1; s = 1; d = 64

224 x 224 x 64

224 x 224 x 64

4

ReLU

-

224 x 224 x 64

224 x 224 x 64

5

最大值汇合操作

f = 2; s = 2

224 x 224 x 64

112 x 112 x 64

6

卷积操作

f = 3; p = 1; s = 1; d = 128

112 x 112 x 64

112 x 112 x 128

7

ReLU

- /

112 x 112 x 128

112 x 112 x 128

8

卷积操作

f = 3; p = 1; s = 1; d = 128

112 x 112 x 128

112 x 112 x 128

9

ReLU

-

112 x 112 x 128

112 x 112 x 128

10

最大值汇合操作

f = 2; s = 2

112 x 112 x 128

56 x 56 x 128

11

卷积操作

f = 3; p = 1; s = 1; d = 256

56 x 56 x 128

56 x 56 x 256

12

ReLU

-c

56 x 56 x 256

56 x 56 x 256

13

卷积操作

f = 3; p = 1; s = 1; d = 256

56 x 56 x 256

56 x 56 x 256

14

ReLU

-

56 x 56 x 256

56 x 56 x 256

15

卷积操作

f = 3; p = 1; s = 1; d = 256

56 x 56 x 256

56 x 56 x 256

16

ReLU

-

56 x 56 x 256

56 x 56 x 256

17

最大值汇合操作

f = 2; s = 2

56 x 56 x 256

28 x 28 x 256

18

卷积操作

f = 3; p = 1; s = 1; d = 512

28 x 28 x 256

28 x 28 x 512

19

ReLU

-

28 x 28 x 512

28 x 28 x 512

20

卷积操作

f = 3; p = 1; s = 1; d = 512

28 x 28 x 512

28 x 28 x 512

21

ReLU

-

28 x 28 x 512

28 x 28 x 512

22

卷积操作

f = 3; p = 1; s = 1; d = 512

28 x 28 x 512

28 x 28 x 512

23

ReLU

-

28 x 28 x 512

28 x 28 x 512

24

最大值汇合操作

f = 2; s = 2

28 x 28 x 512

14 x 14 x 512

操作类型

参数信息

输人数据维度

输出数据维度

25

卷积操作

f = 3; p = 1; s = 1; d = 512

14 x 14 x 512

14 x 14 x 512

26

ReLU

14 x 14 x 512

14 x 14 x 512

27

卷积操作

f = 3; p = 1; s = 1; d = 512

14 x 14 x 512

14 x 14 x 512

28

ReLU

14 x 14 x 512

14 x 14 x 512

29

卷积操作

f = 3; p = 1; s = 1; d = 512

14 x 14 x 512

14 x 14 x 512

30

ReLU

A -

14 x 14 x 512

14 x 14 x 512

31

最大值汇合操作

f = 7; s = 1

14 x 14 x 512

7 x 7 x 512

32

全连接层

f = 7; s = 1; d = 4096

14 x 14 x 512

1 x 1 x 4096

33

ReLU

-

1 x 1 x 4096

1 x 1 x 4096

34

随机失活

J = 0.5

1 x 1 x 4096

1 x 1 x 4096

35

全连接层

f = 1; s = 1; d = 4096

1 x 1 x 4096

1 x 1 x 4096

36

ReLU

-

1 x 1 x 4096

1 x 1 x 4096

37

随机失活

J = 0.5

1 x 1 x 4096

1 x 1 x 4096

38

全连接层

f = 1; s = 1; d = 4096

1 x 1 x 4096

1 x 1 x C

39

损失函数层

Softmax loss

1 x 1 x C

-

3.3.小结

34-layer plain

34-layer residual

inye

3x3 conv, 64

pool, /2

★

3x3 conv, 128

3x3 conv, 128

7x7 conv, 6

pool, /2

★

♦

pool, /2

★

3x3 conv, 256

3x3 conv

3x3 conv, 256

3x3 conv

3x3 conv, 256

3x3 conv

3x3 conv, 256

3x3 conv

3x3 conv

3x3 conv

pool, /2

★

3x3 conv, 512

3x3 conv

3x3 conv

nv, 256, /2

conv, 256

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

V

avg pool

★

conv, 512

图3.11: VGG网络模型(VGG-19)、34层的普通网络模型(34-layer plain)与 34层的残差网络模型(34-layer residual)对比［36］。















* * *





# COMMENT



