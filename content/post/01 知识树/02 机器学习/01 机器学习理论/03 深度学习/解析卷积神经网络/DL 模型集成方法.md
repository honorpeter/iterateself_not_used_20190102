---
title: DL 模型集成方法
toc: true
date: 2018-06-26 19:33:29
---
## 相关资料
1. 《解析卷积神经网络》魏秀参



## 需要补充的






  * aaa



* * *




# INTRODUCTION






  * aaa



















模型集成方法

集成学习(ensemble L

器并将它们组合起来使

更好的预测结果，颇有

竞赛，如 ImageNet1、

复杂其最后一步必然是

但集成学习的使用仍然

















































学习算法，指训练多个学习 实践中能取得比单个学习器


















特别是历届国际重量级学术

竞赛的冠军做法，或简单或

型已经拥有强大的预测能力，

。因此有必要了解并掌握一

的集成多从“数据层面”和


充”曾提到了训练阶段的若干数据扩充策略，实际上，这些

C阶段同样适用，诸如图像多尺度(multi-scale)、随机扣取


net.org/ g/kdd-cup


(random crop)等。以随机扣取为例，对某张测试图像可得到n张随机扣取图 像，测试阶段只需用训练好的深度网络模型对n张图分别做预测，之后将预测 的各类置信度平均作为该测试图像最终预测结果即可。

13.1.2    “简易集成”法

“简易集成”法(easy ensemble) ［59］是Liu等人提出的针对不平衡样本问题的 一种集成学习解决方案。具体来说, “简易集成”法对于样本较多的类采取降 采样(undersampling),每次采样数依照样本数目最少的类别而定，这样每类 取到的样本数可保持均等。采样结束后，针对每次采样得到的子数据集训练模 型,如此采样、训练反复进行多次。最后对测试数据的预测则依据训练得到若 干个模型的结果取平均或投票获得(有关“多模型集成方法”内容请参见本章 第13.2.2节)。总结来说，“简易集成”法在模型集成的同时，还能缓解数据不平 衡带来的问题,可谓一举两得。

13.2
13.2.

多层特征



















?征融合(multi-layer ensemble)是针对单模型的一种模型层面集成方法。 良度卷积神经网络特征具有层次性的特点(参见本书第3.1.3节内容。，不 f征富含的语义信息可以相互补充，在图像语义分割［31］、细粒度图像检 ］、基于视频的表象性格分析［98］等任务中常见多层特征融合策略的使用。 1,多层特征融合操作时可直接将不同层网络特征级联(concatenate)。而 征融合应选取哪些网络层,一个实践经验是：最好使用靠近目标函数的 积特征,因为愈深层特征包含的高层语义性愈强、分辨能力也愈强；相 络较浅层的特征较普适,用于特征融合很可能起不到作用有时甚至会起 作用。


13.2.模型层面的集成方法

网络“快照”集成法 我们知道深度神经网络模型复杂的解空间中存在非常多的局部最优解,但经典 批处理随机梯度下降法(mini-batch SGD)只能让网络模型收敛到其中一个局 部最优解。网络“快照”集成法(snapshot ensemble) ［43］便利用了网络解空 间中的这些局部最优解来对单个网络做模型集成。通过循环调整网络学习率 (cyclic learning rate schedule)可使网络依次收敛到不同的局部最优解处，如图 13.1中左图所示。

40


30


"40

'30


“快照”災edc法

識

40

30




图13.1:网络“快照”集成法［43］。左图为“传统SGD法”和‘快照’集成 法”的收敛情况示意图；右图为两方法在CIFAR-10数据集上的收敛曲线对比 (红色曲线为“‘快速’集成法”，蓝色曲线对应“传统SGD法”)。

具体而言，是将网络学习率n设置为随模型迭代轮数t (iteration，即一次 批处理随机梯度下降称为一个迭代轮数)改变的函数，即：

n⑴ ^tHfif^^+iy    (叫

其中，no为初始学习率，一般设为0+1或0.2。t为模型迭代轮数(即mini-batch 批处理训练次数)。2°为模型总的批处理训练次数。M为学习率“循环退 火”(cyclic annealing) 25次数，对应了模型将收敛到的局部最优解个数。式 13+1利用余弦函数cos(+)的循环性来循环更新网络学习率，将学习率从0+1随t 的增长逐渐减缓到0,之后将学习率重新放大从而跳出该局部最优解，自此开

始下一循环的训练，此循环结束后可收敛到新的局部最优解处，如此循环往复

……直到M个循环结束。因式13.1中利用余弦函数循环更新网络参数，这一 过程被称为“循环余弦退火”过程(cyclic cosine annealing) ［61］。

当经过“循环余弦退火”对学习率的调整后，每个循环结束可使模型收敛到

一个不同的局部最优解，若将收敛到不同局部最优解的模型保存便可得到 M

个处于不同收敛状态的模型，如图13.1右图中红色曲线所示。对于每个循环结 束后保存的模型，我们称之为模型“快照” (snapshot)。测试阶段在做模型集成

时，由于深度网络模型在初始训练阶段未必拥有较优越性能，因此一般挑选最

后 m 个模型“快照”用于集成。关于这些模型“快照”的集成策略可采用本章 后面提到的“直接平均法”。

13.2.2多模型集成

上一节我们介绍了基于单个网络如何进行模型集成，本节向大家介绍如何产生

多个不同网络训练结果并讲解一些多模型的集成方法。

多模型生成策略

-同一模型不同初始化：我们知道，由于神经网络训练机制基于随机梯度下 降，故不同的网络模型参数初始化会导致不同的网络训练结果。在实际使 用中，特别是针对小样本(limited examples)学习的场景，首先对同一模 型进行不同初始化，之后将得到的网络模型进行结果集成会大幅缓解其随

机性，提升最终任务的预测结果。

-同一模型不同训练轮数：若网络超参数设置得当，深度模型随着网络训练 的进行会逐步趋于收敛，但不同训练轮数的结果仍有不同，无法确定到底 哪一轮训练得到的模型最适用于测试数据。针对上述问题，一种简单的 解决方式是将最后几轮训练模型结果做集成，这样一方面可降低随机误

差，另一方面也避免了训练轮数过多带来的过拟合风险。这样的操作被称

为“轮数集成”(epoch fusion或epoch ensemble)。具体使用实例可参考 ECCV2016举办的“基于视频的表象性格分析”竞赛冠军做法［98］。

13.2.模型层面的集成方法

•不同目标函数：目标函数(或称损失函数)是整个网络训练的“指挥棒” 选择不同目标函数势必使网络学到不同的特征表示。以分类任务为例,可 将“交叉熵损失函数”、 “合页损失函数”、 “大间隔交叉熵损失函数”和 “中心损失函数”作为目标函数分别训练模型。在预测阶段,既可以直接 对不同模型预测结果做“置信度级别” (score level)的平均或投票，也可 以做“特征级别” (feature level)的模型集成：将不同网络得到的深度特 征抽出后级联作为最终特征,之后离线训练浅层分类器(如支持向量机) 完成预测任务。

-不同网络结构也是一种有效的产生不同网络模型结果的方式。操作时可在 如VGG网络、深度残差网络等不同网络架构的网络上训练模型，最后将 不同架构网络得到的结果做以集成。

多模型集成方法

根据上一节提到的多

练结果,除特征级别直

级别对得到的若干网

假设共有 N 个模型待

(C 为数据的标记空间

•直接平均法(sir 接平均不同模型




















加权

节不






略或网络“快照”集成法均可得到若干网络训 练离线浅层学习器外,还可以在网络预测结果 成。下面介绍四种最常用的多模型集成方法。 某测试样本x,其预测结果为N个C维向量

s2,...,sN 。

；ing)是最简单有效的多模型集成方法，通过直

置信度得到最后预测结果：

Ns

inal score =    .    (13.2)

waging)是在直接平均法基础上加人权重来调 度：


ial score


N

i=l ⑴isi

N


(13.3)


的权重,且须满足：

N

(13+4)


> 0 and ^2 ⑴i 二 1

i=1

实际使用时关于权重叫的取值可根据不同模型在验证集上各自单独的准

确率而定，高准确率的模型权重较高，低准确率模型可设置稍小权重。

投票法(voting)中最常 先将各自模型返回的预测 的类别标记ci €{1,2,.+ 到样本 x 的最终预测时 本预测结果为该类别；若 作出预测(称为“reject:
























高置信度对应

表决法中在得

投票，则该样

投票，则拒绝


投票法中另一种常用方法是相对多数表决法(plurality voting),与多数表 决法会输出“拒绝预测”不同的是，相对多数表决法一定会返回某个类别 作为预测结果，因为相对多数表决是选取投票数最高的类别作为最后预测 结果。

•堆叠法(Stacking)又称“二次集成法”，是一种高阶的集成学习算法。在 刚才的例子中，样本X作为学习算法或网络模型的输人，Si作为第i个 模型的类别置信度输出，整个学习过程可记作一阶学习过程(first-level learning) „堆叠法则是以一阶学习过程的输出作为输人开展二阶学习过 程(second-level learning),有时也称作“元学习”(meta learning)。拿 刚才的例子来说，对于样本x,堆叠法的输人是N个模型的预测置信 度[S1S2 + + +SN],这些置信度可以级联作为新的特征表示。之后基于这样 的“特征表示”训练学习器将其映射到样本原本的标记空间。注意，此时 的学习器可为任何学习算法习得的模型，如支持向量机(support vector machine)、随机森林(random forest),当然也可以是神经网络模型。不 过在此需要指出的是，堆叠法有较大过拟合风险。



深度网络的模型集成往往是提升网络最终预测能力的一剂“强心针”，本

章从“数据层面”和“模型层面”两个方面介绍了一些深度网络模型集成

13.3.小结

的方法；

§ 数据层面常用的方法是数据扩充和“简易集成”法,均操作简单但效果显

著；

§ 模型层面的模型集成方法可分为“单模型集成”和“多模型集成”。基于 单一模型的集成方法可借助单个模型的多层特征融合和网络“快照”法进 行。多模型集成方面,可通过不同参数初始化、不同训练轮数和不同目标 函数的设定产生多个网络模型的训练结果。最后使用平均法、投票法和堆 叠法进行结果集成。

§需指出的是，本书第10章提到的随机失活（dropout）实际上也是一种隐 式的模型集成方法。有关随机失活的具体内容请参考本书第10.4节“随机 失活”；

§更多关于集成学习的理论和算法请参考南京大学周志华教授的著作26 “Ensemble Methods: Foundations and Algorithms” [100]。

















* * *




# COMMENT
