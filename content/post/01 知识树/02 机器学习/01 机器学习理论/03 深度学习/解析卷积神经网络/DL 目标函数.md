---
title: DL 目标函数
toc: true
date: 2018-06-26 19:32:03
---



## 相关资料
1. 《解析卷积神经网络》魏秀参




## 需要补充的






  * aaa



* * *




# INTRODUCTION






  * aaa













目标函数

深度网络

通过样本

学习。本

务中的一

其搭配使

解)，正

关正则项
































误差反向传播指导网络参数学习与表示 和回归(regression)这两类经典预测任 问题需求选择使用合适的目标函数或将

或达到其他训练目标(如希望得到稀疏

加人目标函数中一起指导模型训练。有

“网络正则化”。


的目标函数




类任务共N个训练样本，针对网络最后分类层第i个样本的输人特征 霉对应的真实标记为yi € {1，2，+ + +，C},另h = (hi, h2，+ + +，hc)T为网 &输出，即样本i的预测结果，其中C为分类任务类别数。

i数，亦称“损失函数”(loss function)或“代价函数” (cost function)。

9.1.分类任务的目标函数

9.1.1交叉熵损失函数

交叉摘(cross entropy)损失函数又称Softmax损失函数，是目前卷积神经网 络中最常用的分类目标函数。其形式为：

Lcross entropy loss = Lsoftmax loss

即通过指数化变换使网络输出 h 转换为概率

9.1.2合页损失函数

在支持向量机中被广泛使用的合页损失函数(hinge loss)有时也会作为目标函 数在神经网络模型中使用：

hinge loss —


i=1

需指出的是，一般情况的分类任务中交叉熵损失函数的分类效果略优于合页损

失函数的分类效果。

9.1.3坡道损失函数

对支持向量机有一定了解的读者应该知道合页损失函数的设计理念，即“对错

误越大的样本施加越严重的惩罚”。可是这一损失函数对噪声的抵抗能力较差。

试想，若某样本标记本身错误或该样本本身是离群点(outlier),则由于错分导 致该样本分类误差会变得很大，如此便会影响整个分类超平面的学习，从而降 低模型泛化能力。非凸损失函数的引人则很好地解决了这个问题。

其中，坡道损失函数(ramp loss function)和Tukey’s biweight损失函数分 别是分类任务和回归任务中非凸损失函数的代表。由于它们针对噪声数据和 离群点具备良好的抗噪特性，因此也常被称为“鲁棒损失函数” (robust loss functions)。这类损失函数的共同特点是在分类(回归)误差较大区域进行了 截断”，使得较大的误差不再大程度影响整个误差函数。但是，这类函数因其 非凸(non-convex)的性质使得在传统机器学习优化中过于繁杂，甚至有时根 本无法进行。不过，“这点”非凸性质放在神经网络模型优化中实属“小巫见大

巫”——整个网络模型本身就是个巨大的非凸函数，得益于神经网络模型的训

练机制使得此类非凸优化不再成为难题。

坡道损失函数［10］ (ramp loss)的定义为：


i=


(9+3)

(9+4)


其中，s指定了 “截断点”的位置。由于坡道损失函数实则在s处“截断”的 合页损失函数，因此坡道损失函数也被称为“截断合页损失函数” (truncated hinge loss function)。图9+1展示了合页损失函数和坡道损失函数。很明显，坡 道损失函数是非凸的，图中其截断点在3 = -0.5处。不过细心的读者或许会提 出“坡道损失函数在x = 1和x = s两处是不可导的，这该如何进行误差的反 向传播啊？”。不要着急，其实真实情况下并不要求必须满足严格的数学上的连 续，因为计算机内部的浮点计算并不会得到完全导数落在“尖点”的非常情况， 最多只会落在“尖点”附近。若导数值在这两个“尖点”附近，仅需给出对应的 导数值即可，因此数学上的“尖点”不可导并不影响实际使用。对于“截断点” s的设置，根据文献［91］的理论推导，s取值最好根据分类任务的类别数C而

定，



图9+1:合页损失函数(蓝色虚线)与坡道损失函数(红色实线)。

以上提到的交叉熵损失函数、合页损失函数和坡道损失函数只是简单衡量了

9.1.分类任务的目标函数

模型预测值与样本真实标记之间的误差从而指导训练过程，它们并没有显式的

将特征判别性学习考虑到整个网络训练中。对此，为了进一步提高学习到的特

征表示的判别性，近来研究者们基于交叉熵损失函数设计了一些新型损失函数，

如大间隔交叉摘损失函数(large-margin softmax loss)、中心损失函数(center loss)。这些损失函数考虑了增大类间距离，减小类内差异等不同要素，进一步 提升了网络学习特征的判别能力。

9.1.4大间隔交叉熵损失函数

上面提到的网络输出结果h实际上是全连接层参数W与该层特征向量xi的 内积，即h = WTXi (为表达简洁式中未体现偏置项b)。因此传统的交叉摘损 失函数(Softmax损失函数)还可表示为：



其中，WT为W第i列参数值。同时，根据内积定义，式9.5可变换为：

Lsoftma:



| COS(&yi;)

HXi|| COS(0j )    +    ⑺上)

式中的0j (0 S f n)为向量WT和Xi的夹角。

以二分类为例，对隶属于第1个类别的某样本Xi而言，为分类正确传统交叉

熵损失函数需迫使学到的参数满足：WTxi > WTxi，亦即||Wi||||xi|| cos(0i) > IIWJIIIx』cos(02)。大间隔交叉熵损失函数(large-margin softmax loss function) [58]为使特征更具分辨能力则在此基础上要求二者差异需更大，即引人 m “拉大”二者差距，这便是“大间隔”名称的由来。||Wi||||xi|| cos(m0i) > ||W2||||xi|| cos(02) (0 < Oi <    )。式中m为正整数，起到控制间隔大小的作

用， m 越大，类间间隔越大，反之易然。特别地，当 m =1时，大间隔交叉熵 损失函数即退化为传统交叉熵损失函数。

Oi) > ||Wi||||xi||


cos(mOi) > ||W2||||xi|| cos(O2) +


(9+7)


可以发现，上式不仅满足传统交叉熵损失函数的约束，在确保分类正确的同时增 大了不同类别间分类的置信度，有助进一步提升特征分辨能力(discriminative ability )。

大间隔交叉熵损失函数［58］的定义为：

Llarge—margin softmax loss


(9+8)


比较可发现，上式与式9.6的区别仅在于将第i类分类间隔“拉大” 了：由 cos(‘)变为桃，)。其中:

梢=



(9.9)

式中dw只需满足“单调递减”条件，且D(m) = coS(m)。为简化网络前向 和反向运算，文献［58］推荐了一种具体的4(0)函数形式如下：

^k, 0 G


kn (k + 1)n

mm


(9.10)


式中k为整数，且满足kG ［0,m-1］。

图9.2的示意图直观的对比了二分类情形下Wi的模和W2的模在“等

于”、“大于”和“小于”三种不同关系下的决策边界［58］。可直观发现，大间隔 交叉熵损失函数扩大了类间距离，由于它不仅要求分类正确且要求分开的类需 保持较大间隔，使得训练目标相比传统交叉熵损失函数更困难。训练目标变困 难后带来的一个额外好处便是可以起到防止模型过拟合的作用。由是，在分类 性能方面，大间隔交叉熵函数要优于交叉熵损失函数和合页损失函数。

9.1.5中心损失函数

大间隔交叉熵损失函数主要考虑増大类间距离。而中心损失函数(center loss function) ［87］则在考虑类间距离的同时还将一些注意力放在减小类内差异上。 中心损失函数的定义为：

center loss ——


2lZllxi -cy』


yil 2 ,


(9+11)


i=1


9.1.分类任务的目标函数



图9+2:二分类情形下，Wi的模和W2的模不同关系时，传统交叉熵损失函数 （左图）和大间隔交叉熵损失函数（右图）决策边界对比［58］。

其中cyi为第yi类所有深度特征的均值（“中心”），故名“中心损失函数”。直 观上，式9.11迫使所有隶属于yi类的样本与之中心不要距离过远，否则将增大 惩罚。在实际使用时，由于中心损失函数本身考虑类内差异，所以中心损失函 数应与其他主要考虑类间距离的损失函数配合使用，如交叉熵损失函数，这样 网络最终目标函数形式可表示为：



loss 十 ALcenter loss (ohx,yain)


ehyi


iN-b


(9+12)


jC ehj


tji=o1n


十 2 二 llxi - cy』


(9.13)


i=1


『项，A越大则类内差异占整个目标函数较大比


图9+3展示了不同A取值对分类结果的影响［87］,其分类任务为“0”至“9

共10个手写字符识别任务2。图中不同颜色的“簇”表示了不同类别手写字符 (“0”至“9”)。可明显发现，在中心损失函数占比重较大时，簇更加集中，说明

类内差异明显减小。另外需要指出的是，类内差异减小的同时也使得特征具备

更强的判别能力(图9.3(a)-(d)簇的间隔越来越大，即类别区分性越来越大)。

在分类性能方面，中心损失函数搭配传统交叉熵损失函数要优于只使用交叉熵

损失函数作为目标函数的网络模型，特别是在人脸识别问题上可有较大性能提 升［87］。


图9.3:中心损失函数示意［87］。随A增大，中心损失函数在整个目标函数中占 比重增加、类内差异减小、特征分辨能力增强。

9.2回归任务的目标函数
在上一节的分类问题中，样本真实标记实际对应了一条独热向量(one hot vector):对样本i，该向量在％处为1表征该样本的真实隶属类别，而其余 C-1维均为0。而在本节讨论的回归任务中，样本真实标记同样对应一条向量， 但与分类任务真实标记的区别在于，回归任务真实标记的每一维为实数，而非 二值(0或1)。

数据集为MNIST，可访问如下链接下载数据：http://yann.lecun.com/exdb/mnist/

9.2.回归任务的目标函数

在介绍不同回归任务目标函数前，首先介绍一个回归问题的基本概念：残差

或称为预测误差，用于衡量模型预测值与真实标记的靠近程度。假设回归问题

中对应于第i个输人特征xi的真实标记为y^ = (yi,y2,.. + ,yM)T, M为标记 向量总维度，则it即表示样本i上网络回归预测值(yi与其真实标记在第t

维的预测误差(亦称残差)：

it = yt - yt +    (9.14)

9.2.1 li损失函数

常用的两种回归问题损失函数为li和12损失函数。对N个样本的li损失函 数定义如下：

Li'


loss ——


9.2.2 l2损失函数

类似地，对 N 个样本的 l2 损失函数定义如下：



在实际使用中， li 与 l2 损失函数在回归精度上几乎相差无几，不过在一些情 况下12损失函数可能会略优于li [98],同时收敛速度方面l2损失函数也略快 于li损失函数。两者的函数示意图如图9.4a和图9.4b所示。

9.2.3 Tukey’s biweight 损失函数

同分类任务中提到的坡道损失函数一样，Tukey’s biweight损失函数[3]也是一 类非凸损失函数,可克服在回归任务中的离群点或样本噪声对整体回归模型的 干扰和影响,是回归任务中的一种鲁棒损失函数,其定义如下：

NM

EE

i=i t=i

1



if |itl





















* * *




# COMMENT
