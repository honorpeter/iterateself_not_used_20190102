---
title: DL 网络正则化
toc: true
date: 2018-06-26 19:32:23
---
# REF
1. 《解析卷积神经网络》魏秀参


# TODO






  * aaa



* * *




# INTRODUCTION






  * aaa



















网络正则化



机器学习的一个核 样本上表现良好， 的这样一种表现我 ability)。若某学习

以说该学习算法有

非常糟糕，我们说

合” (overfitting) 1



ng algorithm)不仅在训练 效，学习算法在新数据上 泛化能力”(generalization

测试集依然工作良好，可

集表现优异，但测试集却

，这种现象也称为“过拟


由于我们非常关心模型的预测能力，即模型在新数据上的表现，而不希望过 拟合现象的发生，我们通常使用“正则化”(regularization)技术来防止过拟合 情况。正则化是机器学习中通过显式的控制模型复杂度来避免模型过拟合、确 保泛化能力的一种有效方式。如图10.1,如果将模型原始的假设空间比做“天 空”，那么天空中自由飞翔的“鸟”就是模型可能收敛到的一个个最优解。在 施加了模型正则化后，就好比将原假设空间(“天空”)缩小到一定的空间范围






,是机器学习中的一个基本概念。给定一个假设空间H, —个假设h属 h属于H,使得在训练样例上h的错误率比h'小，但在整个实例分布

那么就说假设 h 过度拟合训练数据。


“笼子”)，这样一来，可能得到的最优解(“鸟”)能搜寻的假设空间也变得相对

有限。有限空间自然对应复杂度不太高的模型，也自然对应了有限的模型表达

能力，这就是“正则化能有效防止模型过拟合”的一种直观解释。


图10.1:模型正则化示意

许多浅层学习器(如支持向量机等)为了提高泛化性往往都要依赖模型正则

化，深度学习更应如此。深度网络模型相比浅层学习器巨大的多的模型复杂度

是把更锋利的双刃剑：保证模型更强大表示能力的同时也使模型蕴藏着更巨大

的过拟合风险。深度模型的正则化可以说是整个深度模型搭建的最后一步，更

是不可缺少的重要一步。本章将介绍五种实践中常用的卷积神经网络正则化方

法。

10.1 12正则化
l2 正则化与下节的 li 正则化都是机器学习模型中相当常见的模型正则化方式。 深度模型中也常用二者对其操作层(如卷积层、分类层等)进行正则化，约束

模型复杂度。假设待正则的网络层参数为^, 12正则项形式为：

12 = 2*||2.    (lo.i)

其中，A控制正则项大小，较大的A取值将较大程度约束模型复杂度；反之易

然。实际使用时，一般将正则项加入目标函数，通过整体目标函数的误差反向

传播，从而达到正则项影响和指导网络训练的目的。

12正则化方式在深度学习中有个常用的叫法是“权重衰减”(weight decay), 另外I2正则化在机器学习中还被称作“岭回归” (ridge regression)或Tikhonov 正贝1J化(Tikhonov regularization)。

10.2. li正则化

10.2 li正则化
类似的，对于待正则的网络层参数^, li正则化为:

l1


=AIMli



需注意， li 正则化除了同 l2 正则化一样能约束参数量级外， li 正则化还能起 到使参数更稀疏的作用。稀疏化的结果使优化后的参数一部分为0,另一部分 为非零实值。非零实值的那部分参数可起到选择重要参数或特征维度的作用， 同时可起到去除噪声的效果。此外, l2 和 li 正则化也可联合使用,形如：

AiI MI i + A2

这种形式也被称为“Elastic网络正则化” [101

10.3最大范数约束
最大范数约束(max norm constraints)是通过向参数量级的范数设置上限对网 络进行正则化的手段,形如：

(10+4)

其中


c 多取 10 3 或


的内容请参考本书附录A。

10.4随机失活
随机失活(dropout) [78]是目前几乎所有配备全连接层的深度卷积神经网络都 在使用的网络正则化方法。随机失活在约束网络复杂度同时,还是一种针对深 度模型的高效集成学习(ensemble learning) [100]方法。

传统神经网络中,由于神经元间的互联,对于某单个神经元来说,其反向传 导来的梯度信息同时也受到其他神经元的影响,可谓“牵一发而动全身”。这就 是所谓的“复杂协同适应”效应(complex co-adaptation)。随机失活的提出正 是一定程度上缓解了神经元之间复杂的协同适应,降低了神经元间依赖,避免

了网络过拟合的发生。其原理非常简单：对于某层的每个神经元,在训练阶段

均以概率p随机将该神经元权重置0 (故被称作“随机失活”，测试阶段所有

神经元均呈激活态，但其权重需乘 (1-p) 以保证训练和测试阶段各自权重拥有

相同的期望，如图10.2所示。





图10.2:单个神经元的随机失活(dropout)示意。

由于失活的神经元无法参与到网络训练，因此每次训练(前向操作和反向操

作)时相当于面对一个全新网络。以含两层网络、各层有三个神经元的简单神经

网络为例(见图10.3),若每层随机失活一个神经元，该网络共可产生9种子网 络。根据上述随机失活原理，训练阶段相当于共训练了 9个子网络，测试阶段 则相当于9个子网络的平均集成(average ensemble)。类似的，对于Alex-Net 和VGG等网络最后的4096 x 4096全连接层来讲，使用随机失活后，便是指 数级(exponentially)子网络的网络集成，对于提升网络泛化性效果显著。

另外,还需注意随机失活操作在工程实现时并没有完全遵照其原理,而是 在训练阶段直接将随机失活后的网络响应(activation)乘以$，这样测试阶 段便不需做任何量级调整。这样的随机失活被称为“倒置随机失活” (inverted

10.5验证集的使用
通常,在模型训练前可从训练集数据随机划分出一个子集作为“验证集”,用以

在训练阶段评测模型预测性能。一般在每轮或每个批处理训练后在该训练集和

验证集上分别作网络前向运算,预测训练集和验证集样本标记,绘制学习曲线,

以此检验模型泛化能力。

10.5.验证集的使用





图10.3:两层网络、各层含三个神经元的随机失活情形：每层随机失活一个神 经元，共种情况。

以模型的分类准确率为例，若模型在训练集和验证集上的学习曲线(learning curve)如图10.5a所示：验证集准确率一直低于训练集上的准确率，但无明显下

降趋势。这说明此时模型复杂度欠缺，模型表示能力有限——属“欠拟合”状

态。对此，可通过增加层数、调整激活函数增加网络非线性、减小模型正则化

等措施增大网络复杂度。相反，若验证集曲线不仅低于训练集，且随着训练轮 数增长有明显下降趋势(如图10.5b),说明模型已经过拟合。此时，应增大模

型正则化削弱网络复杂度。


图10.4:单个神经元的倒置随机失活(inverted dropout)示意。






图10.5:模型欠拟合和过拟合。

除了上述几种网络正则化方式，借助验证集对网络训练“及时停止” （early stopping,也称为“早停”）也是一种有效的防止网络过拟合方法：可取验证集 准确率最高的那一轮训练结果作为最终网络,用于测试集数据的预测。此外, 在数据方面“做文章”，比如增加训练数据或尝试更多数据扩充方式（第5章） 则是另一种防止过拟合的方式。

10.6小结
§ 网络正则化是深度网络模型搭建的关键一步,可有效防止网络过拟合、提

升其泛化能力；

l2 正则化和 li 正则化是卷积网络中较简单常用的正则化方法,一般而言 l2正则化效果优于li正则化；li正则化可求得稀疏解；另外，二者可联 合使用，此时被称为“Elastic网络正则化”； 最大范数约束是通过约束参数范数对网络施加正则化,它有一个非常吸引

人的优势在于,由于最大范数约束对参数范数约定了上限,即使网络学习

率设置过大也不至于导致“梯度爆炸”； 随机失活是目前针对全连接层操作有效的正则化方式,实际工程实现时多

10.6.小结

采用“倒置随机失活”实际使用中随机失活可与12等正则化方法配合使 用；另外，随机失活已经获得了 2014年的一项美国专利2 ;

§ 在网络训练时可通过验证集上的学习曲线评估模型训练效果,对网络训练

“及时停止”也是一种有效的防止网络过拟合方法；

§ 增加训练数据、使用更多的数据扩充方式也是防止网络过拟合的有效方 式；此外也可以在网络分类层加入随机噪声,从而隐式增加对模型的约 束,提高模型泛化能力。

ihttps://www.google.com/patents/W02014105866Al

















* * *




# COMMENT
