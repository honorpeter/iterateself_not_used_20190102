---
title: 06 模仿学习
toc: true
date: 2018-07-05 21:33:08
---


模仿学习



亦称“学徒学习”

(apprenticeship learning), “示范学习” (learning from demonstration), “观

察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节.


在强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后 的累积奖赏，但在现实任务中，往往能得到人类专家的决策过程范例，例如在种 瓜任务上能得到农业专家的种植过程范例.从这样的范例中学习，称为“模仿 学习” (imitation learning).

16.6.1直接模仿学习

强化学习任务中多步决策的搜索空间巨大，基于累积奖赏来学习很多步之 前的合适决策非常困难，而直接模仿人类专家的“状态-动作对”可显著缓解这 一困难，我们称其为“直接模仿学习”.

假定我们获得了一批人类专家的决策轨迹数据｛Tl,T2,...5Tm｝5每条轨迹 包含状态和动作序列

Ti =    s2> a2? • • • 5 4+1〉，

其中％为第S条轨迹中的转移次数.

有了这样的数据，就相当于告诉机器在什么状态下应选择什么动作，于是 可利用监督学习来学得符合人类专家决策轨迹数据的策略.

我们可将所有轨迹上的所有“状态-动作对”抽取出来，构造出一个新的数 据集合

D = ｛（Sl5 ai）5 （s2, «2）, . • •,    ）
即把状态作为特征，动作作为标记；然后，对这个新构造出的数据集合I）使用 分类（对于离散动作）或回归（对于连续动作）算法即可学得策略模型.学得的这 个策略模型可作为机器进行强化学习的初始策略，再通过强化学习方法基于环 境反馈进行改进，从而获得更好的策略.

16.6.2逆强化学习

在很多任务中，设计奖赏函数往往相当困难，从人类专家提供的范例数据 中反推出奖赏函数有助于解决该问题，这就是逆强化学习（inverse reinforcement learning） [Abbeel and Ng, 2004].

在逆强化学习中，我们知道状态空间X、动作空间式并且与直接模仿学 习类似,有一个决策轨迹数据集逆强化学习的基本思想是：欲 使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略, 该最优策略所产生的轨迹与范例数据一致.换言之，我们要寻找某种奖赏函数 使得范例数据是最优的，然后即可使用这个奖赏函数来训练强化学习策略.

不妨假设奖赏函数能表达为状态特征的线性函数，即= wT®.于是， 策略7T的累积奖赏可写为

'+OO    '


=E

■+oo    ■
y | 7r

=wtE


"+OO

_t=0


7T


(16.37)

即状态向量加权和的期望与系数w的内积.

将状态向量的期望E 简写为注意到获得，需求取期 望.我们可使用蒙特卡罗方法通过采样来近似期望，而范例轨迹数据集恰可看 作最优策略的一个采样，于是，可将每条范例轨迹上的状态加权求和再平均，记 为，.对于最优奖赏函数E(x) = w^Tx和任意其他策略产生的^r，有

w*Tac* - w*T^ =    > 0 .    (16.38)
若能对所有策略计算出即可解出

w* = arg max minwT(»* — x^)    (16.39)

w    冗

s.t. ||w||    1

显然，我们难以获得所有策略,一个较好的办法是从随机策略开始，迭代地 求解更好的奖赏函数，基于奖赏函数获得更好的策略，直至最终获得最符合范 例轨迹数据集的奖赏函数和策略，如图16.15算法所示.注意在求解更好的奖 赏函数时，需将式(16.39)中对所有策略求最小改为对之前学得的策略求最小.

输入：环境均

状态空间X;

动作空间次

范例轨迹数据集D = {ti,T2, . . . , Tm}.

过程：

1:念* =从范例轨迹中算出状态加权和的均值向量；

2： 7T =隨机策略；

3: for 4 = 1,2,... do

4：    =从7T的采样轨迹算出状态加权和的均值向量；

5： 求解 w* = argmaxw min*=1 wT(念* — xf) s.t. ||w||    1;

6：    % =在环境〈X, A, R{x) = w*Ta?)中求癖最优策略；

7： end for

输出：奖赏函数R(x) = w*Tx与策略TT

图16.15迭代式逆强化学习算法














* * *




# COMMENT




强化学习专门书籍中最著名的是［Sutton and Barto, 1998］. ［Gosavi, 2003］ 从优化的角度来讨论强化学习，［Whiteson, 2010］则侧重于介绍基于演化算法 搜索的强化学习方法.［Mausam and Kolobov, 2012］从马尔可夫决策过程的视 角介绍强化学习，［Sigaud and Buffet, 2010］覆盖了很多内容，包括本章未介绍 的部分可观察马尔可夫决策过程(Partially Observable MDP,简称POMDP)、 策略梯度法等.基于值函数近似的强化学习可参阅［Busoniu et al., 2010］.

欧洲强化学习研讨会(EWRL)是专门性的强化学习系列研讨会，多学科强 化学习与决策会议(RLDM)则是从2013年开始的新会议.

［Kaelbling et al., 1996］是一个较早的强化学习综述，［Kober et al., 2013; Deisenroth et al., 2013］则综述了强化学习在机器人领域的应用.

“后悔”(regret)是指在 不确定性条件下的决策与 确定性条件下的决策所获 得的奖赏间的差别.

Samuel跳棋工作参见

p.22.


［Kuleshov and Precup, 2000］和［Vermorel and Mohri, 2005］介绍了 多种 尺-摇臂赌博机算法并进行了比较.多摇臂赌博机模型在统计学领域有大量研 究［Berry and Fristedt, 1985］,近年来在“在线学习” (online learning)、“对 抗学习 ” (adversarial learning)等方面有广泛应用，［Bubeck and Cesa-Bianchi, 2012］对其“悔界”(regret bound)分析方面的结果进行了综述.

时序差分(TD)学习最早是A. Samuel在他著名的跳棋工作中提出, ［Sutton, 1988］提出了 TD(A)算法，由于［Tesauro, 1995］基于 TD(A)研制的 TD-Gammon程序在西洋双陆棋上达到人类世界冠军水平而使TD学习备受 关注.Q-学习算法是［Watkins and Dayan, 1992］提出，Sarsa则是在Q-学习算 法基础上的改进［Rummery and Niranjan, 1994］. TD学习近年来仍有改进和 推广，例如广义TD学习［Ueno et al., 2011］＞使用资格迹(eligibility traces)的 TD 学习［Geist and Scherrer, 2014］等.［Dann et al., 2014］对 TD 学习中的策略 评估方法进行了比较.

模仿学习被认为是强化学习提速的重要手段［Lin, 1992; Price and Boutili-er, 2003］,在机器人领域被广泛使用［Argali et al., 2009］. ［Abbeel and Ng, 2004; Langford and Zadrozny, 2005］提出了逆强化学习方法.

在运筹学与控制论领域，强化学习方面的研究被称为“近似动态规 划” (approximate dynamic programming),可参阅［Bertsekas，2012］.



## 相关资料
1. 《机器学习》周志华
