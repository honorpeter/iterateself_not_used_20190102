---
title: 10 TF.Learn 从入门到精通
toc: true
date: 2018-06-26 20:29:20
---
### TF.Learn从入门到精通



TRLeam是TensorFlow中的一个很重要的模块，它包括各种类型的深度学习及流行 的机器学习算法。这个模块是从之前比较热门的TensorFlow官方Scikit Flow项目迁移过 来的，发起者是谷歌的员工IUia Polosukhin及本书作者之一唐源。代码的风格采用数据 科学界比较热门的Scikit-leam风格’旨在帮助数据科学从业者更好、更快地适应和接受 TensorFlow的代码。它囊括了许多TensorFlow的代码和设计模式，从而使用户能够更快 地开始搭建自己的机器学习模型来实现不同的应用。同时,用户也能极大地避免代码重复， 更好地把精力放在搭建更精确的模型上。自从TensorFlow v0.9版本发布之后，TF.Leam 能够无缝地和其他contrib模块结合起来使用，比如contrib.losses、contrib.layer、 contrib.metrics,等等（我们会在第11章系统地介绍contrib模块）。第10章和第11章将 使用TensorFlow 0.11.0-rc0版本作为示例讲解，其他版本的代码可能会出现不兼容的现象。

[www.aibbt.com](http://www.aibbt.com) 让未来触手可及

##### 10.1 分布式 Estimator

本节我们介绍Estimator的分布式特性、自定义模型的用法、Estimator的架构，并介 绍怎样建立自己的分布式机器学习Estimatoro

厂Te门sorFlow实跋

###### 10.1.1分布式Estimator自定义模型介绍

Estimator包括各种各样的机器学习和深度学习的类，用户能直接使用这些高阶类， 同时可以根据实际的应用需求快速创建自己的子类。有了 graph_actions模块的帮助， Estimator很大一部分在训练和评估模型时需要用到的复杂的分布式逻辑都被实现和浓缩， 使用者就不再需要把精力放在很复杂的Supervisor和Coordinator分布式训练具体实现细 节和逻辑上面。

Estimator接受自定义模型，目前它接受以下几组不同的函数签名。

(1 ) (features, targets) -> (predictions, loss, train一op)

(2 ) (features, targets, mode) -> (predictions, loss, train一op)

(3 ) (features, targets, mode, params) -> (predictions, loss, train_op)

我们以第一组函数签名举例说明，以下是一个简单的自定义的模型。

import tensorflow as tf

from tensorflow.contrib import layers

from tensorflow.contrib import learn def my_model(features, target):

target = tf. one 一 hot (target 3_, 1, 0)

features = layers.stack(features^ layers.fully—connected, [10, 20, 10]) prediction, loss =

tf.contrib.learn.models.logistic_regression_zero_init(features> target)

train_op = tf.contrib.layers.optimize_loss(

losstf.contrib.framework.get_global_step(), optimizer:• Adagrad•, learning_rate=0.1)

return {'class': tf.argmax(prediction1), 'prob': prediction}, loss, train_op

这个自定义模型接受两个参数：features和targets。features是数据的特征，targets是 数据特征每一行的目标或者分类的标识，利用tf.one_hot对targets进行读热编码(One-hot Encoding),让接下来损失函数的计算更方便。接下来，用layers.stack叠加多层

layers.fhlly_comected完全连接的深度神经网络，每一层分别有10、20、10个隐藏节点， 通过不同层的转换和训练，得到新的数据特征。TF.Learn里面的models模块有很多经常 使用的模型(比如逻辑回归)，这里我们用models.logistic_regression_zero_init加一层，以 0作为初始参数值的逻辑回归模型，这也是深度学习里比较常用的一种方法，从而得到最 后的预测值和损失值。最后，使用contrib.layers.optimize_loss函数对损失值进行优化，可 以根据需要选择不同的优化函数和学习率，optimizejoss会得到一个训练算子(Training Operator ),在每次训练迭代时会被用来优化模型的参数和决定模型发展的方向。这个自 定义模型函数需要返回一些要求的值，比如预测值及预测概率、损失值和训练算子。读者 可以比较灵活地使用Python的字典来返回预测值及预测概率，也可以只返回预测值和预 测概率中的一个，这样做的主要目的是在之后能够更方便地使用esdmator.predict函数。

接下来，我们把定义好的模型运用到比较常用的iris数据进行分类。 from sklearn import datasets, cross_validation

iris = datasets.load_iris()

x_trairb x_testj y_train4 y_test = cross_validation.train_ test_split( iris.data, iris.targettest_size=0.2_, random_state=35)

classifier = learn. Estimator(model_f n=my__model)

classifier.fit(x_trainj y_train, steps=700)

predictions = classifier.predict (x_test)

我们利用Scikit-leam的datasets引入数据，并用cross_validation把数据分为训练和评 估。接下来把我们定义好的my_model直接放进leam.Estimator就可以使用Scikit-leam风 格的fit和predict函数。通过快速和简单地定义自己的模型函数，能直接利用Estimator 的各种功能，也能够直接进行分布式模型训练，完全不用担心许多实现的细节［比如不同 的线程之间的交流和主监督(Master Supervisor)的建立］。

目前我们只介绍了其中一种自定义函数签名，其他的函数签名大同小异。简单来说， 模式(Mode河以被用来定义函数的使用阶段，例如training、evaluation，以及prediction。 这些常用的模式可以在ModeKeys里面找到。一些比较复杂的深度学习模型可能会包含一 些特殊的层，例如batch normalization层要求一些计算只发生在训练期间，而评估期间需

要跳过那些计算，所以可以在自定义函数里加一些条件语句来实现这样的复杂逻辑。 params是可以由自定义模型来调节的参数，读者使用fit函数时可以给更多的参数。具体 细节请参考TF.Leam的官方文档。

###### 10.1.2建立自己的机器学习Estimator

10.1.1节我们简单介绍了怎样用自定义的模型使用Estimato,接下来，我们来了解 Estimator的一些基本架构及如何通过实现自己的Estimator子类建立自己的机器学习分布 式 Estimator。

BaseEstimator是最抽象也是最基本的实现TensorFlow模型的训练和评估的类。它提 供了许多简单易用的功能，比如用fit()对模型进行训练，用partial_fit()进行线上训练，用 evaluateO评估模型，用prediCt()使用模型并对新的数据进行预测，等等。它利用了许多包 含在graph_actions里很复杂的逻辑进行模型的训练和预测。前面章节简单提到过它包含 了许多类似Supervisor、Coordinator、QueueRunner的使用，从而使它能够进行分布式地 训练和预测。它也使用了许多leam.DataFeeder或者learn.DataFrame的类来自动识别、处 理和迭代不用类型的数据。再加上estimators.tensor_signature的帮助对数据进行兼容性的 判断[比如稀疏张量(Sparse Tensor)],使数据的读入更加方便和稳定。与此同时， BaseEstimator也对leam.monitors及模型的存储等进行了初始化设置。leam.monitors是用 来监测模型的训练的，在接下来的章节里我们也会对它进行简单的介绍。

虽然BaseEstimator已经提供了大多数建立和评估模型要求的逻辑，但它却把 _get_train_ops()、_get_eval_ops()ffi_get_predict_ops()等实现留给了它的子类，从而让它的 子类能够更自由地实现自定义的一些逻辑处理。10.1.1节中我们使用到的Estimator刚好 提供了怎样实现BaseEstimator那些未实现方法的样本。

我们以Estimator为例，它的_get_train_ops()接受features和targets为参数，使用自定 义的模型函数返回一个Operation和损失Tensor的Tuple,这个函数会被用来在每个训练 迭代时对模型的参数进行优化。如果想实现自己的Estimator,你有绝对的自由来决定训 练的逻辑。例如，如果想实现一个非监督学习模型的Estimator,那么可以在这个函数里 对targets进行忽略。

和_§时_仕以11_叩5()类似，_get_eval_ops()让BaseEstimator的子类来使用自定义的 metrics评估每个模型训练的迭代。在TensorFlow高阶的模块里，比如contrib.metrics，可 以找到许多直接使用的metrics，第11章会对这个模块进行简单的介绍。自定义的metrics 函数需要返回一个Tensor对象的Python字典来代表评估Operation,每次迭代时都会被用 到。以下是Estimator的_@61_^1!_(^()的实现：

predictions^ loss, _ = self._call_model_fn(featuresJ targets^ ModeKeys.EVAL) result = {'loss': contrib.metrics,streaming_mean(loss)}

先用到自定义的模型对新的数据进行预测和计算损失值，用ModeKeys中的EVAL表 明这个函数只会在评估时被用到，然后用到了 contrib.metrics模块里的streaming_mean对 loss计算平均流，也就是在之前计算过的平均值基础上加上这次迭代的损失值再计算平均 值。

_get_prediCt_Ops()是用来实现自定义的预测的，例如在这个函数里可以对预测的结果 进行进一步的处理。再比如，把预测概率转换成简单的预测结果，把概率进行平滑加工 (Smoothing ),等等。这个函数需要返回一个Tensor对象的Python字典来代表预测 Operation。一旦这个函数被实现，就可以很轻松地使用Estimator的predict()函数，充分 利用Estimator的分布式功能，完全不用担心一些复杂的内部实现逻辑。如果想建立非监 督模型，也可以很快地在这个基础之上实现一个类似Scikit-learn里面的transforai()函数。

在TF.Leam的模块里也可以找到许多自定义机器学习Estimator的例子，例如逻辑回 归(LogisticRegressor )。由于Estimator已经提供了绝大部分需要的实现，LogisticRegressor 只需要提供自己的metrics (例如AUC、accuracy、precision,以及recall,只用来处理二分 类的问题)，所以可以很快地在LogisticRegressor的基础上写一个子类来实现一个更个性 化的二分类的Estimator,完全不需要担心其他逻辑的实现。

TF.Leam里的随机森林模型TensorForestEstimator把许多很细节的实现放到了 contrib.tensor_forest里，只利用和暴露一些比较高阶的，需要用到的成分到 TensorForestEstimator里，这样用户就能更择松地使用这个高阶机器学习模块。下.面的代 码中，它所有的超参数都通过contrib.tensor_forest.ForestHParams被传到构造函数的params 里，然后在构造函数里使用params.fill()建造随机森林的TensorFlow图，也就是 tensor_forest.RandomForestGraphso

class TensorForestEstimator(estimator.BaseEstimator):

n""An estimator that can train and evaluate a random forest.H"n

def _init_(selfparams, device_assigner=Nonej model_dir=Nonej

graph_builder_class=tensor_forest.RandomForestGraphs^ master:1、accuracy_metric=None_, tf_random_seed=NoneJ config=None):

self.params = params.fill()

self.accuracy_metric = (accuracy_metric or

('r2' if self.params.regression else 'accuracy')) self,data_feeder = None self.device_assigner =(

device_assigner or tensor_forest.RandomForestDeviceAssigner()) self.graph一builder_class = graph_builder_class self.training_args = {}

self.construction_args = {}

super(TensorForestEstimator4 self)._init_(model_dir=model_dir, config=config)

由于很多实现太复杂而且通常需要非常有效率，它的很多细节都用C++实现了单独 的Kernel。它的_getjpredict_ops()函数首先使用tensor_forest内部C++实现的 data_ops.ParseDataTensorOrDict()函数检测和转换读入的数据到可支持的数据类型，然后 利用 RandomForestGraphs 的 inference_graph 函数得到预测的 Operation。

def _get_predict_ops(selffeatures):

graph—builder = self.graph_builder_class(

self.params^ device_assigner=self.device_assigner, training=Falsej **self.construction_args)

features, spec = data_ops.ParseDataTensorOrDict(features)

_assert_float32(features)

return graph builder.inference graph(features, data_spec= spec)

类似地，它的 _get_train_ops()和 _get_eval_ops()函数分别调用了 RandomForest Graphs.training_loss()和 RandomForestGraphs.inference_graph()函数，它使用了 data_ops.ParseDataTensorOrDict 和 data_ops.ParseLabelTensorOrDict 分别检测和转换 features和targets到可兼容的数据类型。

希望以上关于Estimator架构的介绍和几个例子能够帮助读者更好地了解Estimator。

一旦读者建立好了自己的机器学习Estimator或者准备好使用Estimator,可以轻松地在多 台机器上、多个服务器上进行分布式的模型训练，10.1.3节会介绍RunConfig来帮助读者 更好地调节程序运行时参数。

###### 10.1.3调节RunConfig运行时参数

RunConfig是TF.Leam里的一个类，用来帮助用户调节程序运行时参数，例如用 num_cores选择使用的核的数量，用num_ps_replicas调节参数服务器的数量，用 gpu_memory_fraction控制使用的GPU存储的百分比，等等。

值得注意的是，RunConfig里master这个参数是用来指定训练模型的主服务器地址的， task是用来设置任务ID的，每个任务ID控制一个训练模型参数服务器的replicao以下是 —个例子，读者可以先初始化一个RunConfig对象，再把这个对象传进Estimator里。

config = tf.contrib.learn.RunConfig(task=0j master:"'

gpu_memory_fraction=0.8)

est = tf.contrib.learn.Estimator(model_fn=custom_modelj config= config)

以上例子是使用RunConfig参数的默认值在本地运行一个简单的模型,只使用一个任 务ID和80%的GPU存储作为参数传进Estimator里。当读者运行时，这些运行时参数会 被自动运用上，不用担心ConfigProto、GPUOptions之类的使用细节。读者可以快速地改 变这些参数来实现分布式模型的训练及参数服务器的使用，10.1.4节会简单介绍。注意, 这些API未来会有改动，所以最新的使用方法请参考TF.Leam官方文档。

###### 10.1.4 Experiment 和 LearnRunner

Experiment是一个简单易用的建立模型实验的类，它包含了建模所需要的所有信息， 例如Estimator,训练数据、评估数据、评估指标、监督器、评估频率，等等。可以选择 在当地运行，也可以和RunConfig配合进行分布式地试验。LearnRunner是用来方便做实 验的一个模块。接下来我们举个简单的例子说明。

先用tf.app.flags定义一些可以从命令行传入的参数，例如数据、模型、输出文件的路 径、训练和评估的步数等。这里有几个值得注意的参数。schedule是指想做的试验类型, 比如使用local_run()在当地做试验，可能的一些选项是Experiment里面的一些函数名字, 例如run_std_server()可以在标准服务器上做试验。master_grpc_url是主要的GRPC TensorFlow服务器。num_parameter_servers是参数服务器的数量，等等。

flags = tf.app.flags

flags.DEFINE_string("data_dir"j "/tmp/census-data",

"Directory for storing the cesnsus data data")

flags.DEFINE_string(,,model_dirn',/tmp/census_wide_and_deep_model"

"Directory for storing the model”)

flags.DEFINE_string("output_dir'\ "Base output directory.")

flags.DEFINE_string(11 schedule", "local_runn3

"Schedule to run for this experiment.")

flags.DEFINE_string("master_grpc_url"J

"URL to master GRPC tensorflow server, e.g.?"

"grpc://127.0.0.1:2222")

flags.DEFINE_integer(,,num_parameter_servers"0j

"Number of parameter servers")

flags.DEFINE_integer(,,worker_index,,J 0^ "Worker index (>=0)”)

flags.DEFINE_integer("train_steps,,J 1000, "Number of training steps") flags.DEFINE_integer(neval_stepsn1} "Number of evaluation steps")

FLAGS = flags.FLAGS

接下来写一个建立Experiment对象的函数，在这个函数里首先使用之前设置好的一 些FLAGS建立好RunConfig及想要建立的机器学习模型Estimator,这里我们建立广度深 度结合分类器(DNNLinearCombinedClassifier )。注意，我们省略了 input_train_fh 和 input_test_fn的定义，这两个方程会定义数据的来源、提供训练，以及评估所用的数据。 我们在接下来的机器学习Estimator里都会用到。不同的数据有不同的导入方法，在这里 我们就不详细介绍了。

def create_experiment_fn(output__dir):

config = run_config.RunConfig(master=FLAGS.master_grpc_url^

num_ps_replicas=FLAGS.num_parameter_serversJ task=FLAGS.worker_index)

estimator = tf.contrib.learn.DNNLinearCombinedClassifier^ model_dir=FLAGS.model_dir, linear_feature_columns=wide_columns.,

dnn_feature_columns=deep_columnsj dnn_hidden_units=[5], config=config)

return tf.contrib.learn.Experiment( estimator=estimatorj

train_input_fn=data_source.input_train_fn, eval_input_fn=data_source.input_test_fnj train_steps=FLA6S.train_stepsj eval_steps=FLAGS.eval_steps)

然后就可以把create_experiment_fn()函数传入LeamRunner里进行不同类型的试验， 例如当地试验或者服务器试验，以及把试验的结果存储到不同的路径中，代码如下。

learn_runner.run(experiment_fn=create_experiment_fnj output_dip=FLAGS.output_dir? schedule=FLAGS.schedule)

##### 10.2 深度学习 Estimator

TF.Learn里包含了许多深度学习Estimator的实现，高阶的API让用户使用起来更方 便。本节介绍一些基本的高阶深度学习API及它们和TensorFlow其他模块结合使用的例 子。

###### 10.2.1深度神经网络

TF.Learn里包含简单易用的深度神经网络Estimator,例如分类问题可以使用 DNNClassifier,下面我们介绍一个最简单的例子。先在_input_fn()里建立数据，这里使用 layers模块建立两个特征列-—年龄和语言(后面我们将详细介绍它们的使用方法)。 def _input_fn(num_epochs=None):

features = { ' age' : tf .train. limit_epochs(tf. constant ([[.8] [ .2] j [ .1] ]) 4 num_epochs=num_epochs)

'language1: tf.SparseTensor(values=[*en'3 'fr、 'zh']^

indices=[[0, 0], [0, 1]4L 0]], shape=[3, 2])}

return features) tf.constant([[1]) [0]， [0]]^ dtype=tf.int32)

language_column = tf.contrib.layers.sparse_column_with_hash_bucket(

1 language* hash_bucket_size=20)

feature_columns =[

tf.contrib.layers.embedding_column(language_columrb dimension=l) tf.contrib.layers.real_valued_column(’age')

]

接着就把特征列、每层的隐藏神经单元数、标识类别数等传入DNNClassifier里来迅 速地建立我们的深度神经网络模型。

classifier = tf.contrib.learn.DNNClassifier( n_classes=2j

feature_columns=feature__columnSj hidden_units=[3j 3]

config=tf.contrib.learn.RunConfig(tf_random_seed=l))

然后使用我们习惯的fit()、evaluate)等方法进行模型的训练和评估。

classifier.fit(input_fn=_input_fnj steps=100)

scores = classifier.evaluate(input_fn=_input_fnj steps=l)

在许多实际应用中，每行数据都有它们的权重，比如在图片分类运用中，每张图片的 标识来自于不同的标识者，它们的可信度不一样，所以每张图片的标识权重也不同。在 DNNClassfier中，我们可以指定一列为权重列，然后它会帮我们自动分配到训练过程中去。 在以下的例子中，我们建立四行数据，每行有不同的权重，我们先把权重列和特征列放在 features 里面。

def _input_fn_train():

target = tf.constant([[1]， [0], [0], [0]]) features = {

*x’： tf.ones(shape=[4) 1]) dtype=tf.float32)

'w*: tf.constant([[100.[3.], [2.][2.]])

}

return features, target

然后就可以在DNNClassifier中表明权重列的列名，在这里也就是w,然后表明特征 列的列名x (注意：我们需要将x转换为特征列)。

classifier = tf.contrib.learn.DNNClassifier( weight_column_name= 'vj'j

feature_columns=[tf.contrib.layers.real_valued_column('x')]^ hidden_units=[3j 3],

config=tf.contrib.learn.RunConfig(tf_random_seed=3))

classifier.fit(input_fn=_input_fn_trainsteps=100)

我们也可以传入进我们自定义的metrics方S_my_metric_op(),需要做的就是操作 predictions和targets进行我们心目中的metrics计算，此处只考虑二分类的问题，使用 tf.slice()剪切predictions的第二列当作最终的预测值。

def _input_fn_train():

target = tf .constant ([[1],    [©]> [0]])

features = {'x': tf.ones(shape=[4, 1], dtype=tf.float32),}

return features, target

def _my_metric_op(predictionstargets):

predictions = tf.slice(predictions, [0, 1], [-1, 1]) return tf.reduce_sum(tf.mul(predictions, targets))

这里我们举个例子来帮助理解tf.slice(),假设我们有以下矩阵。

tf.slice()需要传入输入矩阵input，剪切的开始元素begin,以及剪切的Tensor的形状 size, size[i]代表了第 i 个维度想剪切的矩阵的 shape,例如 tf.slice(input，[1，0，0]，[1，1， 3])可以得到[[[3, 3, 3]]];tf.slice(input3 [1, 0, 0], [1, 2, 3])可以得到[[[3, 3, 3]，[4, 4, 4]]]。

我们根据需求任意地在predictions和targets上操作来实现想要的metrics计算，然后 就可以在evaluate()时传入自己定义好的metrics函数，TF.Leam会根据你所指示的metrics

—「TensorFlow 实战

评估模型。

classifier = tf.contrib.learn.DNNClassifier(

feature_columns=[tf.contrib.layers.real_valued_column("x*)]^ hidden_units=[3j 3]j

config=tf.contrib.learn.RunConfig(tf_random_seed=l))

classifier.fit(input_fn=_input一fn_train) steps=100)

scores = classifier.evaluate( input_fn=_input_fn_train? steps=100j

metrics={

'my_accuracy': tf.contrib.metrics.streaming_accuracyj (,my_precision,? 'classes'): tf.contrib.metrics.streaming_precisionj ('my_metric', 'probabilities'): _my_metric_op})

值得注意的是，我们可以在evaluate()时提供多个metrics,其中一^_my_metric_op 是我们之前自定义好的，其他两个是tf.contrib里自带的，之后的章节中也会简单提到一 些内建的metrics的用法。

我们也可以在提供optimizer时提供自己定义的函数，例如，可以定义自己的优化函 数来包含指数递减的学习率。

def optimizer_exp_decay():

global_step = tf.contrib.framework.get_or_create_global_step() learning_rate = tf.train.exponential_decay(

learning_rate=0.global_step=global_stepJ decay_steps=100J decay_pate=0.001)

return tf.train.AdagradOptimizer(learning_rate=learning_rate)

这里用tf.contrib.framework.get_or_create_global_step()得到目前模型训练到达的全局 步数，然后使用tf.train.exponential_decay()对学习率进行指数递减，这种方法在许多应用 中特别常用，尤其是用来避免爆炸梯度之类的问题。

接着可以将这个自定义的优化函数放入DNNClassifier里继续使用我们熟悉的方法建

立深度神经网络分类器及它的训练，我们用iris数据举个例子。

iris = datasets.load_iris()

x_train3 x_testj y_trairb y_test = train一test_split(iris.data) iris.target^ test_size=0.2^ random_state=42)

feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input( x_train)

classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columnsJ hidden_units=[10j 20, 10], n_classes=3j

optimizer=optimizer_exp_decay)

classifier.fit(x_train_, y_train_, steps=800)

###### 10.2.2广度深度模型

广度深度模型的DNNLinearCombinedClassifier是谷歌最新研究的成果，研究团队将 这个模型在TF.Leam里面实现，然后开源，这样更有利于其他研究者再次重复实验结果 及学习。这个模型被谷歌广泛地利用在各种机器学习应用中，它是深度神经网络和逻辑回 归的结合，因为在谷歌的研究中发现，将不同的特征通过两种不同的方式结合起来，更能 够体现应用的意义及更有效的推荐结果，这其实也和Kaggle竞赛中经常使用的Ensemble 的方法比较类似。

使用的DNNLinearCombinedClassifier的方法和之前介绍的DNNClassifier及在接下来 将介绍的LinearClassfier的使用方法类似，唯一的区别是你有更多的参数，并且可以将不 同的特征列选择使用到DNNClassifier或者LinearClassfier中。

gender = tf.contrib.layers.sparse_column_with_keys(

"gender", keys=["female", "male"]) education = tf.contrib.layers.sparse_column_with_hash_bucket(

"education"hash_bucket_size=1000) relationship = tf.contrib.layers.sparse_column_with_hash一bucket(

"relationship", hash_bucket_size=100) workclass = tf.contrib.layers.sparse_column_with_hash_bucket(

"workclass"hash_bucket_size=100)

wide_columns = [gender, education]

deep_columns = [relationship, workclass]

m = tf.contrib.learn.DNNLinearCombinedClassifier( model_dir=model_dirj linear_feature_columns=wide_columns? dnn_feature_columns=deep_columnSj dnn_hidden_units=[100j 50])

我们将 gender、education、relationship、workclass 都转换为 FeatureColumn，这是特 征工程中特别重要的一步。然后，将它们分为wide_colurnns和deep_columns,其中 wide_columns 将被用在 LinearClassifier 中，deep_columns 会被用在 DNNClassifier 中，然 后将它们分别传入DNNLinearCombinedClassifier建立广度深度模型，这样模型既具有线 性特征，也具有深度神经网络特征。官方网站的Tutorials ([https://www.tensorflow.org/tutorials/)](https://www.tensorflow.org/tutorials/)%e4%b8%8a%e6%9c%89%e9%9d%9e%e5%b8%b8%e6%9c%89%e6%84%8f%e6%80%9d%e7%9a%84%e4%be%8b%e5%ad%90%ef%bc%8c%e5%bb%ba%e8%ae%ae%e8%af%bb%e8%80%85%e5%8e%bb%e5%ad%a6%e4%b9%a0%e5%b9%b6%e5%ba%94%e7%94%a8%e5%88%b0)[上有非常有意思的例子，建议读者去学习并应用到](https://www.tensorflow.org/tutorials/)%e4%b8%8a%e6%9c%89%e9%9d%9e%e5%b8%b8%e6%9c%89%e6%84%8f%e6%80%9d%e7%9a%84%e4%be%8b%e5%ad%90%ef%bc%8c%e5%bb%ba%e8%ae%ae%e8%af%bb%e8%80%85%e5%8e%bb%e5%ad%a6%e4%b9%a0%e5%b9%b6%e5%ba%94%e7%94%a8%e5%88%b0) 自己的项目中。

##### 10.3    机器学习 Estimator

TF.Leara里不仅包括了许多流行的深度学习Estimator,还包括了各种各样的机器学 习算法，例如随机森林、支持向量机，等等。这让TF.Learn及TensorFlow与现有的其他 软件包的界限和特色更明显。在谷歌内部的大力支持及外部开源社区的代码贡献下，相信 TF.Learn会成为未来的分布式Scikit-leam。接下来，我们将介绍TF.Learn里比较流行的 机器学习高阶API。

###### 10.3.1线性/逻辑回归

使用TF.Learn建立大家熟悉的线性或者逻辑回归非常简单，与之前提到的深度神经 网络的使用方法类似。

举个简单的例子，假设我们在input_fh()里建立简单的两个特征列的数据，分别是年 龄和语言，以及它们的标识，这里我们用简单的常数代替阐述，使用在后面章节会提到的

特征列API建立稀疏的语言特征列和真值的特征列。

def input_fn(): return {

•age1: tf.constant([l])j

'language': tf.SparseTensor(values=['english*]j indices=[[0, 0]] shape=[lJ 1])

}_, tf.constant([[1]])

language = tf. contrib.layers.sparse_column_with_hash_bucket('language'100) age = tf.contrib.layers.real_valued_column('age')

然后就可以将这些特征列传入LinearClassifier里建立逻辑回归分类器，使用熟悉的 fit()、evaluate()等函数。注意，我们可以使用get_variable_names()得到所有模型包含的变 量的名称：

classifier = tf.contrib.learn.LinearClassifier( feature_columns=[age> language])

classifier .fit (input一fn=input_fn steps=100)

classifier.evaluate(input_fn=input_frb steps=l)['loss']

classifier.get_variable_names()

类似地，我们也可以像前文介绍的那样，使用自定义的优化函数，这里使用 tf.train.FtrlOptimizer()进行优化，也可以对它进行任意改动然后传到LinearClassifier里：

classifier = tf.contrib.learn.LinearClassifier( n_classes=3j

optimizer=tf.train.FtrlOptimizer(learning_rate=0.1), feature__columns=[feature_column])

###### 10.3.2随机森林

随机森林是在工业界得到广泛应用的一种机器学习算法，它是一个包含多个决策树的 分类器及回归算法。在许多实际的运用中，它的效果非常好，尤其是处理不平衡的分类资 料集时，它极大地平衡了误差。在许多Kaggle数据科学竞赛中，它的延伸版XGBoost更

是帮助了许多竞赛者取得了优异的成绩。

TF.Leam里含有随机森林Estimator,接下来我们用iris数据及随机森林Estimator进 行分类。

hparams = tf.contrib.tensor_forest.python.tensor_forest. ForestHParams(

num_trees=3j max_nodes=1000j num_classes=3j num_features=4)

classifier = tf.contrib.learn.TensopForestEstimatop(hparams)

在之前的章节中讲解过TensorForestEstimator代码的内部架构，首先需要使用 tensor_forest.ForestHParams设置随机森林的参数，例如多少棵树、节点数目的上限、特征 和类别的数目，等等。

iris = tf.contrib.learn.datasets.load_iris()

data = iris.data.astype(np.float32)

target = iris.target.astype(np.float32)

classifier.fit(x=data, y=target, steps=100)

然后直接传进TensorForestEstimator里初始化随机森林Estimator,接下来，把数据特 征列和类别列转换成float32的格式，这样能够保证TensorForestEstimator的训练更快地拟 合。接下来，可以直接使用Scikit-leam风格的fit()等方法。

类似地，我们也可以把这个初始化好的classifier运用到MNIST图像数据上，这里我 们从官方tutorials模块里导入MNIST数据。

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(FLAGS.data_dir3 one_hot=False)

一般在实际应用中，随机森林容易过拟合，一种常用的防止过拟合的方法就晕损失减 少的速度变慢或者完全停止减少的情况下，提前停止模型的继续训练。在TF.Leam里， 我们可以用Monitor模块达到这个目的。我们会在接下来的内容中仔细讲解它的各种用法， 但是以下我们给出一个常用的random_forest模块里自带的LossMonitor来迅速地达到我们 的目的。我们设定每隔100步Monitor检查损失减少的速度，如果连续100次迭代仍然没 有看见损失的减少，Monitor会让整个模型训练停止，这样在实际应用中是非常有效的。 from tensorflow.contrib.learn.python.learn.estimators import random_forest

early_stopping_rounds = 100

check一every_n_steps = 100

monitor = random_forest.LossMonitor(early_stopping_roundSj check_every_n_steps)

classifier.fit(x=mnist.train.images, y=mnist.train.labelsj batch_size=1000j monitors=[monitor])

results = estimator.evaluate(x=mnist.test.imagesj y=mnist.test.labels^ batch_size=1000)

###### 10.3.3 K均值聚类

K均值聚类是非常常见的一种聚类方法,它的核心是把多维空间里的每个点划分到K 个聚类中，使得每个点都属于离它最近的均值对应的聚类。TRLearn里也包含了 K均值 聚类的Estimator,我们来看一个简单的例子。

import numpy as np

def make_random_centers(num_centersj num_dims): return np.round(np.random.rand(num_centerSj

num_dims).astype(np.float32) * 500)

def make_random_points(centers, num一points, max_offset=20): num_centerSj num_dims = centers.shape assignments = np.random.choice(num_centerSj num__points) offsets = np.round(np.random.randn(num_points,

num_dims),astype(np.float32) * max一offset) return (centers[assignments] + offsets,

assignments,

np.add.reduce(offsets * offsets, 1))

以上两个函数是利用NumPy制造比较适合做聚类的一组数据，make_random_centers 函数来随机生成num_dims个维度的数据集聚类的num_centers个中心点， make_randomjpoints函数根据所生成的聚类中心点随便生成num_points个点。我们生成 二维的10000个点，以及6个随机的聚类中心点。

num一centers = 6

num_dims = 2

num_points = 10000

true一centers = make_random_centers(num_centers^ num_dims)

points, scores = make_random_points(true_centersJ num_points)

接着，可以调用factorization模块里KMeans里的一些初始化聚类的方法，例如随机 初始化RANDOM_INIT，然后传入RunConfig及聚类中心数来初始化KMeans的Estimator 对象，最后就可以像其他Estimator 一样使用Scikit-leam风格的fit()和predict(),读者可 以通过KMeans的clustersO函数来看训练数据集每个点的聚类分布。

from tensorflow.contrib.factorization.python.ops import kmeans as kmeans_ops from tensorflow.contrib.factorization.python.ops.kmeans import \

KMeansClustering as KMeans kmeans = KMeans(num_centers=num_centerSj

initial_clusters=kmeans_ops.RANDOM_INITj use_mini_batch=Falsej config=RunConfig(tf_random_seed=14), random一seed=12)

kmeans.fit(x=pointsj steps=10j batch_size=8)

clusters = kmeans.clusters() kmeans.predict(points, batch_size=128)

kmeans.score(points? batch_size=128)

kmeans.transform(pointSj batch_size=128)

值得注意的是，KMeans的Estimator有多个经常用到的方法，使用predict()预测新的 数据点的聚类，使用score()预测每个点和它最近的聚类的距离的总和，以及用transform() 计算每个点和模型判断出来的聚类中心的距离。

###### 10.3.4支持向量机

支持向量机也是在机器学习应用中经常用到的一类算法，它包括使用各种不同的 kernel或者不同的距离方程，针对不同特征的数据建立不同的线性及非线性的模型。它们 有一个共同的特性就是能够同时最小化经验误差与最大化几何边缘区，所以也被称为最大 边缘区分类器。在文本及图像分类等领域得到广泛的使用。TF.Leam里面的SVM Estimator

提供了非常简单易用的API来建立支持向量机模型。

我们先定义input_fh()建立一个有着两个数据特征列、一个ID列和一个标识列的模拟 数据，然后使用 contrib.layers 里面的 FeatureColumn API 将 feature 1 和 feature2 转换为方 便和Estimator—起使用的FeatureColumn (我们将在第11章中详细介绍这个功能)。

def input_fn(): return {

•example_id•: tf.constant([11\ '2', '3']),

'featurel*: tf.constant([[0.0], [1.0], [3.0]])^ feature?*: tf.constant([[1.0][-1.2], [1.0]])^

}, tf.constant([[l], [0], [1]])

featurel = tf.contrib.layers.real_valued_column('featurel*)

feature2 = tf.contrib.layers.real_valued_column('feature2')

然后就可以将这些特征列及ID列传入SVM来初始化这个支持向量机，许多参数是 调节的，例如在ll_regularization和12_regularization中加入一些正规化来防止过度拟合之 类的问题，和我们之前在随机森林那一节简单提到过的问题相似，许多机器学习算法在特 征列过多而例子不多的情况下很容易发生这样的情况。

svm_classifier = tf.contrib.learn.SVM(feature_columns=[featurelj feature2] example_id_column=1 example一id *, Il_regularization=0.0j 12_regularization=0.0)

接下来就可以使用熟悉的fit()、evaluate()、predict()之类和其他Estimator共用的方法 了。

svm一classifier.fit(input_fn=input一fn_, steps=30)

metrics = svm_classifier.evaluate(input_fn=input_frb steps=l)

loss = metrics['loss']

accuracy = metrics['accuracy']

##### 10.4 DataFrame

TF.Leam还包括了一个单独的DataFrame模块，类似于Pandas、Spark或者R编程语 言里面的DataFrame,它提供了 TF.Leam所需的读入数据的迭代,包括读入各种数据类型， 例如 pandas.DataFrame、tensorflow.Example、NumPy,等等。它包括了 FeedingQueueRunner 等功能来对数据进行分批读入，然后存在一个Queue里，以便Estimator很容易地取过去 用于模型的训练。简单来说，FeedingQueueRunner在Estimator训练时同时进行了更多数 据的分批读入，这种多线程的方式使Estimator的训练更有效，也使TF.Leam的扩展性更 强。

以NumPy为例，假设我们用NumPy的eye()建了一个简单的对角矩阵，然后就可以 直接使用 TensorFlowDataFrame.fromjnumpyO将这个 NumPy 矩阵转换为 TensorFlow 的 DataFrame。

import tensorflow.contrib.learn.python.learn.dataframe.tensorflow_dataframe as df

x = np.eye(20)

tensorflow_df = df.TensorFlowDataFrame.from^umpyCXj batch一size =10)

类似地，我们也可以直接像Pandas—样读入各种文件类型，这里我们以csv文件为 例。

pandas_df = pd.read一csv(data_path)

tensorflow_df = df .TensorFlowDataFrame.from_csv( [data_path] _,enqueue_size=20, batch_size=10jshuffle=FalseJ default_values=default_values)

当使用TensorFlowDataFrame读入使用的文件或者数据类型之后，就可以使用run() 制造一个数据批量(batch)的生成器，也就是在Python里经常用yield生成的generator, 这个生成器维持着数据列名和数据值的字典mapping□可以调节number_batches来选择生 成的batch的数量，也可以选择性地使用自己的graph和session,这样数据的batch会被 存在对应session的coordinator里，以便之后更方便地获取。

tensorflow_df.run(num_batches=10, graph=graph, session=sess)

我们也可以使用batch()重新改变每个batch的大小，也可以选择将数据洗一遍来打乱

顺序，很多应用都通过这种方式增加数据的随机性。

tensorflow_df.batch(batch_size=10j shuffle=truej num_threads=3)

还有许多实用的函数,例如用split()将DataFrame分成多个DataFrame，用select_rows() 选择具体某行数据，等等。这里我们就不多介绍了。DataFrame将会被主要用在Estimator 里，这样用户就可以把精力放在数据的供给，而不用担心数据的数据类型和文件类型。这 一模块以后的变化将会很大，请读者参考最新的官方文档和代码。

##### 10.5 监督器 Monitors

训练模型时，没有程序日志的话整个过程就像是个黑匣子，我们很难知道模型的进展 及发展方向，例如模型在进行各种优化，使用SGD做优化时，我们无法看到模型是否在 拟合及拟合的速度。

当然，用户可以把训练的过程分为几个部分，然后在fit()迭代时时不时地打印出一些 有用的信息，但是这样的程序往往会很慢。这时，TF.Leam里自带的Monitor就派上用场 了，它提供各种logging及监督控制训练的过程，这样用户就能更清楚地知道模型是否在 进行有效的训练。在之前的章节中我们简单提到过，接下来将给出详细的例子来分析 Monitors的使用方法。

TensorFlow有5个等级的log,以严重性最小到最大排列，它们是DEBUG、INFO、 WARN、ERROR,以及FATAL。当用户选择好log的等级之后，只有那个等级和更严重 等级的log会被打印出来。举例来说，如果等级设置为ERROR,那么你会看到ERROR 和FATAL等级的log;如果等级设置为DEBUG,那么所有等级的log都会打印出来。 TensorFlow的默认log等级是WARN,所以如果想在模型训练时看到log，需要用下面这 行代码把等级改到INFOo

tf.logging.set_verbosity(tf.logging.INFO)

改了等级之后,你会看到类似以下的log。注意这些是由一个默认的Monitor提供的， 每100步会打印出一些损失值信息。

INFO:tensorflow:Training steps [0^200)

INFO:tensorflow:global_step/sec: 0

INFO:tensorflow:Step 1: loss_l:0 = 2.34635

INFO:tensorflow:training step 100) loss = 0.18227 (0.001 sec/batch).

INFO:tensorflow:Step 101: loss_l:0 = 0.191003 INFO:tensorflow:Step 200: loss_l:0 = 0.0835024

INFO:tensorflow:training step 200) loss = 0.080932 (0.002 sec/batch).

TF.Leam提供几个方便使用的高阶Monitor类，例如用Capture Variable将一个指定的 变量的值存储到一个Collection里，用PrintTensor打印Tensor的值，用Summary Saver存 储Summary所需要的协议缓冲(Protocol Buffer), ValidationMonitor在训练时打印多个 评估Metrics,以及监督模型的训练以便提前停止训练防止模型的过度拟合。这些不同的 Monitor都会在每隔TV步时执行。

接下来，我们将详细地讲解怎样使用Monitor,主要以ValidationMonitor作为例子。 首先，假设手头有CSV格式的iris数据，我们可以使用TELearn自带的 leam.datasets.base.load_csv()读入这些 CSV 数据文件。

import numpy as np

import tensorflow as tf

iris_train = tf.contrib.learn.datasets.base.load_csv( filename=,,iris_training.csv", target_dtype=np.int)

iris_test = tf.contrib.learn.datasets.base.load_csv( filename=,,iris_test.csvHJ target_dtype=np.int)

接着，定义一个评估模型的metrics字典，这里使用contrib.metrics模块里面的 streaming_accuracy、streamingjprecision,以及 streaming_recall,对模型的准确度、精石角 度，以及召回率进行评估。

validation一metrics = {"accuracy": tf.contrib.metrics.streaming一 accuracy, "precision": tf.contrib.metrics.streaming_precision, ’’recall": tf.contrib.metrics,streaming_recall}

然后用定义好的validation_metrics建立一个validation_monitor,这里需要提供用来评 估的数据及目标，提供every_n_steps来指示每50步以实行一次这个ValidationMonitor, 把之前定义好的validation_metrics传入metrics，用early_stopping_metric选择用来提前停 止戶斤需要监测的metric, early_stopping_metric_minimize=True表明我们需要最小化之前提 供的early_stopping_metrico最后，用early_stopping_rounds表明如果超过200步训练损失 仍然不减少，ValidationMonitor会停止Estimator的训练。

validation_monitor = tf.contrib.learn.monitors.ValidationMonitor( iris_test.dataj iris_test.target every_n_steps=50j metrics=validation_metricsj early_stopping_metric="loss"4 early_stopping_metric_minimize=TrueJ early_stopping_rounds=200)

紧接着，我们建立一个深度神经网络分类器DNNClassifier,它有三层神经网络，每 一层分别有10、15和10个隐藏单元。我们在分类器进行fit()时来指定我们定义好的监督 器validation_monitor,注意，也可以指定多个监督器来实现不同功能的监督，例如 [validation_monitor, debug_monitor5 print_monitor]。

classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 15, 10], n_classes=3j

model_dir="/iris_model_dir"4

config=tf.contrib.learn.RunConfig(save_checkpoints_secs=2))

classifier.fit(x=iris_train.dataJy=iris_train.targetJsteps=1000J monitors=[validation_monitor])

接下来，可以使用我们熟悉的evaluateO或者predictO用新的数据评估模型的准确度。

accuracy_score = classifier.evaluate(x=iris_test.data• y=iris_test. target) ["accuracy**]

new_samples = np.array([[5.243.1J6.5J2.2]^ [2.8,3.2,5.5,3.3]], dtype=float) y = classifier.predict(new_samples)

我们将会得到类似以下的log,可以观察到模型在750步时被终止了，因为损失值没 有继续减少。

INFO:tensorflow:Validation (step 950): recall = 1.0_, accuracy = 0.966667, gl obal_step = 932) precision = 1.0j loss = 0.0608345

INFO:tensorflow:Stopping. Best step: 750 with loss = 0.0581324.

虽然ValidationMonitor提供了很多信息和功能，但是当训练步数很大时，我们很难观 察模型的准确率到底是怎么变化的。值得庆幸的是，TRLeam生成的log及checkpoint的 文件是能够直接读入TensorBoard里进行可视化的。如果在命令行里执行以下几行，就会 在给出的地址里看到TensorBoard对整个模型训练可视化，如图10-1所示。

$ tensorboard --logdir=/iris_model_dir/

Starting TensorBoard 22 on port 6006

(You can navigate to <http://0.0.0.0:6006>)

Q    <i Sit}!：j vine i> a ibtg.-pup X

口 Split on umkrsccxet

口 D#W d wnlo«d links



accuracy

accuracy

cent#r©d_bias.O

centered_bi85_1



Horizontal Axit

RILATiVC    WALL

Wrtlffa rps»lo runs

图10-1 TensorBoard对模型训练的可视化

图片来源于tensorf low. org
