---
title: 03 命令式 符号式
toc: true
date: 2018-09-01
---


# 需要补充的

- 还是要系统的学习一下 Caffe，保证对于所有类型的项目，都能够熟练的做出来，别人的论文也能够熟练的重现出来。



# MXNet 基本使用


在第 3 章已经提到过，当描述一个神经网络或者一些计算公式及函数的时候，实质上是在描述一种可以用图表示的计算关系。在MXNet中，这种计算关系可以有两种方式表达和计算，即命令式(Imperative)和符号式(Symbolic)。比如 (a+b)*c，命令式计算的代码如下：


```python
import mxnet as mx

# 申请内存并赋值，默认利用CPU
a = mx.nd.array([1])
b = mx.nd.array([2])
c = mx.nd.array([3])

# 执行计算
d = (a + b) * c

# 将结果以NumPy的array的形式表示 [9.]
print(d.asnumpy())
# 将结果以标量的形式表示 9.0
print(d.asscalar())
```

和 NumPy 的 array 很像，其实 NDArray 里很多的操作和方法确实和 NumPy 是一样的。

回到正题，相应的符号式计算的代码如下：

```python
import mxnet as mx

# 定义3个符号变量，注意符号变量都需要一个显式指定的名字
a = mx.sys.Variable('a')
b = mx.sys.Variable('b')
c = mx.sys.Variable('c')

# 定义计算关系
d = (a + b) * c

# 指定每个输入符号对应的输入
input_args = {
    'a': mx.nd.array([1]),
    'b': mx.nd.array([2]),
    'c': mx.nd.array([3]),
}

# a、b、c和d定义的只是计算关系，执行计算(包括申请相应内存等操作)需要Executor
# 用bind ()函数指定输入，d为输出，cpu ()指定计算在cpu上进行
executor = d.bind(ctx=mx.cpu(), args=input_args)
# 执行计算
executor.forward()
# 打印结果，[9.]
print(executor.outputs[0].asnumpy())
```

<span style="color:red;">这种方式看了有点眼熟，与 Tensorflow 感觉差不多类似的</span>

可以看到，命令式计算非常灵活直接，每个变量的内存分配是即时完成的，计算也是即时完成。而符号式计算则是函数式编程的思路，计算也是延迟(lazy)的，符号变量只能定义计算关系。<span style="color:red;">这就是函数式编程的思路吗？想知道这个思路到底是什么？</span>

这种计算关系在执行前需要通过 bind() 方法产生一个执行器(Executor), 用来把数据的 NDArray 和 Symbol 绑定起来，实际的计算发生在 Executor 调用时。

符号式计算很明显要麻烦一些，不过优点是延迟计算和对计算图的优化能得到更优的性能。另外，在MXNet中通过符号式计算求导是非常方便的，继续接前面例子：<span style="color:red;">为什么对计算图的优化能得到更优的性能？</span>

```python
import mxnet as mx

# 定义3个符号变量，注意符号变量都需要一个显式指定的名字
a = mx.sys.Variable('a')
b = mx.sys.Variable('b')
c = mx.sys.Variable('c')

# 定义计算关系
d = (a + b) * c

# 指定每个输入符号对应的输入
input_args = {
    'a': mx.nd.array([1]),
    'b': mx.nd.array([2]),
    'c': mx.nd.array([3]),
}

# a、b、c和d定义的只是计算关系，执行计算(包括申请相应内存等操作)需要Executor
# 用bind ()函数指定输入，d为输出，cpu ()指定计算在cpu上进行
executor = d.bind(ctx=mx.cpu(), args=input_args)
# 执行计算
executor.forward()
# 打印结果，[9.]
print(executor.outputs[0].asnumpy())

#定义一个变量用来保存关于a的梯度，随便初始化一下
grad_a=mx.nd.empty(1)

#在bind ()函数中指定要求梯度的变量
executor=d.bind(
    ctx=mx.cpu(),
    args=input_args,
    args_grad={'a':grad_a}
)
#因为梯度是传播的，所以最后输出节点的梯度需要指定，这里用1
executor.backward(out_grags=mx.nd.ones(1))
#计算出梯度为 3.0，也就是 c 的值，将自动刷新在 grad_a 中
print(grad_a.asscalar())
```

<span style="color:red;">什么意思？上面的求导的过程没大看懂，为什么最后的输出的节点的梯度需要指定？而且 `args_grad={'a':grad_a}` 是用来做什么的？ </span>


在 MXNet 中，第一段代码中，用于命令式计算的 NDArray 是一个非常基础的模块，符号式计算的 Symbolic 模块结合 NDArray 一起使用可以定义一些基础的计算关系并进行计算。在这两个模块基础上可以搭建一些简单的计算关系，比如神经网络。但是如果每次都像上面代码一样从底层搭建，并且自己指定计算梯度等操作，甚至更进一步比如在神经 网络中进行后向传播和梯度更新等，将是一件非常麻烦的事情。所以在 NDArray 和 Symbolic 基础上，MXNet提供了一些接口进行封装来简化这些操作，包含通用性更好的 Module 模块和更为简单的 Model 模块。<span style="color:red;">嗯。</span>

既然要训练模型，就不能避免与数据和机器打交道，所以MXNet也提供了数据读取和处理的 IO Data Loading 模块和用来支持多 GPU 卡及分布式计算的 KVStore 模块。本书 作为一本入门书籍，将主要涉及 6 大模块中除了 KVStore 以外的模块，神经网络模型的搭 建也主要基于 Module 和 Model 模块，而不需要从 Symbolic 开始进行复杂的底层编写。更多关于这些模块的细节可以参考官方文档 http://mxnet.io/zh/api/python/ 。<span style="color:red;">嗯，这些模块都要总结，而且 KVStore 也要掌握，</span>





## 相关资料

- 《深度学习与计算机视觉》
