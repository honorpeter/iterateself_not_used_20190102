---
title: 11 TF.Contrib 的其他组件
toc: true
date: 2018-06-26 20:29:18
---
### TF.Contrib的其他组件

TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一些比较新的功能，由于都是一些刚贡献的功能，谷歌会将这些代码暂时 放在这里，由谷歌内部及外部的用户一起测试，根据反馈意见改进性能和改善API的友 好度，等它们的API都比较稳定时，会被移到TensorFlow的核心模块。

这个模块里提供了机器学习需要的大部分功能，包括统计分布、机器学习层、优化函 数、指标，等等。本章将简单介绍其中的一些功能让大家了解TensorFlow的涵盖范围和 感受到社区积极地参与和贡献度。注意这部分功能在未来会不断变动和改进，如果是写生 产代码的话，请以最新的官方教程和API指南作为最权威的参考。

##### 11.1统计分布

在 TF.Contrib.distributions 模块里有许多的统计分布，例如 Bernoulli、Beta、Binomial、 Gamma、Exponential、Normal、Poisson、Uniform,等等。这些统计分布大多数都是统计 研究和应用中经常用到的，也是各种统计及机器学习模型的基石，许多的概率模型和图形 模型（例如Bayesian模型）都非常依赖这些统计分布。

每个不同的统计分布有着不同的特征和函数,但是它们都是从同样的子类Distribution

扩展开来的，Distribution是建立和组织随机变量和统计分布的一个最基础的类，它有着 许多有用的属性及类函数，例如用is_continuous表明这个随机变量分布是不是连续的， 用allow_nan_stats表示这个分布是否接受nan的数据，用sample()从这个分布里取样，用 prob()计算随机变量密度函数，用cdf()求累职分布函数，以及用entropy()、mean()、std()、 varianceO得到统计分布的平均值和方差之类的特征。如果想贡献自己的统计分布类，需 要实现一些对应以上的方程，例如_11^11()、_std(^n_variance()，也需要实现_is_continuous 之类表明这个变量分布的属性。

我们接下来以实现好的Gamma分布为例来说明这个模块的大概使用方法。首先，从 contrib.distributions 里导入 Gamma 分布，然后初始化 alpha 和 beta 的 tf.constant,这些 constant被用于建立Gamma分布，我们可以通过batch_shape().eval()得到每个样本的形状， 这里例子的样本形状shape 1是(5，)，我们也可以使用get_batch_shape()得到样本形状，但 是是以tf.TensorShape的类出现的，这个例子里shape2是tf.TensorShape(5),两种方法各 有所长，需要依据具体应用的需求来使用。

from tensorflow.contrib.distributions import Gamma

import tensorflow as tf

alpha = tf.constant([3.0] * 5)

beta = tf.constant(11.0)

gamma = Gamma(alpha=alphaj beta=beta)

shapel = gamma.batch一shape().eval()

shape2 = gamma.get_batch_shape()

然后，可以用lOg_pdf()函数取对应的一些值的log转换后的概率密度函数，我们把6 个值放在numpy.array里，然后得到相应的log概率密度函数值。

x = np.array([2.54 2.5， 4.0, 0.1， 1.0/ 2.0], dtype=np.float32)

log_pdf = gamma.log_pdf(x)

也可以建立多维的Gamma分布，和一维的类似，只需要传入多维的alpha和beta参 数就可以建立多维的Gamma分布。同样，我们可以对多维的x取得相应的log概率密度 函数值。

batch_size = 6

alpha = tf.constant([[2.0, 4.0]] * batch__size)

beta = tf.constant([[3.0, 4.0]] * batch_size)

x = np.array([[2.5j 2.5, 4.0， 0.1, 1.0， 2.0]]) dtype=np.float32).T gamma = Gamma(alpha=alpha_, beta=beta) log_pdf = gamma.log_pdf(x)

##### 11.2 Layer 模块

Contrib.layer包含了机器学习算法所需的各种各样的成份和部件，例如卷积层、批标 准化层、机器学习指标、优化函数、初始器、特征列，等等。有了这些基础的建设部件, 我们可以高效地建立复杂的机器学习及机器学习系统。本章将介绍这个模块里一些主要的 音(5件，来帮助理解TensorFlow的各种可能性及灵活性。

###### 11.2.1机器学习层

contrib.layers里含有许多常用的深度学习及机器学习的层，例如卷积层、pooling层、 批标准化等,这些都是各种模型必不可少的部分，也是机器学习研究领域最活跃的一部分。

深度学习和计算机视觉里经常用到的二维的平均池是avg_pool2d0我们用 np.random.uniform建立宽和高都是3的几张假图片，读者可以通过 contrib.layers.avg_pool2d()对图片快速地建立3x3的二维平均池，这里output的形状是[5, 1，1，3],因为我们对每个3x3的区域取计算平均值。

height^ width = 3^ 3

images = np.random.uniform(size=(5_, heightwidth, 3))

output = tf.contrib.layers.avg_pool2d(images[3, 3])

用类似的方法建立卷积层，这里使用同样的图片矩阵，然后用 contrib.layers.convolution2d()建立一个有32个3><3过滤器的卷积层，也可以改动.stride、 padding, aCtivatiOn_fh等参数建立不同架构的卷积层，使用不同的卷积层激活函数。

output = tf.contrib.layers.convolution2d(imagesj num_outputs=32j kernel_size= [3_, 3])

值得注意的是，contrib.layers会自动建立op的名字，例如output.op.name的值是 'Conv/Relu1,因为我们使用了 Conv层及使用了 ReLU的激活函数，这些layer有自己对应 的op名字，然后会在每个op空间存储对应的变量，可以通过

contrib.framework.get_variables_by_name()得到对应的op空间变量的值。例如，可以用 get_variables_by_name得到我们建立的卷积层的权重，这里权重的形状，也就是 weights_shape 的值，是[3，3，4, 32] o

weights = tf.contrib.framework.get_variables_by_name('weights1)[0]

weights_shape = weights.get_shape().as_list()

接下来我们看看怎么将卷职层layers.convolution2d()和批标准化层layers.batch_norm() 结合使用，我们先建立一些图片的矩阵。

images = tf. random_uniform((5^ heightwidth, 32), seed=l)

接着，使用contrib.framework里面的arg_scope减少代码的重复使用，我们将 layers.convolution2d及一些即将传入的参数放入arg_scope中，这些参数通常是接下来会 被重复使用的，把它们放在arg_SCOpe里就可以避免重复在多个地方传入，这里需要用到 的参数是normalizer_fn和normalizer_params,也就是需要用到的标准化方程及它所需要 的参数，一但在arg_scope里设置了这些，接下来用到convolution2d()时就不用重复传入 normalizer_fii 和 normalizer_params 这两个参数了。

with tf.contrib.framework.arg_scope(

[tf.contrib.layers.convolution2d]j normalizer_fn=tf.contrib.layers.batch_norm? normalizer一params={'decay•: 0.9}):

net = tf.contrib.layers.convolution2d(images_, 32s [3, 3])

net = tf.contrib.layers.convolution2d(net32_, [3_, 3])

可以看到，TensorFlow自动帮我们建立好了默认的一些层的名字。以上的例子里， 我们可以通过 ler^tf.contrib.framework.geLvariablesCConv/BatchNorm1))得到第一个 Conv/BatchN orm 层的长度。

再来看一个完全连接的神经网络层fUlly_COnneCted()的例子。首先，建立一些输入的 矩阵，用fUlly_connected()建立一个输出7个神经单元的神经网络层。 heightj width =3,3

inputs = tf.random一uniform((5_, height * width * 3), seed=l)

with tf.name_scope('fe'):

fc = tf. contrib. layers. fully__connected (inputs, 1}

outputs_collections='outputs'3 scope='fc*)

output_collected = tf.get_collection('outputs1)[0]

self.assertEquals(output_collected.alias, 'fe/fc')

值得注意的一些小细节是，我们利用tf.name_scope将截下来的运算放进一个 namejcope里，这样以后就可以更简单地找到我们想要的某个层的值，我们在 fhlly_connected()里传入一个scope，然后就可以通过“fe/fc”，也就是这个层的别号得到这 个层的一些信息。我们通过传入的outputs_collections，可以直接得到这个层的输出。

在contrib.layers里有许多特别方便使用的方法，例如，可以通过repeat()重复使用同 样的参数重复建立某个层，例如y = repeat(x, 3, conv2d, 64, [3, 3], scope='convr)是和 以下代码等同的。

x = conv2d(x, 64, [3_, 3]，scope='convl/convl_l')

x = conv2d(Xj 64) [3， 3]， scope='convl/convl_2')

y = conv2d(Xj 64^ [3j 3], scope='convl/convl_31)

可以使用stack()来使用不同的参数建立多个fhlly_connected()层，我们可以建立一个 三层的完全连接的神经网络，每层的单元数分别为32、64和128。

y = stack(Xj fully_connected， [32， 64, 128]， scope='fc')

以上代码等同于：

x = fully_connected(xJ 32^ scope='fc/fc_l')

x = fully_connected(Xj 64』scope='fc/fc_2*)

y = fully_connected(x, 12% scope='fc/fc_3')

注意，stack会帮你建立一个新的scope,通过在scope里附加一个增量，例如在“fc” 的基础上加上“fc_l”、“fc_2”等。之前提到的repeat()也会使用类似的机制建立新的scope。

我们只简单介绍一些在深度学习中经常使用的层，如果想了解更多、更复杂的层，例 如 conv2d_transpose、conv2d_in_plane、separable_conv2d 等，可以参考官方文档o

###### 11.2.2损失函数

Tf.COntrib.lOSSeS模块里包含了各种常用的损失函数，适用于二类分类、多类分类，以

及回归模型等各式各样的机器学习算法。接下来，我们将举例说明它们的使用方法。

我们先以绝对差值举例说明，首先用tf.constant建立一些predictions和targets的数列。 注意，它们必须是同样的shape,然后可以选择性地建立权重，因为在许多实际应用中， 每个预测值的权重也是特别关键的。

predictions = tf• constant([4_, 8) 12) 8, 1_, 3]) shape=(2j 3))

targets = tf.constant([l, 9, 2, -5, -2, 6], shape=(2, 3))

weight = tf.constant([1.2^ 0.0], shape=[2,])

接着，可以使用losses.absolute_difference()计算这组预测的损失值，从而在之后的建 模中起到引导性的作用。

loss = tf.contrib.losses.absolute_difference(predictionsj targets, weight)

接下来，来看一个计算softmax交叉熵的例子，这种方法多适用于多类分类的机器学 习模型。同样地，我们先建立predictions和labels,与之前不一样的是，它们是多维的， 也就是softmax交叉嫡最擅长处理的。然后，使用losses.softmax_cross_entropy()计算这组 预测中softmax交叉嫡的值。注意，需要像其他TensorFlow的应用一样使用loss.eval()运 行得到它的值。可以从loss.op.name得到TensorFlow自动赋值的op的名字，这个情况下 它是'softmax^ross-entropyjoss/value1。其他的损失函数也是使用这样的命名习俗。

predictions = tf .constant ([[10.0)0.0_, 0.0], [0.0,10.0_,0.0]    [0.0_,0.0,10.0]])

labels = tf.constant([[l_, 0，0]，[0，1，0]，[0，0，1]])

loss = tf.contrib.losses.softmax—cross一entropy(predictions, labels)

loss.eval()

loss.op.name

其他的损失函数的使用方法大同小异，值得注意的是，许多损失函数里有许多的参数 可以使用。例如，可以使用softmax_cross_entropy()里面的label_smoothing将所有的标识 进行平滑，从而使在某些应用中计算出来的softmax交叉嫡更具有实际应用的代表性，使 用方法如下。

logits = tf.constant([[100.0, -100.0, -100.0]])

labels = tf.constant([[1_, 0, 0]])

label一smoothing = 0.1

loss = tf.contrib.losses.softmax_cross_entropy(logitsj labels) label_smoothing=label_smoothing)

许多应用大部分标识的分布都比较稀疏，可以使用sparse_softmax _cross_entropy(), 这样计算起来会更有效率。

logits = tf.constant([[10.0^ 0.0，0.0], 0.0) 10.0_, 0.0]j [0.0，Q.Qj 10.0]])

labels = tf.constant([[0]， [1]， [2]]^ dtype=tf.int64)

loss = tf.contrib.losses.sparse_softmax_cross_entropy(logitslabels)

###### 11.2.3 特征列 Feature Column

在很多数据科学和机器学习的应用中，大家都习惯以表格的形式存储和处理数据，然 后将数据输入机器学习模型中。处理数据的方式多种多样，例如Python里有大家熟悉的 Pandas包。数据从各种数据源得来，经过各种方式的清理、筛选、合并，以及特征工程， 然后进行模型的建立。在TensorFlow里怎样更好地进行我们的特征工程和建模的工作 呢？

TF.contrib.layers里有许多高阶的特征列(Feature Column ) API,可以让大家的特征 工程更有效率，然后紧密地和TELearn的API结合使用，建立最适合自己数据的模型。

接下来，我们将介绍如何使用这些高阶的特征列API及如何和TF.Leam结合使用。

数据里一般包含连续特征(Continuous Feature )及类别特征(Categorical Feature )□ 像花瓣的长度和宽度这样连续的数值特征称为连续特征,我们可以直接把它们用在模型里。 如果特征代表了类别，例如性别、种族这样的不连续的类别特征，那么往往需要对它们进 行处理，例如将它们数值化，也就是将它们转换为一系列的数值来代表每个不同的类别。 Feature Column API可以很方便地将各种类型的特征转换为想要的格式。

假设读者已经用类似以下的leam.datasets的API来读入数据，例如：

training_set = learn.datasets.base.load_csv(filename=iris_training, target_dtype= np.int)

test_set = learn.datasets.base.load_csv(filename=iris_testing, target_dtype=np.int)

接下来就可以用layers.FeatureColumn的API定义一些特征列，例如，使用 real_valued_column()定义连续的特征(如年龄、收入、开销，以及工作市场)。

from tf.contrib import layers

age = layers.real_valued_column("age")

income = layers.real_valued_column("income")

spending = layers.real_valued_column("spending")

hours_of_work = layers.real_valued_column("hours_of_work")

紧接着，用sparse_column_with_keys()处理像性别这样的类别特征。

gender = layers. sparse_column_with_keys(column_name=,,gender,'?

keys:「female"male'1])

注意，使用sparse_column_with_keys()前，必须要知道这个特征所有可能的值，本例 中，性别分为男性和女性。如果事先不知道所有可能的值，可以使用 sparse_column_with_hash_bucket()将特征转换为特征列。以教育程度这样的特征为例，由 于对数据不是特别熟悉，无法事先知道所有可能的教育程度，我们可以用哈希表建立这样 的特征。

education = layers.sparse_column_with_hash_bucket(neducation"?

hash_bucket_size=1000)

............... ........................................ 1—— - •- - ■ 1.....: J,,-'. ..…„ -X.    , ...i ....    ...........

sparse_column_with_keys()及 sparse_column_with_hash_bucket()都能将数据转换为 SparseColumn,然后可以直接在TF.Leam里使用，传入Estimator里。

有时，在数据科学的应用中，一些连续的特征可能需要被离散化，从而形成新的类别 特征，这样能更好地代表特征和目标分类类别之间的关系。例如年龄是连续特征，分类的 类别是职业的类别(如经理、猎头等)，往往这些职业的类别和年龄阶段有关，而不是简 单的数值年龄。因为18岁、19岁、20岁往往没有明显的区别，所以有时会将这样的连续 特征区间化和离散化，例如将I8岁~20岁分为一类。在FeatureColumn API里，.我们可 以很快地进行这样的转换。

age_range = layers.buck?tized_column(age, boundaries=[18J 25,    3S, 40)

45, 50, 55, 60, 65])

在以上的例子里，使用bucketized_column()将之前的年龄SparseColumn进行进一步 的区间化，将年龄段分为18岁~25岁、26岁~30岁、31岁~35岁，等等。

在许多应用里，一个好的模型不仅需要一些单独的特征列，有时两个或多个特征之间 的综合和交互与目标分类类别之间的关系更紧密。有时多个特征之间是相关的，使用特征 的交互往往能建立更有效的模型。例如，对年龄、职业和种族这三个特征，我们可以使用 crossed_column()建立交叉特征列：

combined = layers.crossed_column([age_rangerace_, occupation] hash_bucket_size=int(le7))

建立好各式各样的特征列之后，我们可以直接将它们传入不同的TF.Learn Estimator。 以下面这个简单的逻辑回归分类模型为例，我们可以使用之前介绍过的fit()、戸出呦等 方法训练和评估模型。

classifier = tf.contrib.learn.LinearClassifier(feature_columns=[

gender, education, occupation^ combined, age_rangej race, income, spending], model_dir=model_dir)

这里我们只是简单地介绍了一些比较常用的函数，在实际应用中有各种各样的需求， 例如有时想取一部分特征的加权求和作为一个新的特征列，可以使用 weighted_sum_from_feature_columns()来很快地实现。读者可以在官方文档里找到更多需 要的函数。

###### 11.2.4 Embeddings

在许多深度模型应用中，包含许多稀疏的、高维的类别特征向量，我们通常先把它们 转换成低维的、稠密的实数值的向量，也通常将它们和连续特征向量联合起来，一起输入 进神经网络模型中进行训练和优化损失函数，这些被统一称为嵌入向量(Embedding Vectors )o大部分文本识别都是先将文本转换成嵌入向量，然后对它们进行分析并用在模 型训练中。

contrib.layers模块里的embedding_column()能迅速将高维稀疏的类别特征向量转换为 读者想要的维数的嵌入向量，以下是一个例子。

embedding_columns =[

tf.contrib.layers.embedding_column(title, dimension=8)i tf.contrib.layers.embedding_column(education, dimension=8) tf.contrib.layers.embedding_column(gender^ dimension=8), tf.contrib.layers.embedding_column(race, dimension=8), tf.contrib.layers.embedding_column(country, dimension=8)]

这里的title、education、gender、race,以及country都是比较稀疏的类别特征向量， 我们通过使用embedding_column(),把它们转换为低维数的稠密向量，从而更好地归纳数 据中的特性，特别是当一组特征中的交互矩阵比较稀疏，级别比较高时，这种方法会使模 型更具有概括性且更有效。

接下来，可以直接将它们传入TF.Leam的Estimator里进行觀的建立、训练，以及 评估。例如，可以将 embedding_columns 传入 DNNLinearCombinedClassifier 里的深度神 经网络特征列里。

est = tf.contrib.learn.DNNLinearCombinedClassifier( model_dir=model_dirj linear_feature_columns=wide_columns? dnn_feature_columns=embedding_columnsj dnn_hidden_units=[100, 50])

embedding_columns()是contrib.layers模块里最简单易用的一个，当涉及实际数据时， 许多稀疏高维的数据里通常有空的特征及无效的ID，这时可以使用 safe_embedding_lookup_sparse()安全地建立嵌入向量。这里先用tf.SparseTensor建立好稀 疏的ID及稀疏的权重。

indices = [[0, 0], [0, 1], [0, 2], [1, 0], [3, 0], [4, 0], [4, 1]]

ids = [0, 1, -1, -1, 2, 0) 1]

weights = [1.0, 2.0^ 1.0, l.Q, 3.0, 0.0, -0.5]

shape = [5, 4]

sparse_ids = tf.SparseTensor(

tf.constant(indices, tf,int64)tf,constant(idSj tf.int64)j    .

tf.constant(shapetf.int64))

sparse_weights = tf.SparseTensor(

tf.constant(indices? tf.int64), tf.constant(weights, tf.float32), tf.constant(shape? tf.int64))

接下来，建立嵌入向量的权重embedding_weights,这取决于词汇量大小、嵌入向量 维数，以及shard数量。然后，使用initializer.runQ和eval()初始化嵌入向量的权重，具体

细节和参数的说明请参考最新的官方文档。

vocab_size=4

embed_dim=4

num_shards=l

embedding_weights = tf.create_partitioned_variables( shape=[vocab_sizej embed_dim] slicing=[num_shardsJ 1]_,

initializer=tf.truncated_normal_initializer(mean=0.0j

stddev=1.0 / math.sqrt(vocab_size), dtype=tf.float32))

for w in embedding一weights: w.initializer.run()

embedding_weights = [w.eval() for w in embedding_weights]

最后，可以使用safe_embedding_lookup_sparse()将原来的特征向量安全地转换为低维 和稠密的特征向量，这里使用eval()，然后将它们收集到一个tuple里。

embedding_lookup_result = (tf. contrib. layers. safe_embedding_lookup_sparse( embedding_weightssparse_idSj sparse_weights).eval())

##### 11.3性能分析器tf prof

TensorFlow也在Contrib模块里提供了自己的性能分析器tlprof,可以通过它帮助分 析模型的架构及衡量系统的性能。它涵盖了许多实用的功能，例如衡量模型的参数、浮点 运算、叩执行时间、要求的存储大小、探索模型的结构等。本节将简单地介绍一些功能。

首先，通过以下命令安装命令行的工具。

bazel build -c opt tensorflow/contrib/tfprof/...

可以通过以下命令查询帮助文件。

bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof help

可以执行互动模式，然后指定graph_path来分析模型的shape和参数。

bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof \

--graph一path=/graph.pbtxt

类似地，我们用graph_path和checkpoint_path查看checkpoint里Tensor的数据和相 对应的值。

bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof \

--graph_path=graph.pbtxt \

--checkpoint一path=model.ckpt

与此同时，我们可以多提供一个run_meta_path来查看不同op请求的存储和计时。

bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof \

--graph_path=graph.pbtxt \

--run_meta_path=run_meta \

--checkpoint_path=model.ckpt

值得注意的是，上面用到了 run_meta_path、graph_path和checkpoint」）ath几个路径， 我们是怎么得到这几种类型的文件的呢？

graphj^ath的文件是GraphDef文本文件，用来在内存里建立模型的代表，例如用 tf.Supervisor写出来的graph.pbtxt就是一个GraphDef文本文件的例子。如果不使用 tf.Supervisor,那么可以使用tf.Graph.as_graph_def（咸者其他类似的API存储模型的定义 到一个GraphDef文件里。

mn_meta_path所需的文件是tensorflow::RunMetadata的结果，这个方程是用来得到模 型中每个op所需的存储和时间消耗的，以下简单的几行代码可以写出一个RunMetadata 文件。

run_options = config_pb2.RunOptions(

trace_level=config_pb2.RunOptions.FULL_TRACE)

run_metadata = config_pb2.RunMetadata()    .

_ = self.一sess.run(…，options=run_optionsj run_metadata=run_metadata) with gfile.Open(os.path.join(output_dirj "run^eta'^^ "w") as f:

f.write(run_metadata.SerializeToStning())

checkpointjpath是模型的checkpoint，它包含了所有checkpoint的变量的op类型、shape 和它们的值。

读者也可以提供其他的路径，例如op_log_path路径，它是tensorflow::tfjprof::OpLog 的结果，包含了额外的叩的信息，由于它包含了 op的组的类别名字，用户可以很简单地 综合op的一些数据，而不会不小心错过其中一部分叩。以下用一个暴露出来的API来很 快地写出了一个OpLog文件。

tf.contrib.tfprof.tfprof_logger.write_op_log(graphj log_dirj op_log=None)

由于tfjjrof是一个CLI命令行的工具，当输入之前的tfjKof命令按下回车键时，会进 入互动模式，再按一下回车键会看到一些类似以下的命令行参数的默认值。

tfprof> -max_depth

-min_bytes

-min_micros

-min_params

-min_float_ops

-device_regexes -order_by

-account_type_regexes -start_name_regexes -trim_name_regexes -show_name_regexes -hide一name_regexes

4

0

0

0

0



name



Variable

木



•氺

VariableInitialized_[0-9]+^ save\/.*jAzeros[0-9_]*



-account_displayed_op_only false

-select    params

-viz    false -dump_to_file

然后就可以调节里面的参数，例如用show_name_regexes查找scope名字正则式为 unit_ 1 _0. * gamma, max_depth 为 5 的变量，查看这些 tensor 的值：

tfprof> scope -show_name_regexes unit_l_0.*gamma -select tensor_value \ -max_depth 5

读者会得到类似以下符合条件的tensor的值：

unit_l_0/shared_activation/init_bn/gamma ()

[1.80 2.10 2.06 1.91 2.26 1.86 1.81 1.37 1.78 1.85 1.96 1.54 2.04 2.34 2.22

1.99 L

unit_l_0/sub2/bn2/gamma ()

[1.57 1.83 1.30 1.25 1.59 1.14 1.26 0.82 1.19 1.10 1.48 1.01 0.82 1.23 1.21 1.14 ]

tfj)rof提供了两种类型的分析：scope和graph。当读者想查看一些变量和scope的值 的时候，你可以使用scope,也就是我们上面阐述过的例子。当读者想查看op在graph里 所花的内存和时间时，可以使用graph查看。

类似地，可以改动一些命令行参数，例如使用start_name_regexes选择想要查看的op 的名字，这里我们假设需要查看命名为cost的损失op的内存和时间花费的情况：

tfprof> graph -start一name_regexes cost.* -max_depth 100 -min_micros 10000 \

-select micros -account_type_regexes .*

init/init_conv/Conv2D (11.75ms/3.10sec)

random_shuffle_queue_DequeueMany (3.09sec/3.09sec)

unit_l_0/sub2/conv2/Conv2D (74.14ms/3.19sec)

unit__l__3/sub2/conv2/Conv2D (60.75ms/3• 34sec)

unit_2_4/sub2/conv2/Conv2D (73.58ms/3.54sec)

unit_3_3/sub2/conv2/Conv2D (10.26ms/3.60sec)

我们就先简单地介绍以上这些模块，这一部分变化很大，更多的功能还需要读者自己 摸索并查看官方文档。

k 296

[www.aibbt.com](http://www.aibbt.com) 让未来触手可及
