---
title: 01 训练
toc: true
date: 2018-09-25
---
# 需要补充的

# 训练

## 参数 <span style="color:red;">这个地方叫参数是不是不是很正确。</span>

模型的训练参数，共有5个，即：<span style="color:red;">这个怎么会是模型的训练参数呢？</span>


(1) 已标注边界框的图片数据集，其格式如下：

```
图片的位置 框的4个坐标和1个类别ID (xmin,ymin,xmax,ymax,id) ...
dataset/image.jpg 788,351,832,426,0 805,208,855,270,0
```

(2) 标注框类别的汇总，即数据集中所标注物体的全部类别，例如：

```
aeroplane
bicycle
bird
...
```

(3) 预训练模型，用于迁移学习中的微调，可选 YOLO v3 已训练完成的 COCO 模型权重，即：<span style="color:red;">一定要使用这个 COCO 的权重吗？</span>
<span style="color:red;"></span>

```
pretrained_path = 'model_data/yolo_weights.h5'
```

(4) 预测特征图的 anchor 框集合：<span style="color:red;">什么是 anchor 框？对这个地方不是很清楚</span>

- 3 个尺度的特征图，每个特征图 3 个 anchor 框，共 9 个框，从小到大排列；
- 框 1~3 在大尺度 52x52 特征图中使用，框 4~6 是中尺度 26x26，框 7~9 是小尺度 13x13 ；
- 大尺度特征图用于检测小物体，小尺度检测大物体；
- 9 个 anchor 来源于边界框的 K-Means 聚类。



例如，COCO 的 anchors 列表，如下：

```
10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
```

(5) 图片输入尺寸，默认为 416x416，选择 416 的原因是：<span style="color:red;">可以任意选定吗？图像的大小与之前我们的 COCO 的图像有关系吗？</span>

- 图片尺寸满足 32 的倍数，在 DarkNet 网络中，执行 5 次步长为 2 卷积，降采样，其卷积操作如下：<span style="color:red;">这个与 DarkNet 有什么关系？</span>

```
x = DarknetConv2D_BN_Leaky(num_filters, (3, 3), strides=(2, 2))(x)
```

- 在最底层时，特征图尺寸需要满足为奇数，如 13，以保证中心点落在唯一框中。如果为偶数时，则中心点落在中心的 4 个框中，导致歧义。<span style="color:red;">这句话是什么意思？</span>


## 创建模型

创建 YOLOv3 的网络模型，输入：

- input_shape：图片尺寸；
- anchors：9 个 anchor box；<span style="color:red;">这个 anchors 到底是什么概念？</span>
- num_classes：类别数；
- freeze_body：冻结模式，1 是冻结 DarkNet53 的层，2 是冻结全部，只保留最后 3 层；<span style="color:red;">冻结模式什么时候使用？</span>
- weights_path：预训练模型的权重。



即：

```python
model = create_model(input_shape,
                     anchors,
                     num_classes,
                     freeze_body=2,
                     weights_path=pretrained_path)
```

其中，网络的**最后3层**是：3个1x1的卷积层，用于将3个尺度的特征图，转换为3个尺度的预测值。<span style="color:red;">嗯。</span>

即：

```python
out_filters = num_anchors * (num_classes + 5)
// ...
DarknetConv2D(out_filters, (1, 1))
```

结构如下：

```
conv2d_59 (Conv2D)      (None, 13, 13, 18)   18450       leaky_re_lu_58[0][0]
conv2d_67 (Conv2D)      (None, 26, 26, 18)   9234        leaky_re_lu_65[0][0]
conv2d_75 (Conv2D)      (None, 52, 52, 18)   4626        leaky_re_lu_72[0][0]
```

## 3.  样本数量

样本洗牌，将数据集拆分为10份，训练9份，验证1份，比较简单。

实现：

```python
val_split = 0.1       # 训练和验证的比例 with open(annotation_path) as f:
lines = f.readlines()
np.random.seed(47)
np.random.shuffle(lines)
np.random.seed(None)
num_val = int(len(lines) * val_split)   # 验证集数量
num_train = len(lines) - num_val        # 训练集数量
```


## 4.  第1阶段训练

第 1 阶段，冻结部分网络，只训练底层权重：

- 优化器使用常见的 Adam；
- 损失函数，直接使用模型的输出 y_pred，忽略真值 y_true；

即：

```
model.compile(optimizer=Adam(lr=1e-3), loss={
    # 使用定制的 yolo_loss Lambda层
    'yolo_loss': lambda y_true, y_pred: y_pred})  # 损失函数
```

<span style="color:red;">上面的</span>

其中，关于损失函数 yolo_loss，以及 y_true 和 y_pred：<span style="color:red;">没有很理解</span>

- 把 y_true 当成输入，作为模型的多输入，把 loss 封装为层，作为输出；
- 在模型中，最终输出的 y_pred 就是 loss；
- 在编译时，将 loss 设置为 y_pred 即可，无视 y_true；
- 在训练时，随意添加一个符合结构的 y_true 即可。

Python 的 Lambda 表达式：

```
f = lambda y_true, y_pred: y_pred
print(f(1, 2))  # 输出2
```

模型 fit 数据，使用数据生成包装器，按批次生成训练和验证数据。最终，模型 model 存储权重。

实现如下：

```python
batch_size = 32  # batch
model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
                    steps_per_epoch=max(1, num_train // batch_size),
                    validation_data=data_generator_wrapper(
                        lines[num_train:], batch_size, input_shape, anchors, num_classes),
                    validation_steps=max(1, num_val // batch_size),
                    epochs=50,
                    initial_epoch=0,
                    callbacks=[logging, checkpoint])
# 存储最终的权重，再训练过程中，也通过回调存储
model.save_weights(log_dir + 'trained_weights_stage_1.h5')
```

同时，在训练过程中，也会不断保存，epoch 完成的模型权重，设置参数为：

- 只存储权重（save_weights_only）
- 只存储最优结果（save_best_only）
- 每隔 3 个 epoch 存储一次（period）

即：

```python
checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',
                             monitor='val_loss',
                             save_weights_only=True,
                             save_best_only=True,
                             period=3)  # 只存储 weights 权重
```


## 5.  第2阶段训练

第 2 阶段，使用第 1 阶段已训练完成的网络权重，继续训练：

- 将全部的权重都设置为可训练，而在第 1 阶段中，则是冻结部分权重；
- 优化器，仍是 Adam，只是学习率有所下降，从 1e-3 减少至 1e-4；
- 损失函数，仍是只使用 y_pred，忽略 y_true。



实现：

```python
for i in range(len(model.layers)):
    model.layers[i].trainable = True

model.compile(optimizer=Adam(lr=1e-4),
              loss={'yolo_loss': lambda y_true, y_pred: y_pred})
```

第 2 阶段的模型 fit 数据，与第 1 阶段类似，从第 50 个 epoch 开始，一直训练到第 100 个 epoch ，当触发条件时，则提前终止。额外增加了两个回调 reduce_lr 和 early_stopping ，用于控制训练提取终止的时机：

- reduce_lr：当评价指标不在提升时，减少学习率，每次减少 10%，当验证损失值，持续 3 次未减少时，则终止训练。
- early_stopping：当验证集损失值，连续增加小于 0 时，持续 10 个 epoch，则终止训练。

实现：

```python
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)  # 当评价指标不在提升时，减少学习率
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)  # 验证集准确率，下降前终止
batch_size = 32
model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),
                    steps_per_epoch=max(1, num_train // batch_size),
                    validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors,
                                                           num_classes),
                    validation_steps=max(1, num_val // batch_size),
                    epochs=100,
                    initial_epoch=50,
                    callbacks=[logging, checkpoint, reduce_lr, early_stopping])
model.save_weights(log_dir + 'trained_weights_final.h5')
```



至此，在第 2 阶段训练完成之后，输出的网络权重，就是最终的模型权重。

------

## 补充1.  K-Means

K-Means 算法是聚类算法，将一组数据划分为多个组，每组都含有一个中心。

> 在YOLOv3中，获取数据集的全部 anchor box，通过 K-Means 算法，将这些边界框的宽高，聚类为 9 类，获取 9 个聚类中心，面积从小到大排列，作为 9 个 anchor box。

模拟 K-Means 算法：

- 创建测试点，X 是数据，y 是标签，如 X:(300,2), y:(300,)；
- 将数据聚类为 9 类；
- 输入数据 X，训练；
- 预测 X 的类别，为 y_kmeans；
- 使用 scatter 绘制散点图，颜色范围 viridis；
- 获取聚类中心 cluster_centers_，以黑色点表示；



源码：

```python
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()  # for plot styling
from sklearn.cluster import KMeans
from sklearn.datasets.samples_generator import make_blobs


def test_of_k_means():
    # 创建测试点，X是数据，y是标签，X:(300,2), y:(300,)
    X, y_true = make_blobs(n_samples=300, centers=9, cluster_std=0.60, random_state=0)
    kmeans = KMeans(n_clusters=9)  # 将数据聚类
    kmeans.fit(X)  # 数据X
    y_kmeans = kmeans.predict(X)  # 预测

    # 颜色范围viridis: https://matplotlib.org/examples/color/colormaps_reference.html
    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=20, cmap='viridis')  # c是颜色，s是大小

    centers = kmeans.cluster_centers_  # 聚类的中心
    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=40, alpha=0.5)  # 中心点为黑色

    plt.show()  # 展示


if __name__ == '__main__':
    test_of_k_means()
```



输出：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/MVibsicfR1qiaQITUTtU53eicKqEJte5JYQBibvRDn35De2TeONGg9P3zSYCI2XcZgnocDzsxrBzpOx6Ua8Aq0D9RPw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)


## 补充2.  EarlyStopping

EarlyStopping 是 Callback 的子类，Callback 用于指定在每个阶段开始和结束时，执行的操作。在 Callback 中，有已经实现的简单子类，如 acc、val、loss 和 val_loss 等，还有复杂子类，如ModelCheckpoint和TensorBoard等。



Callback的回调接口，如下：

```
def on_epoch_begin(self, epoch, logs=None):
def on_epoch_end(self, epoch, logs=None):
def on_batch_begin(self, batch, logs=None):
def on_batch_end(self, batch, logs=None):
def on_train_begin(self, logs=None):
def on_train_end(self, logs=None):
```

EarlyStopping 是提前停止训练的 Callback 子类。具体地，当训练或验证集中的 loss 不再减小，即减小的程度小于某个阈值，则会停止训练。这样做，可以提高调参效率，避免浪费资源。

在 model 的 fit 数据时，以列表设置 callbacks 回调，支持设置多个 Callback，如：

```python
callbacks=[logging, checkpoint, reduce_lr, early_stopping]
```

EarlyStopping的参数：


- monitor：监控数据的类型，支持acc、val_acc、loss、val_loss等；
- min_delta：停止阈值，与mode参数配合，支持增加或下降；
- mode：min是最少，max是最多，auto是自动，与min_delta配合；
- patience：达到阈值之后，能够容忍的epoch数，避免停止在抖动中；
- verbose：日志的繁杂程度，值越大，输出的信息越多。



min_delta和patience需要相互配合，避免模型停止在抖动的过程中。min_delta降低，patience减少；而min_delta增加，则patience增加。



例如：

```
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)
```

------

OK, that's all! Enjoy it!




# 相关资料

- [探索 YOLO v3 实现细节 - 第1篇 训练](https://mp.weixin.qq.com/s/T9LshbXoervdJDBuP564dQ)
- 本文的GitHub源码：https://github.com/SpikeKing/keras-yolo3-detection
