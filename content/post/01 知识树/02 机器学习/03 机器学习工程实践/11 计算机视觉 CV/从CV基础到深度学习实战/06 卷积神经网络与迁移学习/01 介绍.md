---
title: 01 介绍
toc: true
date: 2018-08-18 16:38:00
---



## 需要补充的

- 还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结更好些。


论文资源：

- RCNN 链接：https://pan.baidu.com/s/1LrJ-uQRfjb41I1btvGbecA 密码：ks3g
- Fast-RCNN 链接：https://pan.baidu.com/s/1XWm6emUWAoBPK8oE_kY2lw 密码：vxt4
- Faster-RCNN 链接：https://pan.baidu.com/s/1e2i6SSX34pEwG6B52gGsKg 密码：gey3
- R-FCN  链接：https://pan.baidu.com/s/1fXvKITAoFPfE1prBolai8Q 密码：45y3

主要讲卷积神经网络和一些迁移学习的内容

- 卷积神经网络快速回顾
    1.层级结构 2.数据处理 3.训练算法 4.优缺点
- 典型CNN
    1.AlexNet 2.GoogLeNet 3.VGG Net 4.ResNet
- 物体定位
    1.回归的思路
- 物体检测
    1.早期做法 2.RCNN/Fast-RCNN/Faster_RCNN 3.R-FCN
- 文艺绘画与Neural Style
    1.风格描述 2.主体对调与损失最小化


从物体定位到物体检测到Neural Style


我们先回顾一下卷积神经网络

卷积神经网络层级结构

不同层次结构有不同形式(运算)与功能

![mark](http://images.iterate.site/blog/image/180811/lJ0GaGmbCd.png?imageslim)


这是一个非常典型的卷积神经网络，现在的有些变更，但是差不多。


主要是以下层次

- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层 / ReLU layer  activation layer
- 池化层 / Pooling layer 有时候会说叫下采样层。
- 全连接层 / FC layer
- Batch Normalization层(可能有)

FC 可以在一定程度上完成小部分的信息还原。因为前面做了大量的下采样，大量的参数共享，捕捉到了大的图片信息，比较detal 的信息可能在这个途中有所损失，因此接一个 全连接层可以对信息有一定程度的还原。<span style="color:red;">真的是这样吗？为什么可以有一定程度的还原？为什么detail 的信息会损失？</span>

这个 Batch Normalization 是 15年的时候 google 的一篇paper，现在看各种各样的开源的package，包括 caffee tensorflow，等现在都有这个 Batch Normalization 。

我们分别过一下：

### 数据输入层/ Input layer

有3种常见的图像数据处理方式

- 去均值
    - 把输入数据各个维度都中心化到0
- 归一化
    - 幅度归一化到同样的范围
- PCA/白化
    - 用PCA降维
    - 白化是对数据每个特征轴上的幅度归一化

基本上在我们的 image classification 或 object detection 里面，我们对数据做得处理只会有一个去均值，我会把训练集上的图片的均值求出来，然后做训练的时候，把训练集上减去这个均值，预测的时候，也都要减去这个训练集上的均值。

均值的求法有两种，

- 看早期的 AlexNet，它会求出一幅图 在 scale 为同一幅度比如 225*225*3 之后，它把整个训练集的 100万张图片的225*225*3 的矩阵求和，然后除以100万，拿到的均值矩阵是 225*225*3 的维度的。
- VGG 是14年在ImageNet上提出来的，在 transferLearning 上非常好用，它只求了三个颜色通道的均值。所以，相当于 100万*255*255 个像素的平均 。所以这是不同的求均值的方法，任何一种都OK。<span style="color:red;">有什么优劣的区别吗？</span>

所以一般情况下只会做去均值。下面的 归一化和PCA、白化其实只是在其他的场景中可能会用到，在图像中基本不会使用，因为 RGB本身就在同样的范围内，因为不用做 scaling 归一化。



### 卷积计算层/ CONV layer

卷积层的目的是为了抽取特征。不是为了降噪。

<span style="color:red;">对了，对于这个地方我还有个问题，之前不是说这种对应位置的相乘是相关运算吗？只有位置反过来的相乘才是卷积吗？为什么这个地方是对应位置相乘但是是卷积呢？对于卷积和相关性的计算的概念还是要确认下的。</span>

- 局部关联。每个神经元看做一个filter。
- 窗口(receptive field)滑动，filter对局部数据计算
- 涉及概念：
    - 深度/depth
    - 步长/stride demo
    - 填充值/zero-padding

![mark](http://images.iterate.site/blog/image/180811/5CdAme5CBJ.png?imageslim)

cs231n.github.io/assets/conv-demo/index.html

进入到 CONV layer 层的数据已经是经过 resize 到一个尺寸之后再减去均值会后的数据。

卷积层

![mark](http://images.iterate.site/blog/image/180811/d314di11EF.png?imageslim)

最左边的三个矩阵标识的是三个图像 7*7 大小 ，之所以是三个矩阵，是因为有三个颜色通道。卷积层这一层有很多个神经元，每个神经元可以看做一个滤波器，对原来的数据进行卷积处理。

depth 就是下一层有多少个神经元在共同完成这个事情，上面这个图像上是两个神经元，这个 depth 是当前的这个卷积层的神经元的个数，也是你下一层会得到的输出的矩阵的厚度。比如上图，因为你有两个神经元，因此，你得到的输出的矩阵是两个。

stride 扫描的时候是重叠还是不重叠，取决于你的滑动窗口的大小和你的步长。

zero-padding 这个东西很重要，因为你从最左侧滑到最右侧，并不一定会滑到边缘，比如你的inputsize 是 7*7 的，你的 窗口是 3*3 的，strde是3，那么你滑到第二次的时候，右边只有一列了，不能滑了，这一列一次也没参加过计算。怎么办呢？可以看到上面的额图中，实际上图像周围是有一圈0 的，因此，当你办法滑动完整个图像的时候，可以补一圈0或者若干圈0 。使得他刚好能从最左侧滑动到最右侧。之所以补的是0 而不是其他的数据，因为 0*w 还是0 ，不会影响到真正的像素的计算的结果。

这个卷积的运算是什么样的呢？

它是对应的位置相乘然后加起来，然后，把三个通道的结果加起来。然后再加一个偏执项，就得到了每个位置的结果。<span style="color:red;">我一直想知道这个输入的时候，每个神经元与对应的三个通道的输入是怎么连接的？事实上我感觉这个用图来画是不好画的，因为这个连接的时候，是这个窗口的权重在滑动的时候是不变的，不能像全连接一样可以画出一个完整的连接，只能画出对这个虚拟的窗口的连接。所以，用数学来表示反而更好一些。</span>

![mark](http://images.iterate.site/blog/image/180811/0hBaIhI68B.png?imageslim)



- 参数共享机制
- 假设每个神经元连接数据窗的权重是固定的
- 固定每个神经元连接权重，可以看做模板 每个神经元只关注一个特性
- 需要估算的权重个数减少: AlexNet 1亿 => 3.5w
- 一组固定的权重和不同窗口内数据做内积: 卷积

![mark](http://images.iterate.site/blog/image/180811/5CdAme5CBJ.png?imageslim)



怎么去体现每个神经元从自己的角度去观察这幅图？主要就是靠参数共享这个机制 。



### 激励层

ReLU

把卷积层输出结果做非线性映射

![mark](http://images.iterate.site/blog/image/180811/C9BgEBAHib.png?imageslim)

为什么需要这一层呢？比如有阵风吹过来，但是这个对于人体是没有伤害的，因此这个信息可能不需要给大脑说，但是有的人给了你一拳，这个是需要有强烈的反应的。所以，这个信号需要传递给大脑。

那么什么样的信号需要传递给大脑的？什么样的信号不需要？这里就用了一个激励函数来对传过来的信号进行过滤。<span style="color:red;">实际上，我还是有些不清楚，为什么这个对传进来的信号进行这种形式的过滤是符合道理的？</span>

在卷积层后面接上这些激励函数就是对输出做一个限定，到底该不该往后传。

有一些非常常用的接在卷积层后面的激励函数：

- Sigmoid
- Tanh(双曲正切)
- ReLU
- Leaky ReLU
- ELU
- Maxout

这些都有人在用。目前位置用的最多的还是 ReLU 修正线性单元。

激励层(Sigmoid/Tanh/Tanh/Maxout/ELU)

![mark](http://images.iterate.site/blog/image/180811/LFglh6e5E2.png?imageslim)

所有的这些图像都是非线性的。

这个 ReLU 是没有 w 的，但是 Maxout 是有 w 的。

Maxout 是 Leaky ReLU 的推广的一个形式。

在激励层后面接一个池化层。


### 池化层

池化层 / Pooling layer

- 夹在连续的卷积层中间
- 压缩数据和参数的量，减小过拟合

他做得事情就是一个事情：减少数据量。
因为经过卷积之后，我们得到的数据量非常大，数据量多不一定是好事，以为可能带来过拟合的问题。

![mark](http://images.iterate.site/blog/image/180811/9D80aihJ99.png?imageslim)

![mark](http://images.iterate.site/blog/image/180811/0HEl3bD4LB.png?imageslim)

池化层有两种 Pooling 的方式来降维：

- Max pooling 用一个滑动窗口，在 max pooling 的时候每次从窗口对应的位置中取出最大的。
- average pooling 用一个滑动窗口，在 max pooling 的时候每次从窗口对应的位置中取平均值。

使用的场景：

- 传统的图像识别 image classifcation 和物体检测 object detection 里面用的都是max pooling
- average pooling 在图像识别 image classification 用的不多，但是在其他地方，比如 neural style 里面用的是average pooling


注意：老师在这个地方提了下 dropout 不是一层，而是一种正则化的方式。

老师还提到了一种 random pooling ，用的很少，比 average pooling 用的还少。


### 全连接层 FC

全连接层 / FC layer

- 两层之间所有神经元都有权重连接
- 通常全连接层在卷积神经网络尾部

这个一般是接在尾部的，因为如果接在中间，那么运算量太大了。

但是近几年的神经网络越来越不倾向与使用全连接层了，他们会用很小的卷积比如 1*1 的滑动窗口来代替全连接层。这个 在 object detection 里面频繁使用。<span style="color:red;">不知道现在还是不是这样，还是要确认下的。</span>


### 一般CNN结构依次为

- INPUT
- [[CONV -> RELU]*N -> POOL?]*M
- [FC -> RELU]*K
- FC

注意：[CONV -> RELU]*N -> POOL? 是若干个 卷积层+激励层 然后才接一个 池化层。<span style="color:red;">这个我之前还真的没注意。我一直以为是卷积层+激励层+池化层。</span>

而且全连接层后面可能还会接 RELU ，当然最后还是会以全连接层结束掉。

![mark](http://images.iterate.site/blog/image/180811/bbe8e38H55.png?imageslim)

### 总结一下：

![mark](http://images.iterate.site/blog/image/180811/bEhm478l4L.png?imageslim)

这个在 FC 之前是要把 pooling 过来的东西拉平，


## 对卷积层的理解


![mark](http://images.iterate.site/blog/image/180811/2HE44EBH5A.png?imageslim)

第一个卷积层：

![mark](http://images.iterate.site/blog/image/180811/jBLCE6k2Fg.png?imageslim)

可见，可解释性不是那么的强，他做得事情是对的，但是人不明白他为什么要这么做。它的物理含义并不是那么的明确。

到后面的卷积层，里面的内容拿出来看的话人根本没有办法理解。


- 优点
    - 共享卷积核，对高维数据处理无压力
    - 无需手动选取特征，训练好权重，即得特征
    - 分类效果好
- 缺点
    - 需要调参，需要大样本量，训练最好要GPU
    - 物理含义不明确





## 相关资料

- 七月在线 opencv计算机视觉
