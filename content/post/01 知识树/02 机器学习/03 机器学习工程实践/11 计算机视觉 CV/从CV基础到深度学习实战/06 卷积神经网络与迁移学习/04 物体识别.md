---
title: 04 物体识别
toc: true
date: 2018-08-18 16:38:21
---
# 物体识别



![mark](http://images.iterate.site/blog/image/180812/3CIgk1bFLA.png?imageslim)


![mark](http://images.iterate.site/blog/image/180812/0I1Ag26555.png?imageslim)

物体识别很麻烦，因为物体识别有多个主体，但是你实际上并不知道这个图里有多少个主体。

所以你用刚才的 regression 方法是处理不了的，因为对每个物体而言是有4个值，但是由于你不知道有多少个主体，所以你也不知道到底需要拿多少个值。

所以，物体识别的思路是这样的：

我有一个滑动框，我在滑动的时候，去判定这个位置是什么。这个是一个比较粗暴的思路。

这个方式会选取很多的框，而且框的大小你并不知道。这样每次都要判断那么计算量非常大，感觉不太合理。

所以，有人提出了边缘策略 region proposal：


![mark](http://images.iterate.site/blog/image/180812/jFgLdgGK20.png?imageslim)

边缘策略 region proposal 之前是一个研究的热点，他是想找出来到底哪些可能是候选框：我先用别的方法来找出一些可能是一个东西的框。然后我再对框内的东西进行分类识别。

这个方法就是 region proposal

<span style="color:red;">我还是没清楚 region proposal 和 select search 区分点在哪里？还是说他们就是相同的东西？</span>

OK，下面我们看下选择性搜索：

选择性搜索

![mark](http://images.iterate.site/blog/image/180812/13kKm86K5K.png?imageslim)


上面这个 13 年的 paper 在很长一段时间会用到，他做得是一个自底向上的一个merge 的过程。

比如说，你可以用 knn 的方式对周边的点进行扩散，然后对这个再进行扩张，然后再做扩张，由于扩张的程度不同，你可以拿到不同粒度的图像分割区域。

那么怎么从上面这排到下面的图像呢？我们可以找一个框把上面的一个个色块框出来。就找到了一个个框。

所以有一个算法叫做 select search，他做得方法就是在图上找到可能的框。

实际上图框的选择还是有别的方式的：

![mark](http://images.iterate.site/blog/image/180812/lJa057FFl9.png?imageslim)


上面这个paper 就总结了各种图框的选择方法，可以看到 EdgeBoxes 和 SelectiveSearch 是比较好的方法。

<span style="color:red;">感觉这部分老师讲的还是太粗略了 需要仔细补充一下</span>





## R-CNN

于是就诞生了下面的三部曲：

R-CNN
Fast-RCNN
Faster-RCNN

### RCNN

![mark](http://images.iterate.site/blog/image/180812/gJ0iigj5bE.png?imageslim)

Girschick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014

这个老师没有讲。

RCNN 主要做的事情是这样的：

有一张图片，我从里面选出了一些区域。比如说我先通过 select search 拿到一些框。框的个数非常多，一般一幅图上有 2000个框。<span style="color:red;">这么多吗？老师还特地提了下，说没有我们想象的那么少。</span>

每一个框拿出来都要过一个CNN，所以每一层都会有输出，我把某一层的信息拿出来，这个就可以看做图像的特征，然后根据这个特征去放到分类器里面，这里用的是 SVM 它会判断这个ROI 里面是背景还是物体，然后这个 Bbox regression 会告诉你这个框是大了还是小了，要怎么调整一下才贴近这个真实的小猫的位置。<span style="color:red;">没怎么懂</span>


有同学问 ConvNet 的权重怎么学到的？R-CNN 不是一个端到端的，他的组件都是独立开的，这个卷积神经网络可以直接找 AlexNet 或者 VGG，因为我用这个神经网络就是来抽取特征用的。

Bbox regression 也是单独训练的。他做得事情就是 我的框到底大了还是小了。

<span style="color:red;">具体的细节还是要弄明白的。</span>


R-CNN 是非常慢的，两千个ROI，每一个框中的数据都要经过一个卷积神经网络。是很耗时的，因为我只是想在图上找到一个物体而已。

然后就有了 Fast-RCNN

### Fast-RCNN

有两个改进：

- 一个是把候选框的运算极大的缩减了
- 一个是他不用 SVM 来进行分类了，直接在后面接了FC来做 regression 和classification。

![mark](http://images.iterate.site/blog/image/180812/1H19C4mIGJ.png?imageslim)

第一个优化，是他把整幅图过了卷积神经网络之后找到原图上的位置和我现在的位置的对应关系，一幅图单个过了卷积神经网络拿到了整个 feature  map。我能不能把候选框映射到特征图上把对应的位置抠出来？

第二个优化是，在后面直接接全连接层去完成我的分类和回归。

![mark](http://images.iterate.site/blog/image/180812/mCkmLB5cjD.png?imageslim)

有的同学会问，我怎么知道原始的图片上的框映射到我的 feature map 上？

![mark](http://images.iterate.site/blog/image/180812/Ie9fC9flH5.png?imageslim)

实际上根据卷积的原理和 pooling的方式，我们可以知道 某个像素实际上是可以对应到后面的输出上去的。

但是由于原始的 ROI 的大小可能是不同的，因此，我是有一层 ROI Pooling 的。它做的事情是把不同大小的 feature map 区域 下采样成相同的大小，在这个基础上再接固定的大小的神经元。

![mark](http://images.iterate.site/blog/image/180812/L6A7FD5i4A.png?imageslim)

![mark](http://images.iterate.site/blog/image/180812/8lLIH7H1L5.png?imageslim)


### Faster-RCNN

大神觉得这个 Fast-RCNN 还不够快，后面部分已经很快了，慢的地方就在于开始的找候选图框。

![mark](http://images.iterate.site/blog/image/180812/ckC2FgL0I6.png?imageslim)

他就在想有没有什么办法让神经网络来完成这个功能。

所以，他就像，干脆找个网络来完成这个，注意，这个不是 YOLO，YOLO 是和 Faster-RCNN 同时提出来的，但是被拒了。

我们这里不说 YOLO ，因为 Faster-RCNN 和 Faster-RCNN 是有启发式的，而且，YOLO比较奇怪，是自己做的dl 框架，不是 用的 Caffe 或 Tensorflow。

他干脆把选择区域的这部分也用一个神经网络来完成，就是 Region Proposal Network 这样就很快了，就可以用一整套的端到端的系统来完成，也不用找额外的select search 什么的。


![mark](http://images.iterate.site/blog/image/180812/ghKKBLl98b.png?imageslim)

他依然是从一个大图上找到产出一些 feature map。然后找了一些中心点，在每个中心点找到了一些不同大小的候选框

<span style="color:red;">中心点怎么选定的？候选框怎么选定的？</span>

![mark](http://images.iterate.site/blog/image/180812/eGm9m7cHJa.png?imageslim)

注意，这个 anchors 是在feature map上，不在原图上。

对这些 anchors 分别判定这里面到底有没有东西。

注意，这个 anchors 是在feature map上定义的，但是由于是候选框，要映射到原图上的，因此在代码里面实现的时候是有些 tricks的。


老师再次推荐看原文。



### Region-based Fully Convolutional Networks

RFCNw

最新方法：Region-based Fully Convolutional Networks

Jifeng Dai,etc “R-FCN: Object Detection via Region-based Fully Convolutional Networks ”, 2016

![mark](http://images.iterate.site/blog/image/180812/1BHgfm47Gf.png?imageslim)


每个颜⾊代表不同的位置选择区域。
The	bank	of	kxk		score	maps	correspond	to	a	kxk spatial	grid	describing	relative	positions.

![mark](http://images.iterate.site/blog/image/180812/J53G6EC5E2.png?imageslim)


![mark](http://images.iterate.site/blog/image/180812/gfiDgdhKjE.png?imageslim)

老师讲了下重点：

他们做这个的时候，发现一个问题，使用 Faster-RCNN 最后接的都是全连接层，但是这个 全连接层 在分类的时候和回归时候。

在分类的时候是这样的，比如一只猫，不管它是大是小，还是怎么姿态，我都想要识别出它是一只猫。所以它是一个位置不敏感的。所以，这就有一个问题了：在卷积神经网络中，如果我想把分类做得准确，那么肯定会抛弃位置相关的一些信息。但是 object detection 是希望我是位置敏感的，就是在不同的地方有不同的score。

所以，上面这个 R-FCN 的 paper 就对Faster-RCNN 的网络结构做了一些变更。

他在得到 feature maps 之后接了一个 k^2(C+1)-d 维度的 卷积层。他把整个 feature map 分成了k*k 个区域，如果我是识别的是两类，那么 C就是 2，2+1 意味着我有一个背景，所以是3类。所以，我就有了 k^2(C+1) 个 position-sensitive score maps， 就是你在这个地方之后有 C+1 个类，对于每个类都会对这么多的区域进行分类得到一个得分，但是这些得分是位置相关的，然后他会把所有位置敏感的 score 做一个 vote，得到最后的 C+1 个 score 去接一个 softmax。

老师也推荐了看这个paper ，这个paper 很好，因为这个对之前的 RCNN Fast-RCNN  Faster-RCNN 都做了一个总结可以看得到是如何从那样一个网络优化到这样一个网络的。

现在这个 RFCN 是到目前为止 真的能在速度和质量上都超过 Fast-RCNN 的一个方法。

之前出过 YOLO，SSD，都没有对 Fast-RCNN 有太大优势。

老师再次推荐看paper。





## 相关资料

- 七月在线 opencv计算机视觉
