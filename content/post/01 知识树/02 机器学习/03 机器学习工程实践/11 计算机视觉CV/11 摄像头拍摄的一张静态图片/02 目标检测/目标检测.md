---
title: 目标检测
toc: true
date: 2018-09-02
---

# 需要补充的

- 每一种方法都要拆出来单独明确，包括理论和相关的Tensorflow、Caffe、MXNet 实现。
- 论文也要整理进来
- SSD 是 16 年的了，不知道现在的发展是什么样的了？
- 几种方法中的原理上的问题要完全弄明白。与之前的课程内容合并到一起。




# 目标检测



本章针对 R-CNN 系和 YOLO/SSD 系这两类算法，简要介绍了基于深度学习的目标检测算法的发展史，并给出了基于 MXNet 的 SSD 检测算法实例，以及分析了结果的可视化。



本章简要介绍基于卷积神经网络的目标检测(Object Detection)算法，并运行一个基于SSD (Single Shot Detection)算法的目标检测例子。

## 目标检测算法简介

本节介绍常见的目标检测算法背后的基本思想，并简要回顾基于深度学习的目标检测算法发展历史。

### 滑窗法

滑窗(Sliding Window)法的思路极其简单，首先需要一个已经训练好的分类器，然后把图像按照一定间隔和不同的大小分成一个个窗口，在这些窗口上执行分类器，如果得到较高的分类分数，就认为是检测到了物体。

把每个窗口都用分类器执行一遍之后，再对得到的分数做一些后处理如非极大值抑制(Non-Maximum Suppression, NMS)等，最后就得到了物体类别和对应区域，其方法示意图如图11-1所示。<span style="color:red;">什么是非极大值抑制？之前学习过这个，但是又忘记了。明确下。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180901/84Eh3Cdb3C.png?imageslim)

滑窗法非常简单，但是效率极其低下，尤其是还要考虑物体长宽比。如果是执行比较耗时的分类器，用滑窗法就不太现实。<span style="color:red;">为什么还要考虑物体长宽比。滑窗的大小和长宽比是怎么确定的？</span>

常见的都是一些小型分类网络和滑窗法结合的应用，如 Dalle Molle 人工智能研究所（IDSIA）的高级研究员 Dan Claudiu CireSan 就做过用卷积神经网络结合滑窗法，检测胸腔切片图像中的有丝分裂用于辅助癌症诊断，论文 《Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks》 于 2013 年发表在医 学影像处理领域最顶级的刊物 MICCAI 上。<span style="color:red;">嗯，还是有人深入的做过的。</span>

### PASCAL VOC、mAP 和 IOU 简介

在继续介绍目标检测算法演进历史之前，有必要先介绍一下最常用的目标检测算法数据集，以及评估一个目标检测算法的最常见指标。

PASCAL VOC，全称是 Pattern Analysis Statistical Modelling and Computational Learning, Visual Object Classes，是一套用于评估图像分类、检测、分割和人体姿势动作等的数据集，当然被用到最多的，还是物体检测。PASCAL VOC包含 4 大类共 20 个细分类别，分别是人、动物（鸟、猫、牛、狗、马、羊）、交通工具（飞机、自行车、船、大巴、 轿车、摩托车、火车）、室内（瓶子、椅子、餐桌、盆栽、沙发、电视/显示器）。

直观上讲，评价一个检测算法的时候，主要看两个标准，即：<span style="color:red;">之前上的课好像没有讲这两个标准的，还是说讲了但是我没作为重点？</span>

- 是否正确预测了框内的物体类别；
- 预测的框和人工标注框的重合程度。

这两个标准的量化指标分别是：

- mAP （mean Average Precision）
- IOU （Intersection Over Union）

mAP 中文翻译过来叫做平均精度均值，其中的概念在第10章已经讲解过。mAP 是把每个类别的 AP 都单独“拎”出来，然后计算所有类别的平均值，代表着对检测到的目标平均精度的一个综合评价。

IOU 用来衡量预测的物体框和真实框的重合程度，计算方法如图11-2所示。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180901/7lC51J92ia.png?imageslim)

图11-2中实线是人工标注的框，虚线是模型预测的框。重合度的计算方法如图11-2 最右边的灰色区域所示，是用两个框重合的面积，除以两个框并集所占的面积，所以叫做交并比。评价一个算法的时候，一种常见的方法是先设定一个 IOU 的阈值，只要算法找到的框的 IOU 大于这个阈值，就是一个有效的检测，把结果拿来计算作为最终的评价指标。

在PASCAL VOC中，这个阈值设定为0.5。需要提示的是，物体检测的评价方法仍在不断演化中，如虽然简单易懂，不过很多时候未必合理，视觉上重合度差不多的两个框，实际应用中很可能因为分辨率不同得到差异很大的值。<span style="color:red;">嗯，也是，分辨率会导致计算的时候差别挺大的。现在的评价方法有什么吗？</span>

### Selective Search 和 R-CNN 简介

滑窗法相当于对一张图像上的子区域进行类似穷举式的搜索，一般情况下这种低效率搜索到的区域里大部分都是没有任何物体的。所以一个很自然的想法就是只对图像中最有可能包含物体的区域进行搜索，进而提升物体检测的效率。在这种思路的方法中，最为熟知的是 Selective Search。<span style="color:red;">是的。这个方法之前课上重点讲解了。</span>

Selective Search 的思路是，可能存在物体的区域都应该是有某种相似性的或连续的区域。

针对这两个特点，Selective Search采用的是超像素合并的思路：

- 首先用分割算法在图像上产生很多的小区域，这些区域就是最基础的子区域，或者可以看作是超像素。
- 然后根据这些区域之间的相似性进行区域合并，成为大一点的区域。衡量相似性的标准可以是颜色、纹理和大小等。<span style="color:red;">纹理和大小怎么作为相似性的衡量标准的？</span>
- 不断迭加这种小区域合并为大区域的过程，最后整张图会合并成为一个区域。
- 这个过程中，给每个区域做一个外切的矩形，就得到了许许多多的可能是物体的区域方框。

<span style="color:red;">的确是个非常好的，非常 nice 的方法。</span>


算法执行过程的示意图如图11-3所示。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180901/1Bb4HG8lcl.png?imageslim)

可以看到 Selective Search 和滑窗法相比：

- 第一个优点就是高效，因为不再是漫无目的的穷举式搜索。
- 另外，在Selective Search中，一开始的区域是小区域，合并过程中不断产生大区域，所以天然能够包含各种大小不同的疑似物体框。
- 另外，计算相似度采用了多样的指标，提升了找到物体的可靠性。<span style="color:red;">什么多样的指标？怎么在计算相似度的过程中使用不同指标的？</span>

当然，这个算法的过程也不能太慢，否则和滑窗法相比的优势 就体现不出来了。算法的具体细节这里就不展开了，有兴趣的读者可以参考作者 J.R.R. Uijlings 的论文 《Selective Search for Object Recognition》。<span style="color:red;">嗯，一定好好好读一下，总结到这里。</span>

有了 Selective Search 高效地寻找到可能包含物体的方框（实际中常进行一定像素的外扩包含一定背景），那么接下来只要接个 CNN 提取特征，然后做个分类不就相当于检测吗？<span style="color:red;">什么叫一定像素的外扩？实际中进行了什么优化处理吗？</span>

OK，这正是 Ross B. Girshick 的基于深度学习做物体检测的开山之作 R-CNN （Region-based Convolutional Neural Networks），当然直接用 Selective Search 选出的框未必精确，所以还加入了一些改进，如和物体标注框的位置的回归来修正 Selective Search 提出的原始物体框。R-CNN 就像 Alexnet 一样，让物体检测的指标跃上了新台阶（PASCAL VOC，mAP: 40.1% -> 53.3%)。<span style="color:red;">怎么回归来修正 Selective Search 提出的原始的物体框的？</span>

R-CNN 的更多细节可以参考发表在 2014 年 CVPR 上的论文《Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation》。<span style="color:red;">嗯，这个也需要总结下。</span>


其实无论是滑窗还是 Selective Search，这种找出可能包含物体的区域的方法，统称为 Region Proposal。<span style="color:blue;">嗯，之前还有点迷糊，原来 Region Proposal 包括了滑窗和 Selective Search 。</span>


### SPP、ROI Pooling 和 Fast R-CNN 简介

R-CNN 虽然比起滑窗法已经快了很多，但可用性还是很差，因为一个稍微“靠谱”的识别任务需要用 Selective Search 提出上千个框 (R-CNN中是2000个)。这上千个图像区域都需要单独过一遍卷积神经网络进行前向计算，速度自然快不了。<span style="color:red;">嗯，是呀。</span>

#### SPP 和 ROI Pooling 简介

在第 4 章中已经讲过了卷积的同变性质(equivariance)。第 10 章的最后通过每类别对应物体的激活响应图的例子，定性理解了物体通过卷积网络之后，会在语义层的响应图上的对应位置有特别的响应。<span style="color:red;">之前这两章没有看，看完后，在这里补充一下，什么是同变性质？会出现什么特别的响应？</span>

所以一个思路就是，对整张图片执行一次卷积神经网络的前向计算，到了最后一层的激活响应图的时候，通过某种方式把目标物体所在区域部分的响应图拿出来作为特征给分类器。<span style="color:red;">厉害呀，如果真的有通便性质和特别的响应的话，的确可以这么做，方便了很多。</span>

这样做对画面内所有可能物体的检测就可以共享一次卷积神经网络的前向计算，大大节省了时间。

第一个在物体检测中实现这个思路的就是当时还在 MSRA 的 Kaiming He 提出的 SPP，全称为空间金字塔池化(Spatial Pyramid Pooling)， 示意图如图11-4所示。<span style="color:red;">想更多了解下 SPP。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180901/B473HIaC91.png?imageslim)

如图11-4所示，假设输入图片中框住小马和摄影师的两个框是 Selective Search 选出来的框，那么经过了(没有全连接层的)卷积神经网络，到了最后一层输出的 n 个通道的响应图时，原图像上的两个框也会对应两个区域。这样的区域称为感兴趣区域(Region Of Interest, ROI)。

一般常用的分类器，无论是 SVM 还是称浅层神经网络，都需要输入固定的维度。所以如果可以有一种方式把 ROI 的信息都转化成固定维度的向量，那么就能把每个 ROI 都给分类器去计算获得结果，并且在进入分类器之前，只需要运行一次卷积神经 网络的前向计算，所有的 ROI 都共享同样的响应图。<span style="color:red;">嗯，是的，ROI 还需要转化成相同给的维度才能进行分类。或回归。</span>

SPP 就是这样一种方法，对于每一个 ROI，如图 11-4 中所示，SPP 分层将指定区域划分为不同数目，图中分为 3 个层次，最底层划分为 4x4=16 个子区域，中间层是 2x2=4 个子区域，最顶层则直接整个区域进行池化，对于每个通道，每个 ROI 变成了一个21维的向量，因为有 n 个通道，所以每个 ROI 都生成了一个 21n 维的向量。

因为越是底层划分的数目越多，SPP 是通过像金字塔一样的结构来获得感兴趣区域不同尺度的信息，所以叫 空间金字塔池化。借助 SPP，不仅实现了对 ROI 的分类，而且对于整张图像只需要进行一次卷积神经网络的前向计算，大幅降低了算法执行的时间。

另外需要提的是，这里只讲了将 SPP 用于检测的思路，其实 SPP 把任意大小的向量转化为固定大小的向量的方法还有另一个意义，就是让输入神经网络的图像大小不再固定，在执行分类任务的时候，这种做法的优点是不需要再对图像进行裁剪或者缩放。<span style="color:red;">真牛逼啊，现在这种SPP 的方式应用的广泛吗？现在处理图像是缩放成统一的维度还是用这种 SPP 的方式？处理高清图片比如航拍地图的时候这种 SPP 有用吗？确认下。</span>

SPP 的论文 《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》 发表在 2014 的 ECCV 上，有兴趣的读者可以自行搜索原文参考。

在 SPP 中，包含信息最多的其实就是最底层，所以另一个思路是直接把 ROI 划分为固定的大小，并不分层。如把所有 ROI 区域池化到7x7的大小，再送入分类器，这就是 ROI Pooling 。

<span style="color:red;">对于上面这两种方法还是要再补充一下的。</span>



# FastR-CNN简介

SPP 用于物体检测相比 R-CNN 获得了速度上的巨大提升，但仍然继承了一些 R-CNN 的缺点。最明显的是分阶段训练，不仅麻烦，而且物体框回归训练过程和卷积神经网络的训练过程是割裂的，整个参数优化的过程不是一体的，限制了达到更高精度的可能性。<span style="color:red;">嗯，是的，不过这种割裂的训练限制了达到更高精度的可能吗？</span>

针对 SPP 的缺点，R-CNN 的作者 RossB.Girshick 再度发力，在 SPP 检测的基础上提出了两个主要的改进/变化：

- 第一点是 ROI 提取特征后，把物体框的回归和分类这两个任务的loss融合一起训练，相当于端到端的多任务训练（end-to-end with a multi-task loss）。这让整个训练过程不再是多个步骤分别进行，训练效率也更高；
- 第二点是把SPP换成了 ROI Pooling，这就是 Fast R-CNN。在计算预测的框和标注框的 loss 时，Ross B. Girshick 并没有像在 R-CNN 和 SPP 中那样采用常见的 L2 方法，而是采用了一种叫做 $Smooth_{L1}$ 的 loss 计算方法：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180902/14C8jkcdFj.png?imageslim)

其中，

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180902/kE8GhJA550.png?imageslim)

<span style="color:red;">这个公式还是不是很清楚。</span>

其实就是把 L2 和 L1 拼一块了，其中小的偏差利用 L2 计算，大的偏差利用 L1 计算。 $Smooth_{L1}$ 对偏差很大的值没有那么敏感，好处是提高了 loss 计算的稳定性。

在这种框架下，因为卷积神经网络计算对每张图像只执行了一次，所以重复计算大都在 ROI Pooling 之后，于是 Ross B. Girshick 又提出用 SVD 分解然后忽略次要成分，把全连接层的计算量减小，达到精度损失极其微小的情况下，获得较大幅度的计算速度提高，这也是算是论文中的一个小的优点。<span style="color:red;">怎么进行 SVD 分解然后忽略次要成分的？</span>

Fast R-CNN 的更多细节可以参考 2015 年 ICCV 的论文 《Fast R-CNN》。<span style="color:red;">论文是要从头到尾仔细看下并总结下。</span>

### RPN 和 Faster R-CNN 简介

Fast R-CNN 主要改进的是卷积神经网络开始往后面的计算，这部分的计算速度大幅提升，Selective Search 反倒成为了限制计算效率的瓶颈。<span style="color:red;">嗯。</span>

所以是不是可以考虑用神经网络的 办法取代 Selective Search 呢？

这次 Ross B. Girshick 联合 Kaiming He 一起提出了 Faster R-CNN 。

在第10章末尾激活响应图的例子中了解到，响应图中是可以包含粗略的位置信息的， 所以 Region Proposal 的这一步也完全可以放到最后一层响应图上来做。所以在 Faster R-CNN 中，Region Proposal Networks (RPN) 就被提出来替代 Selective Search。<span style="color:red;">嗯，之前课上讲过 Faster R-CNN，但是没有注意过这个 Region Proposal Networks 就是 RPN。</span>

这样做的一个重要意义是算法的所有步骤都被包含到一个完整的框架中了，实现了端到端的训练。

RPN 首先对基础网络的最后一层卷积响应图，按照执行一次 $n\times n$ 卷积，输出指定通道数(原文中为256，github 代码中为 512) 的响应图。这步相当于用滑窗法对响应图进行特征提取，在论文中 n 的大小是 3，也就是 3x3 的窗口大小。<span style="color:red;">为什么是输出指定通道数的响应图？为什么相当于用滑窗法对响应图进行特征提取？</span>

然后对得到的响应图的每个像素分别进入两个全连接层：

- 一个计算该像素对应位置是否有物体的分数，输出是或否的分数，所以有两个输出；
- 另一个计算物体框的二维坐标和大小，所以有 4 个输出。

其中对于每一个 $n\times n$ 卷积输出的响应图像素，都尝试用中心在该像素位置，不同大小和不同长宽比的窗口作为 anchor box，回归物体框坐标和大小的网络是在 anchor box 基础上做 offset。<span style="color:red;">anchor box 是怎么定的？</span>

所以假设有 k 个 anchor box，则计算是否有物体分数的输出实际有 2k 个，计算物体框坐标和大小的输出实际有 4k 个。因为是对每个像素计算，所以其实 RPN 就是在前面章节中讲过的 NIN，使用 1x1 卷积实现。在论文中采用的是 3 种尺寸和 3 种长宽比，每个像素对应 9 个 anchor box，于是每个像素对应的物体分数有 2x9=18 个，对应的物体框的输出有 4x9=36 个。<span style="color:red;">NIN 是什么？为什么要是 9 个 anchor box ？</span>

<span style="color:red;">对于 RPN 想详细的理解下。</span>

一个 RPN 的示意图如图11-5所示。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180902/62Lh4cfe54.png?imageslim)

图11-5中最左边的响应图是基础的卷积神经网络得到的最后一层卷积响应。经过 256 通道的 3x3 卷积得到了每个位置对应的 256 维特征，然后以一个 1x1 的卷积层用于得到每个位置对应的 k 个 anchor box 是否物体的得分，另一个 1x1 的卷积层用于得到每个位置对应 k 个 anchor box 的位置和大小。

基于 RPN 的物体分数和物体框得到可能的物体框之后，训练时经过 NMS 和分数从大到小排序筛选出有效的物体框，从中随机选取作为一个 batch。然后通过 ROI Pooling 进行分类的同时，会进一步对物体框的位置及大小进行回归，ROI Pooling 之后的这两个任务对应两个loss，再和 RPN 的两个 loss 放一起就实现了端到端的训练。<span style="color:red;">什么是 NMS？</span>

Faster R-CNN无论是训练/测试的速度，还是物体检测精度都超过了 Fast R-CNN，达到了这一系算法的巅峰。<span style="color:red;">这么厉害吗？</span>

这个方法的论文正式版本发表在 2015 的 NIPS 上《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》。


### YOLO 和 SSD 简介

从 R-CNN 到 Faster R-CNN，这一系列的方法始终都是基于 Region Proposal 的。不用 Region Proposal 的方法其实也有很多的研究，其中最有代表性的是 YOLO 和 SSD。<span style="color:red;">我去，不用 Region Proposal 怎么做目标检测？</span>

YOLO 全称 You Only Look Once，取这个名字的意思是源于人眼看东西时，只需要一瞥就能感知出认识的物体，YOLO 也希望能达到这种简单高效的检测。

执行速度快，是 YOLO 提出时最大的特点，达到非常高效的检测，其背后是 YOLO 原理和实现上的简单。

YOLO的基本思想是，把一幅图片划分为一个 $S\times S$ 的格子，以每个格子所在位置和对应内容为基础，来预测：

1. 物体框，包含物体框中心相对格子中心的坐标 $(x,y)$ 和物体框的宽 $w$ 和高 $h$，每个格子预测 $B$ 个物体框。
2. 每个物体框是否有物体的置信度，其中当前这个格子中如果包含物体，则置信度的分数为当前预测的物体框和标注物体框的IOU，否则置信度分数为 0。
3. 每个格子预测一共 $C$ 个类别的概率分数，并且这个分数和物体框是不相关的，只是基于这个格子。

综上所述，每个格子需要输出的信息维度是 $B\times (4+1)+C=Bx5+C$。

在 YOLO 的论文中，B 为 2, C 是 PASCAL VOC 的类别数 20。所以每个格子预测的信息维度是 2x5+20=30。格子数 S 为7,所以最后得到的关于物体预测的信息是一个 $7\times 7\times 3O$ 的张量。实现这个过程的原理示意图如图11-6所示。

一幅图片首先缩放为一个正方形的图片，论文中采用的是 448x448，然后送进一个卷积神经网络，到最后一层卷积响应图的时候，接两层全连接，输出（并Reshape）是 7x7x30，对应前面提到的 7x7x30 的张量。最后从这 7x7x30 张量中提取出来的物体框和类别的预测信息经过 NMS ，就得到了最终的物体检测结果。

和基于 Region Proposal 方法的不同之处在于，YOLO 就是一个单纯的卷积神经网络，把物体检测转化成了个单纯的回归问题，端到端的味道比 R-CNN 系列更加纯正。没有了 Region Proposal 和对每个 ROI 的单独计算， 再加上利用GPU对计算响应图的并行处理，执行效率得到了极大提升。

YOLO 在速度上获得了很大的提升，但是精度上比 RCNN 系还是逊色一些。其中一个原因是基于格子回归物体框的方式也在一定程度上限制了物体框位置和大小的灵活性。另外7x7的格子并不能将画面划分为足够精细的区域，如在一个格子对应的区域内如果同时出现多个小物体就比较麻烦了。YOLO的论文最终发表在2016的CVPR上《You Only Look Once: Unified, Real-Time Object Detection》。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180902/b9d0f2k05J.png?imageslim)


SSD 全称是 Single Shot multibox Detector，算是同时借鉴了 YOLO 和 Faster R-CNN 思想的方法。可以在达到实时的检测速度水平下，仍然保持很高的检测精度。

和 YOLO 相近的地方是，SSD 也会在卷积神经网络的最后阶段，得到 $S\times S$ 的响应图。然后是和 Faster R-CNN 相近的地方，SSD 会基于每个格子的位置借鉴 anchor box 的思想生成默认的物体框。

本节一开始也提到过，相对于Faster R-CNN， SSD 并没有 Region Proposal 对 ROI 分类的两步框架，所以叫做 Single Shot，其实从这个角度来说 YOLO 也是一种 Single Shot 的检测方法。相比 YOLO， SSD 的主要改进是从一个分辨率较大的响应图幵始，逐渐得到分辨率更低的响应图，每个分辨率下的响应图都作为产生物体类别分数和物体框的格子，这样就得到了不同大小感受野对应的局部图像信息。

如图11-7所示为SSD的一个示意图：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180902/f2IGmbK7JK.png?imageslim)

图11-7中的例子是一幅输入图像经过基础网络后，得到了 8x8 的响应图，然后这组响应图的每个像素位置会产生类似 anchor box 那样 A 个默认物体框，其中每个框的大小和位置的修正量对应 4 个数值，每个框内物体所属类别对应 C 个数值，所以用一个通道数为 $(C+4)k$ 的卷积得到预测的框和结果。$8\times 8$ 的响应图进一步卷积可以得到 $4x4$ 的响应图，这个响应图中的每个像素对应更大的感受野，如图11-7右下的两个图所示。对于这个 4x4 的响应图也可以用同样办法得到 $(C+4)k$ 通道的卷积响应作为预测结果，注意不同的响应图上，A 的取值可以不同。在 SSD 中 k 的取值策略也是不同大小和不同长宽比，最常见的配置是如图 11-7 左下角所示，取 4 个默认物体框，或者让长宽比更夸张一些多取两个，即一共6个。

在 SSD 论文中，基于 VGG-16 的基础模型在 300x300 输入分辨率下，得到的 conv5 是 38x38 的响应图，每个像素上取 $k=4$，经过进一步降低采样分别得到 19x19、10x10 和 5x5 的响应图，对这3个响应图取 $k=6$，最后继续降低采样得到 3x3 和 1x1 的响应图，取 $k=4$, 则每个类别一共得到 38x38x4+(19x19+10x10+5x5)x6+(3x3+1x1)x4=8732 个默认物体框。 而 YOLO 默认配置 448x448 的分辨率，最后 7x7 的格子上每个格子默认预测 2 个物体框， 每个类别一共是 7x7x2=98 个物体框。、

SSD-300 比起 YOLO 输入的分辨率更低，但是感受野的精细程度更高，而且默认物体框的数量高出了快两个量级，结果就是执行速度和精度的双双提升。

训练SSD的思路就和其他流行方法一样，两种 loss，一种用来分类，一种用来定位。

不过 SSD 是一个细节非常多的方法，就像 SSD 的作者在 2016 的 ECCV 上讲的那样“The Devil is in the Details”，对 SSD 做了非常详尽的实验，从训练样本的选取，到数据增加、默认框的长宽比策略、输入图像分辨率，甚至是卷积核的类型等都做了不同尝试，这些细 节这里就不一一讲解了，有兴趣的读者可以参考发表在 2016 年 ECCV 的原文《SSD: Single Shot MultiBox Detector》来了解这篇实验完备且细节丰富的文章。<span style="color:red;">嗯，要认真阅读并总结思考下。SSD 之后后来有什么别的进展吗？</span>
