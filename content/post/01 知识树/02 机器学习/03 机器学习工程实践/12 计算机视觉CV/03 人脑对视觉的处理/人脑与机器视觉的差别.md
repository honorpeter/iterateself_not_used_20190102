---
title: 人脑与机器视觉的差别
toc: true
date: 2018-08-18 19:09:59
---




DNN/CNN把cv往前推了一大步，与其说现在cv到了瓶颈期，不如说是DNN/CNN在cv这个领域到了个瓶颈期，接下来等新理论新算法出来，又能把cv往前推进一大截。有趣的是，因为NN在cv领域的成功引起了很多关注，现在NN的台风刮到了各行各业，在别的领域倒是有巨大的发展。

CNN在一定程度上模拟了假设的生物大脑的视觉处理过程，但远没能达到人脑的视觉处理能力，今年看了几本计算神经学入门的书，人脑与CNN比起来，粗略地说有这么几点差异：

1.人脑中有3D和2D两个认识部分，其中2D部分处理平面形状和平面组合，3D部分处理3D物体和空间关系，目前的CNN只是2D平面像素的处理。

2.人脑对物体认知有尺度一致性，是统一到物理尺度上的，不受分辨率、远近、角度影响，人脑有一个3D与2D之间的映射系统，能把肉眼看到的2D的物体图像投射到脑中的3D虚拟空间中尺度一致地进行识别，而CNN没有，CNN只比对2D像素特征，所以cnn需要海量的各个角度的图片，cnn并不是理解了物体的结构，cnn只是统计上能够对物体投影到2D平面上的图像的像素特征进行标签。

3.人脑内有一个raytrace光照模型，能够推测场景的光照方向和角度，这个光照模型是长时间对光照现象的知识积累出来的，所以人脑可以理解折射、反射、通过阴影推测物体相互关系。人脑还积累着一大堆各种材质BRDF的知识。

4.人脑有一个3D与2D之间的小孔成像模型，人脑能够通过尺度叛变距离，通过距离判别尺度，并在多个物体组成的场景中通过上下文推理从而理解整个场景的空间关系。

现阶段的CNN只比对2D像素特征，这只是人脑的视觉处理过程中很小的一部分，缺失了很多，所以一定不会好。而对于其它的关键部分，例如raytrace model ，一致性 model，3D-2D转化model这些如何用NN来做？数据如何编码？NN如何设计？如何训练？如何与其它模块串成pipeline？我没看到有影响力的论文出来，甚至都没听到有人在做。

我觉得现在很多做NN的都在钻牛角尖，换来换去各种网络，没有人从计算神经学角度看问题，在这些问题解决之前，纯靠微调model、堆网络规模和训练数据来实现对场景的理解我觉得是不可能的。一年前听andrew ng的演讲，他说他们解决NN效果不好的方法就是上更深的model和更多的数据，太简单粗暴了。

而计算神经学的那帮家伙，只会忙着用fMRI标记大脑活动。

我也是一脸懵逼。

update：

书单：

[Seeing: The Computational Approach to Biological Vision (MIT Press) (9780262514279): John P. Frisby, James V. Stone: Books](https://link.zhihu.com/?target=https%3A//www.amazon.com/Seeing-Computational-Approach-Biological-Vision/dp/0262514273)

[Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (MIT Press): David Marr, Tomaso A. Poggio, Shimon Ullman: 9780262514620: Amazon.com: Books](https://link.zhihu.com/?target=https%3A//www.amazon.com/Vision-Computational-Investigation-Representation-Information/dp/0262514621/ref%3Dsr_1_1%3Fie%3DUTF8%26qid%3D1500919252%26sr%3D8-1%26keywords%3Ddavid%2Bmarr%2Bvision)

[Medicine &amp; Health Science Books @ Amazon.com](https://link.zhihu.com/?target=https%3A//www.amazon.com/Vision-Science-Phenomenology-Stephen-Palmer/dp/0262161834/ref%3Dpd_bxgy_14_img_3%3F_encoding%3DUTF8%26pd_rd_i%3D0262161834%26pd_rd_r%3D1V5VC032K7V9ECJVETFT%26pd_rd_w%3DrdAd3%26pd_rd_wg%3DtulJK%26psc%3D1%26refRID%3D1V5VC032K7V9ECJVETFT)

这3本我只是跳着看了几个章节，可能看完后会有不同的理解吧，自己胡乱在网上搜到的







哈哈哈，还是有一些人在认真搞视觉底层的计算原理的。今天刚读了一篇 paper，用 受限玻尔兹曼机 来拟合 V1 的simple cell 的感受野。




有看过一些单独做图片深度预测的论文。。应该能结合当前的2d认知搞点事？

通常您说的这类论文得到的结果仅仅针对部分数据集，得到规定范围内的深度。深度信息和拍摄的位置有很大的关系。虽然之前nips2016上有一篇文章用来处理in the wild的深度信息，但是得到的也只仅仅是相对深度关系，从2d生成3d这部分丢失的信息很难去获取，不同的场景下不同物体深度之间的关系也可能不相同。




改模型和调参数的那些人都是做计算机视觉的，不是做神经网络的。做神经网络那帮人才真是遇到瓶颈了，直到今年的CVPR才看见一篇对神经网络结构中每个神经元作用进行探讨的论文。
http://openaccess.thecvf.com/content_cvpr_2017/papers/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.pdf


人也是慢慢train出来立体感的，并不是天生的。

三维空间认知以现在的cnn和dnn的设计是绝对不可能做到的



今天有幸听到隔壁心理所一个老师的观点。就出在 你推荐的Daviad Marr 的《Vision》里，视觉是 2+1/2 维度的，不是单纯的二维信息，也没到三维，视觉神经元在计算 surface。这个观点我现在有点理解，但是解释不清楚，放上几个关键词，之后想清楚了再来讨论。
2.5D的想法是猜测，这本书是80年代出的，把输入的图像映射到三维一致性的模型是现在主流观点


theoretical neuroscience先看完这本书吧




个人认为如果想从神经学角度推进CNN，第一件事就是解释清楚为什么“毫无仿生学特性的Relu”比“所谓模拟神经突触的Sigmoid”效果要好
谁说relu不仿生?





训练立体感可以直接用Unity啊Unreal之类的引擎产生数据，这样连人工都不需要，理论上训练没这么难的吧。开车之类的可能直接到GTA里面训练都成
我以前也这么想的，但看了这几本书发现完全不是一回事




## REF

- [计算机视觉是否已经进入瓶颈期？](https://www.zhihu.com/question/51863955)
