---
title: 02 几个典型的网络
toc: true
date: 2018-08-18 16:38:05
---



### AlexNet

2012 Imagenet 比赛第一，Top5准确度超出第二10%

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/gkFB6D2F6i.png?imageslim)

图像上的 224*224 是写错了，是 227*227 。
也就是说进来的图像都是 227*227*3 的。


### ZFNet

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/Jjd48AGDCI.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/Hj474D57ha.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/li3IHIc8iG.png?imageslim)

这个网络很少有人提了，因为这个与 AlexNet 差不多，只是一些参数不同而已。


### VGG

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/Bc9Hb4eibC.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/7BJ0DIJAbc.png?imageslim)

这个用的非常多，非常非常多。

命名14年的比赛第一名是 googlenet 但是现在大家基本上都记得 VGG，因为 VGG 非常适合做迁移学习 Transfer learning。

VGG 提出的时候有一系列的网络。VGG-16、VGG-19，有不同的参数。

VGG 网络层次很深，很难训练，当时训练出来的时候是比较惊人的。与 ResNet 不同，对于传统的网络，训练出 VGG 是很了不起的。

上面的图片中，没有写激励层，实际上是有的，与 卷积层合着写在一起了，因为层数太多了。


### GoogleNet

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/1JB71fHG2L.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/LK8KfHA8Hi.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/FdBjeG69f8.png?imageslim)


他用的方式比较 tricky ，在尾部的时候，把所有的全连接层全部换成了卷积层，去加速他的运算。在 14年的时候他还没有提出 batch normazation ，可以看出它设立了几个目标级联起来，来使训练可以继续下去。

而且他把全连接层换成了 1*1 的小卷积层。极大的加速了他们的运算。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/387gc6mJbL.png?imageslim)


由于他们用的是小卷积核的处理，只需要500万个参数。训练的很快，而且，他把 12年的 16.4% 降到了 6.67% 。非常的厉害。



### ResNet

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/AG3i17de8c.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/D4Fliahmk1.png?imageslim)

这个 ResNet 现在已经能有 1024 层了。。这个是 CVPR 2016 的bestpaper。

训练卷积神经网络非常困难，它困难在梯度衰减，因为我们用的是 反向传播来训练的，它会不断的求每一层的偏导，对每一层的偏导进行连乘，这样的话，当里面如果有任何一个值非常小的时候，趋于0 的时候，最后的结果就趋于0了。这个时候，我就不可能对我的w 进行有效的更新了。

所以经常有同学说：我的网络训练训练，但是 loss 不下降了。或者准确度不动了，很有可能是梯度衰减，导致 w 没有进行什么有效的更新。

但是更深的网络在图像上是有用的。因为图像是一个层次非常丰富的数据。

所以，15年的时候 何开明提出了这么一个：

如果连乘的形式会因为出现一个0 导致训练不下去了，那么我能不能把乘法做一个变更呢？我每次去学习的时候，不学完整的映射，只学残差的映射。

所以 ResNet 有很多个这样的模块。老师说有人训练了一个1万层的，但是老实说 2048 层的比 1024 层的效果差一些，说在这么深的神经网络已经有些过拟合了。即时你用了dropout 等各种正则化的手段。

因此所有的残差网络都是用这个模块去堆，github 上有很多1024层的这种网络。

关于这个模块：

BP算法，在现在有两条路的是偶，会分一部分出去，是一个求和的形式。

<span style="color:red;">关于这个模块的讲解还是不够透彻。为什么是加呢？而且具体的计算还是要弄明白的，而且现在有什么变种？</span>






## REF

- 七月在线 opencv计算机视觉
