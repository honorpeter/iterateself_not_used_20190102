---
title: 03 迁移学习
toc: true
date: 2018-08-18 16:38:10
---


##

老师主要使用的是 caffe 、tensorflow 、mxnet ，这三个

tensorflow 是因为 tensorbord 对于可视化做得比较好。

keras 比较容易上手。

transferlearning  这几年在cv 界的应用怎么样？ dl 落地的在 cv 还是比较多的，在 nlp 还是比较少的，nlp 只有翻译系统和问答系统，cv 上有物体检测、各种各样的识别什么的，年龄识别、情绪识别、性别识别 等等。应用挺多的。

有的人做一些有意思的东西，比如自动从 NBA 的视频里面做一些视频集锦。<span style="color:red;">这也可以</span>

再比如现在的广告，视频前面的广告可以与视频本身比较相关，这个就需要了解你的视频。所以 cv 是有用的。

有的同学说，网络在训练的时候经常震荡，老师说，震荡是很正常的，因为你用的是 minibatch sgd，是在一部分数据上求的偏导去做得迭代，因为不是全量数据，因此一定是震荡下降的。只有经过若干个 batch 之后才会把所有的训练集看一遍，这样才下降。

有的同学问：filter 的 w 的初始化，最早的时候使用的是全部的随机的最小数初始化。这个是有问题的，现在大部分各种各样的package 有一个初始化方式是哈维尔初始化，再加上 15年  google 提出的 batch normalization ，因此现在初始化的要求没有那么高了。<span style="color:red;">为什么全部的随机的最小数是有问题的？什么是哈维尔初始化？为什么有了 batch normalization 之后初始化的要求没有那么高了？</span>

有的同学问，batch 训练是 shuffer 的还是 顺序的？老师说，这个影响不大。



OK，正式开始：


我们刚才讲的几种网络做得任务都只有一个 就是图像识别 image classification 。这个任务很重要，因为他是很多任务的先驱和启发式的。

图像相关任务：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/7a95FhKgkJ.png?imageslim)

比如说，你识别出这个图像是一个猫，那么想要知道这个猫具体的位置，这个就是 image location 。

或者，我想从图像上圈出来独立的一个个物体，这个叫做物体检测。

还有一个比较困难的就是图像分割：不仅需要框处物体的位置，还需要把物体的轮廓标出来。

这四个任务就是能在工业界应用的四个例子。

至于 neural style 还没有什么可以在工业界落地的项目。


#### 图像识别+定位

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/D9m9a4HfkD.png?imageslim)

<span style="color:red;">交并准则是什么？</span>

ImageNet

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/idD9Ce2fHd.png?imageslim)

那么怎么做呢？我们先看单物体的：

### 思路1：视作回归问题

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/La9IAb075e.png?imageslim)

如果一个图像，我是能知道这个图片是不是一只猫的，但是这个猫的位置的定位，一个最简单的方式是这样的：刚才我们是分类问题，分成是不是猫。
但是，现在我可以改成一个回归问题，输出的是4个值，分别是左上角的坐标和长宽。


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/JBehgjhf7A.png?imageslim)

在 CNN 里面，区分 猫狗鸭子的时候，最后的是FC输出的是每个种类的概率。

换到我现在要找到 (x,y,w,h) 我可以把 FC 改成输出4个值。当然，这个也需要一些训练数据，这个训练数据的label 要提供正确的手工标注的 (x,y,w,h) 值来供我们训练。然后损失函数我们可以使用  L2 loss。嗯，分类问题用的损失函数是 cross entropy loss。是不同的。

所以，这个其实还是比较容易理解的。

所以，我们可以用相同的主体，但是后面接一个分类的head。那么具体从哪里把积木断开呢？

- 在VGG 这个网络上提了一个 overfeat 方式，在最后一个卷积层后面开始接新的积木，
- 但是 R-CNN 的 DeepPose 是在最后一个全连接层之后再接一个regression 的 head。<span style="color:red;">是这样吗？是在分类的后面接回归吗？还是直接替换了分类的head？确认下。</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/IdGgE5fJ09.png?imageslim)

那么对于同一张图片是先分类再回归吗？还是两个任务合在一起作为一个损失来训练？

训练的方式是这样的，比如说我们已经对分类问题训练好了，这时候为了复用前面的积木，我们可以把前面的 w fix 住(这个操作可以在框架中设定的)，只针对回归问题的新的 head 学后面的部分，这样是可以的。

不过有的人做过这样的试验，在分类问题训练好之后，w 不 fix，只把他的学习率 learning rate 调低，然后把后面的新 head 的学习率调高。然后进行训练。这样会得到一个新的完整的用来做回归的网络，而且结果相比 w fix 住的方式是有提升的。

但是这样虽然效果好了些，但是由于你的网络的w已经与分类问题的w 不同了，那么对真正的一个照片进行区分猫狗和圈出位置的时候，你就需要跑两个网络，而对于上面的权重相同的情况，只需要跑相同的网络然后 head 分别跑一下就行。<span style="color:red;">嗯，的确，还真的是这样。</span>


<span style="color:red;">的确是个好办法。之前我一直不清楚这个框是怎么画出来的，原来也是需要很多的打标的框的数据来训练出来的。而且这个分类和画框的网络之间的关系也稍微清楚了些。</span>


### 思路2: 图窗+识别与整合


事实上，如果你用刚才的回归的思路，实际的效果不是很好。因为真的有人做实验，回归的准确率挺低的。

所以他们想了其他的方法：

如果不能把刚才的问题视为回归的问题，就是说如果直接找 (x,y,w,h) 很麻烦的话，我能不能提前给一些框去帮助他找这个位置？

比如：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/geabdI8h7A.png?imageslim)

这个原始的图片是 221*221 的，然后在这个图像上，我找了左上角、左下角、右上角、右下角四个框，然后，我分别用这4个框里面的图像做一个分类，就得到了每个框内图像的分类的得分。比如左上角框内的猫是0.5。

然后由于 0.75 和 0.8 比较大，那么这个主题一定是比较靠近右边的。

他们训练的时候我标准的答案超出了这个框。

两组神经网络的输入数据都是黑框内的数据吗？是的。

红框为什么不在黑框内部？是的，他们在训练的饿时候做了一些比较 tricky 的事情，我们输入的时候是这个框，但是你的标准答案可以只是4个数，这4个数可以任意指定，所以我妈们在训练集里面，我给的是这个框的数据，但是我给的位置超出了这个框，他给的标准答案实际上是超出这个黑框的，其实我训练的时候只是四个数而已，不管是框内还是框外、<span style="color:red;">什么意思？</span>

现在你有四个框，你有这4个框的概率值，你也有不同的红框的位置。


他后面做得东西是 Box Mereging 把所有的框 merge 在一起，至于 merge 的方式可以看下这篇文章：![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/dAKGBm9cFd.png?imageslim)

这个文章里面做得方式是：我有4个概率，有4个红框，我根据概率值对红框进行merge。这个merge 的方式比较tricky，而且现在的主流的方法已经没有太用到这个了。<span style="color:red;">嗯，这个是不是已经过时了？</span>

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/mKeAAd5Ka9.png?imageslim)

这个框大小怎么定？位置怎么定？难道就取左上，右上，左下，右下？这个问题其实没有标准答案，上面这个 14年的paper做了很粗暴的非常多的框，然后再进行merge。当然他在不同的框的公共部分的计算是有优化的。没有那么暴力。

<span style="color:red;">第一种思路的训练出的回归网络是不是没有用到了？我看这个思路直接用分类的网络来得到概率再进行 merge。确认下？而且，这种产生很多框再进行识别出概率，这个是需要并行的把，感觉计算量比第一种方法大很多。不知道是不是？为什么老师说的时候是在每个框里面做了classification 和 regression ？是说顺嘴了吗？还是的确是这样？</span>

有同学说，既然这个paper 里面的方法比 NMS 要好，为什么现在还在用 NMS？这个方法是有亮点的，但是实际上并没有 paper 里面说的那么有效。

下面这个是表示的，为了对不同的框的公共部分的重复计算进行优化，他进行了 computation  sharing。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/4B9BkL5cg0.png?imageslim)

为了把算量降到最低，因为 FC 没有办法共享，所以他把 FC 都踢掉了，把后面的东西换成 1*1 的卷积。使得里面的一部分运算都可以共享。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180812/L4GiJcF98f.png?imageslim)

这个 paper 的一些东西现在已经不用了。<span style="color:red;">好吧，哪些不用了？为什么不用了？</span>





## REF

- 七月在线 opencv计算机视觉
