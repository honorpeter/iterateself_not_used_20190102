---
title: 05 海量数据的处理
toc: true
date: 2018-07-22 17:30:05
---

借助于大规模的工具

比如 Spark



Spark MLlib
- Spark’s machine learning (ML) library.
- Goal is to make ML scalable and easy
- Includes common learning algorithms and utilities, like classification, regression, clustering, collaborative filtering, dimensionality reduction
- Includes lower-level optimization primitives and higher level pipeline APIs
- Divided into two packages
      - Spark.mllib: original API built on top of RDDs
      - Spark.ml: provides higher level API built on top of DataFrames for constructing ML pipelines


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/ig4c17BggE.png?imageslim)

Spark MLlib 可以用来建模，他提供了一个类似于流程的东西 ML pipeline，


ML pipelines for complex ML workflows

- DataFrame: Equivalent to a relational table in Spark SQL
- Transformer: An algorithm that transforms one DataFrame into anothe
- Estimator: An algorithm that can be fit on a DataFrame to produce a Transformer
- Pipeline: Chains multiple transformers and estimators to produce a ML workflow
- Parameter: Common API for specifying parameters

spark sql 里面也可以读出来一个 dataframe
Transformer 可以对 DataFrame 做一些数据的预处理

一个 pipeline 大概是这样的：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/CAfb1cbD2D.png?imageslim)





我们大概看一下：ML pipeline for Click Through Rate Prediction

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/65IBkBd1f6.png?imageslim)


stringindexer 实际上就是给这个类别一些下标，然后 OneHotEncoder 就对这些类别进行编码

看起来和 sklearn 非常像，但是这个是在集群上跑。



因为对scala 不熟，因此使用的是 pyspark，



Step 1 - Parse Data into Spark SQL DataFrames

- Spark SQL converts a RDD of Row objects into a DataFrame
- Rows are constructed by passing key/value pairs where keys define the column names of the table


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/9im8iA3CEA.png?imageslim)

把读进来的每一行用逗号进行分割。



Step 2 – Feature Transformer using StringIndexer

- Encodes a string column to a column of indices
- Indices are from 0 to max number of distinct string values,
ordered by frequencies
- If column is numeric, cast it to string and index string values

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/gK41HehkF8.png?imageslim)

它是按照频次的高低依次进行编码

工业界 scala 和 pyspark 都会用，但是最新的功能基本上都是 scala 的，目前所有的模型 比如 LR 和GBDT 的这些 pyspark 都有。
因此可以用 pyspark。

XGBoost 也有分布式的。



Step 3 - Feature Transformer using One Hot Encoding

- Maps a column of label indices to a column of binary vectors
- Allows algorithms which expect continuous features, to use categorical features

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/h39mGbBlb3.png?imageslim)

上面这个图，可以看出 做 One-hot encoding 的时候，不是每个类别都给一个feature 的，这里就只给了两个feature，<span style="color:red;">这样不会有问题吗？</span>



Step 4 – Feature Selector using Vector Assembler

- Transformer that combines a given list of columns into a single vector column
- Useful for combining raw features and features generated by different feature transformers into a single feature vector

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/9CfdiHD99h.png?imageslim)

Vector Assembler 就把全面所有处理完的这些列拼在一起，得到最终的feature 。



Step 5 – Train a model using Estimator Logistic Regression

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/4f5Bjcdj46.png?imageslim)




Parameter Tuning using CrossValidator or TrainValidationSplit

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180722/K35EAfFb2C.png?imageslim)



下面这段没有讲：

Some alternatives:

- Use Hashed features instead of OHE
- Use Log loss evaluation or ROC to evaluate Logistic Regression
- Perform feature selection
- Use Naïve Bayes or other binary classification algorithms
