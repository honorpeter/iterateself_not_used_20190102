---
title: 04 案例2：文本相似度
toc: true
date: 2018-07-24 17:25:47
---



## 比赛地址

Kaggle竞赛题：https://www.kaggle.com/c/home-depot-product-search-relevance

这是一个五金类商品的网站，牛逼之处在于只需要在搜索框内告诉比如：我要打地基。

也就是说这个网站就是一个垂直领域的 google ，直接搜索就行。因为通用的搜索已经有 google了，但是垂直领域的搜索还是可以做的。

他找了很多人，来评价我的之前返回的数据是否准确。准确的是3分，不准确的是1分。

这些所有的标注好的数据，

相似度检测

# 关键词搜索



鉴于课件里已经完整的 show 了 NLTK 在各个 NLP 处理上的用法，我这里就不再重复使用了。

本篇的教程里会尽量用点不一样的库，让大家感受一下 Python NLP 领域各个库的优缺点。<span style="color:red;">好的。</span>

## Step1：导入所需

所有要用到的库


```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from nltk.stem.snowball import SnowballStemmer
```

读入训练/测试集


```python
df_train = pd.read_csv('../input/train.csv', encoding="ISO-8859-1")
df_test = pd.read_csv('../input/test.csv', encoding="ISO-8859-1")
```

这里还有个有用的玩意儿，叫产品介绍


```python
df_desc = pd.read_csv('../input/product_descriptions.csv')
```
产品介绍就是：比如说是 AA Battery ，那么这个 desc 里面就会对这进行解释，说是一个规格为 AA 的电池。

这个是有用的，因为我们需要更多的信息来支撑我们的搜索。

看看数据们都长什么样子。


```python
df_train.head()
```


|      | id   | product_uid | product_title                                     | search_term        | relevance |
| ---- | ---- | ----------- | ------------------------------------------------- | ------------------ | --------- |
| 0    | 2    | 100001      | Simpson Strong-Tie 12-Gauge Angle                 | angle bracket      | 3.00      |
| 1    | 3    | 100001      | Simpson Strong-Tie 12-Gauge Angle                 | l bracket          | 2.50      |
| 2    | 9    | 100002      | BEHR Premium Textured DeckOver 1-gal. #SC-141 ... | deck over          | 3.00      |
| 3    | 16   | 100005      | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | rain shower head   | 2.33      |
| 4    | 17   | 100005      | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | shower only faucet | 2.67      |



```python
df_desc.head()
```


|      | product_uid | product_description                               |
| ---- | ----------- | ------------------------------------------------- |
| 0    | 100001      | Not only do angles make joints stronger, they ... |
| 1    | 100002      | BEHR Premium Textured DECKOVER is an innovativ... |
| 2    | 100003      | Classic architecture meets contemporary design... |
| 3    | 100004      | The Grape Solar 265-Watt Polycrystalline PV So... |
| 4    | 100005      | Update your bathroom with the Delta Vero Singl... |

嗯，更多的描述信息是在 desc 里面的。

对于这种特别复杂的数据，我么要做的事情就是统一预处理。

看来不要做太多的复杂处理，我们于是直接合并测试/训练集，以便于统一做进一步的文本预处理：


```python
df_all = pd.concat((df_train, df_test),
                   axis=0,
                   ignore_index=True)
```


```python
df_all.head()
```


|      | id   | product_title                                     | product_uid | relevance | search_term        |
| ---- | ---- | ------------------------------------------------- | ----------- | --------- | ------------------ |
| 0    | 2    | Simpson Strong-Tie 12-Gauge Angle                 | 100001      | 3.00      | angle bracket      |
| 1    | 3    | Simpson Strong-Tie 12-Gauge Angle                 | 100001      | 2.50      | l bracket          |
| 2    | 9    | BEHR Premium Textured DeckOver 1-gal. #SC-141 ... | 100002      | 3.00      | deck over          |
| 3    | 16   | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | 100005      | 2.33      | rain shower head   |
| 4    | 17   | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | 100005      | 2.67      | shower only faucet |




看一下合并之后的数量:



```python
df_all.shape
```


```
(240760, 5)
```

有 24 万条数据，与之前的两千条数据的例子比起来就算数据量比较高的。


产品介绍也是一个极有用的信息，我们把它拿过来：<span style="color:red;">merge 还是要掌握的，合并的过程还是很方便的</span>


```python
df_all = pd.merge(df_all,
                   df_desc,
                   how='left',
                   on='product_uid')
```


```python
df_all.head()
```



|      | id   | product_title                                     | product_uid | relevance | search_term        | product_description                               |
| ---- | ---- | ------------------------------------------------- | ----------- | --------- | ------------------ | ------------------------------------------------- |
| 0    | 2    | Simpson Strong-Tie 12-Gauge Angle                 | 100001      | 3.00      | angle bracket      | Not only do angles make joints stronger, they ... |
| 1    | 3    | Simpson Strong-Tie 12-Gauge Angle                 | 100001      | 2.50      | l bracket          | Not only do angles make joints stronger, they ... |
| 2    | 9    | BEHR Premium Textured DeckOver 1-gal. #SC-141 ... | 100002      | 3.00      | deck over          | BEHR Premium Textured DECKOVER is an innovativ... |
| 3    | 16   | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | 100005      | 2.33      | rain shower head   | Update your bathroom with the Delta Vero Singl... |
| 4    | 17   | Delta Vero 1-Handle Shower Only Faucet Trim Ki... | 100005      | 2.67      | shower only faucet | Update your bathroom with the Delta Vero Singl... |




好了，现在我们得到一个全体的数据大表格

接下来就是文本域处理。

## Step 2: 文本预处理

我们这里遇到的文本预处理比较简单，因为最主要的就是看关键词是否会被包含。

所以我们统一化我们的文本内容，以达到任何 term 在我们的数据集中只有一种表达式的效果。

在搜索领域，词形归一是非常重要的。

我们这里用简单的 Stem 做个例子：

（有兴趣的同学可以选用各种你觉得靠谱的预处理方式：去掉停止词，纠正拼写，去掉数字，去掉各种emoji，等等）


```python
stemmer = SnowballStemmer('english')

def str_stemmer(s):
    return " ".join([stemmer.stem(word) for word in s.lower().split()])
```

为了计算『关键词』的有效性，我们可以 naive 地直接看『出现了多少次』


```python
def str_common_word(str1, str2):
    return sum(int(str2.find(word)>=0) for word in str1.split())
```

str1 是搜索string，str2 是 produce string，我们看 str1 里面出现的单词在 str2 里面出现的次数。

在文本处理这件事的时候，之前我们在案例 1 里面都是直接调用的 tf-idf 或者 词频率统计。这里我们讲的就是怎么创建自己想要的特征。这个是这个比赛不一样的地方，因为一般情况下都是直接调用第三方库就可以解决了。

接下来，就用这两个方法把每一个column都跑一遍，以清洁所有的文本内容


```python
df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(x))
```

```python
df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))
```

```python
df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))
```

OK，这样数据已经是词形统一了。


## Step 3: 自制文本特征

一般属于一种脑洞大开的过程，想到什么可以加什么。

当然，特征也不是越丰富越好，稍微靠谱点是肯定的。

完全取决于

#### 关键词的长度：


```python
df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)
```

<span style="color:red;">可见，特征工程在自然语言处理中就是这么简单。就是脑洞，把它有意义的东西更好的表达出来。这是在文本处理中非常典型的自制文本特征的方式。可以做很多的事情，一些通用的做法还是要收集一下的。</span>

<span style="color:red;">这个也能作为特征吗？</span>

<span style="color:red;"></span>

#### 标题中有多少关键词重合


```python
df_all['commons_in_title'] = df_all.apply(lambda x:str_common_word(x['search_term'],x['product_title']), axis=1)
```

嗯，这个也是不错的特征。

#### 描述中有多少关键词重合


```python
df_all['commons_in_desc'] = df_all.apply(lambda x:str_common_word(x['search_term'],x['product_description']), axis=1)
```

嗯，这个也是不错的特征。

等等等等。。变着法子想出些数字能代表的 features，一股脑放进来~可以放很多的不同的内容，各种 feature 。乱七八糟的什么都可以加。

比如可以把 search_term 和 product_title 都表达成 word2vec，然后把这两个vec 之间的夹角值作为一个特征。<span style="color:red;">这也行。</span>

实际上一般是越多越好的，尤其是通过 gbdt 来做，两重随机性基本可以避免毫无用处的 feature 来扰乱我们的分类。

搞完之后，我们把不能被『机器学习模型』处理的 column 给 drop 掉。因为他们是文本，在我们真正使用模型进行训练的过程中不能被机器所处理的。


```python
df_all = df_all.drop(['search_term','product_title','product_description'],axis=1)
```




## Step 4: 重塑训练/测试集

舒淇说得好，要把之前脱下的衣服再一件件穿回来。


如果你在之前你为了简便，把一些东西合起来，处理了，那么你真正要跑模型的时候，在按照你顺序把该处理的重新处理回来。

数据处理也是如此，搞完一圈预处理之后，我们让数据重回原本的样貌

#### 分开训练和测试集


```python
df_train = df_all.loc[df_train.index]
df_test = df_all.loc[df_test.index]
```
根据之前的 index 来吧 df_all 划分开。

#### 记录下测试集的id

留着上传的时候，能对的上号。

```python
test_ids = df_test['id']
```

#### 分离出y_train


```python
y_train = df_train['relevance'].values
```

#### 把原集中的label给删去

否则就是cheating了


```python
X_train = df_train.drop(['id','relevance'],axis=1).values
X_test = df_test.drop(['id','relevance'],axis=1).values
```

嗯，id 也要去掉。


## Step 5: 建立模型

我们用个最简单的模型：RandomForest 回归模型


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
```

用 CV 结果保证公正客观性；并调试不同的alpha值


```python
params = [1,3,5,6,7,8,9,10]
test_scores = []
for param in params:
    clf = RandomForestRegressor(n_estimators=30, max_depth=param)
    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
```

画个图来看看：


```python
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(params, test_scores)
plt.title("Param vs CV Error");
```


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/BIH8lDCmDg.png?imageslim)

大概6~7的时候达到了最优解

## Step 6: 上传结果

用我们测试出的最优解建立模型，并跑跑测试集


```python
rf = RandomForestRegressor(n_estimators=30, max_depth=6)
```


```python
rf.fit(X_train, y_train)
```




```
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=6,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_split=1e-07, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,
           verbose=0, warm_start=False)
```




```python
y_pred = rf.predict(X_test)
```

把拿到的结果，放进 PD，做成CSV上传：


```python
pd.DataFrame({"id": test_ids, "relevance": y_pred}).to_csv('submission.csv',index=False)
```

## 总结：

这一篇教程中，虽然都是用的最简单的方法，但是基本框架是很完整的。

同学们可以尝试修改/调试/升级的部分是：

1. **文本预处理步骤**: 你可以使用很多不同的方法来使得文本数据变得更加清洁。这里只是使用了简单的 stemmer，可以使用lemma+词性

2. **自制的特征**: 相处更多的特征值表达方法（关键词全段重合数量，重合比率，等等）

3. **更好的回归模型**: 根据之前的课讲的Ensemble方法，把分类器提升到极致




嗯，还是有很多可以改进的地方的。
