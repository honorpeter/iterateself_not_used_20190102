---
title: 09 模型融合
toc: true
date: 2018-07-19 22:29:24
---
## 最后，进行模型融合

模型融合在比赛中是肯定要做的，不然，单一的模型很可能不会取到很好的效果。

<span style="color:red;">关于模型融合还是要拿出来单独写的。</span>

### 模型融合的方式

有三种方式进行模型融合，他们分别基于几个道理：

- 群众的力量是伟大的，集体智慧是惊人的
    - Bagging。最简单最常见。
    - 随机森林/Random forest
- 站在巨人的肩膀上，能看得更远
    - 模型stacking。比赛时候可能会用一个弱化的 stacking 方式叫做 blending
- 一万小时定律 <span style="color:red;">这个没明白</span>
    - Adaboost
    - 逐步增强树/Gradient Boosting Tree

OK，我们依次介绍：

### 群众的力量是伟大的

#### Bagging

模型很多时候效果不好的原因是什么？
- 过拟合啦！！ ！

如何缓解？
- 少给点题，别让它死记硬背这么多东西
- 多找几个同学来做题， 综合一下他们的答案


Bagging

```python
class sklearn.ensemble.BaggingClassifier(
  base_estimator=None,
  n_estimators=10,
  max_samples=1.0,
  max_features=1.0,
  bootstrap=True,
  bootstrap_features=False,
  oob_score=False,
  warm_start=False,
  n_jobs=1,
  random_state=None,
  verbose=0)
```



如果是使用相同的算法：
- 不用全部的数据集， 每次取一个子集训练一个模型
- 分类： 用这些模型的结果做投票
- 回归： 对这些模型的结果取平均

如果使用不同的算法：
- 用这些模型的结果做 vote 或 求平均

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/eh6dK42Ikm.png?imageslim)

如果是直接用全部的data，那么结果可能会overfitting，如果是用的100个树做得bagging，那么结果会平滑很多。

### 站在巨人的肩膀上，能看得更远

#### Stacking

用多种predictor结果作为特征训练

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/fkIce1hA73.png?imageslim)

我有很多的 predictor 我的每个分类器都会给一个结果，分别是 r1,r2,r3 我把这一层的这些分类器的结果作为特征来构建我第二层的分类器。

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/fh4d0D4CCL.png?imageslim)


stacking 并没有在sklearn 中有现成的。
blending 课程已经提供了一个脚本，他是把第一层的结果提供给一个linear regressior 里面，然后得到这个线性分类器的权重。


### 一万小时定律

#### Boosting

最常见的就是boosting

http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble

Adaboost
考得不好的原因是什么？
- 还不够努力， 练习题要多次学习
    - 重复迭代和训练
- 时间分配要合理， 要多练习之前做错的题
    - 每次分配给分错的样本更高的权重
- 我不聪明， 但是脚踏实地， 用最简单的知识不断积累， 成为专家
    - 最简单的分类器的叠加

如果你要用GBDT，不要用sklearn 里面的，因为它很慢，用 XGBoost，或者是微软刚开源的lightGBM。可以在网上找到lightGBM 的python 封装。


Adaboost 模型融合： Boosting

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/3ALLgl4LIJ.png?imageslim)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180718/hAhIJ7g1hc.png?imageslim)

AdaBoost 是调样本的权重，Gradient boosting 是在loss function 上做的。<span style="color:red;">要弄清楚。</span>




## 相关资料

- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html
