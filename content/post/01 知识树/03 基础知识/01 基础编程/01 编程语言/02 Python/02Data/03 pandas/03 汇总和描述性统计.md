---
title: 03 汇总和描述性统计
toc: true
date: 2018-06-24 08:25:58
---
# 5.3 Summarizing and Computing Descriptive Statistics（汇总和描述性统计）

pandas 有很多数学和统计方法。大部分可以归类为降维或汇总统计，这些方法是用来从 series 中提取单个值（比如 sum 或 mean ）。还有一些方法来处理缺失值：


<font color=red>嗯，这种初始化 DataFrame 的方式</font>
```python
import pandas as pd
import numpy as np
df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5],
                   [np.nan, np.nan], [0.75, -1.3]],
                  index=['a', 'b', 'c', 'd'],
                  columns=['one', 'two'])
df
```

输出：

|      | one  | two  |
| ---- | ---- | ---- |
| a    | 1.40 | NaN  |
| b    | 7.10 | -4.5 |
| c    | NaN  | NaN  |
| d    | 0.75 | -1.3 |

使用sum 的话，会返回一个series:



```python
df.sum()
```

输出：

```text
one    9.25
two   -5.80
dtype: float64
```

使用 `axis='columns'` or `axis=1` ，计算列之间的和：



```python
df.sum(axis='columns')
```

输出：

```text
a    1.40
b    2.60
c    0.00
d   -0.55
dtype: float64
```

计算的时候，NA（即缺失值）会被除外，除非整个切片全是NA。我们可以用 skipna 来跳过计算NA：<font color=red>这种跳过 NA 有什么用处吗？</font>

<font color=red>skipna 默认是True</font>

```python
df.mean(axis='columns', skipna=False)
```

输出：

```text
a      NaN
b    1.300
c      NaN
d   -0.275
dtype: float64
```

一些 reduction 方法：<font color=red>什么是reduction 方法？</font>
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180624/9mm3maKC5l.png?imageslim)

一些方法，比如 idxmin 和 idxmax ，能返回间接的统计值，比如 index value：



```python
df
```

输出：

|      | one  | two  |
| ---- | ---- | ---- |
| a    | 1.40 | NaN  |
| b    | 7.10 | -4.5 |
| c    | NaN  | NaN  |
| d    | 0.75 | -1.3 |



```python
df.idxmax()
```

输出：<font color=red>嗯，计算的是每一列的最大值的index</font>

```text
one    b
two    d
dtype: object
```

还能计算累加值：<font color=red>一直想知道累加会在什么时候用到？</font>



```python
df.cumsum()
```

输出：

|      | one  | two  |
| ---- | ---- | ---- |
| a    | 1.40 | NaN  |
| b    | 8.50 | -4.5 |
| c    | NaN  | NaN  |
| d    | 9.25 | -5.8 |

另一种类型既不是降维，也不是累加。describe能一下子产生多维汇总数据：

```python
df.describe()
```
输出：<font color=red>一般我们什么时候用这个 describe 呢？为什么要使用呢？</font>

|       | one      |   two     |
| ----- | -------- | --------- |
| count | 3.000000 | 2.000000  |
| mean  | 3.083333 | -2.900000 |
| std   | 3.493685 | 2.262742  |
| min   | 0.750000 | -4.500000 |
| 25%   | 1.075000 | -3.700000 |
| 50%   | 1.400000 | -2.900000 |
| 75%   | 4.250000 | -2.100000 |
| max   | 7.100000 | -1.300000 |

对于非数值性的数据，describe能产生另一种汇总统计：



```python
obj = pd.Series(['a', 'a', 'b', 'c'] * 4)
obj
```

输出：

```text
0     a
1     a
2     b
3     c
4     a
5     a
6     b
7     c
8     a
9     a
10    b
11    c
12    a
13    a
14    b
15    c
dtype: object
```



```python
obj.describe()
```

输出：<font color=red>嗯，非数值型的 describe 结果还是有很大不同的</font>

```text
count     16
unique     3
top        a
freq       8
dtype: object
```

下表是一些描述和汇总统计数据：<font color=red>都要总结一下，因为最起码要知道都是用来做什么的。</font>
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180624/1HAGc8AH7j.png?imageslim)

# 1 Correlation and Covariance (相关性和协方差)

假设 DataFrame 是股价和股票数量。这些数据取自yahoo finace，用padas-datareader包能加载。如果没有的话，用conda或pip来下载这个包：



```cmd
conda install pandas-datareader
```



```python
import pandas_datareader.data as web
all_data = {ticker: web.get_data_yahoo(ticker)
            for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}
price = pd.DataFrame({ticker: data['Adj Close']
                      for ticker, data in all_data.items()})
volumn = pd.DataFrame({ticker: data['Volumn']
                       for ticker, data in all_data.items()})
```

上面的代码无法直接从 yahoo 上爬取数据，因为 yahoo 被 verizon 收购后，好像是不能用了。于是这里我们直接从下好的数据包里加载。




```python
price = pd.read_pickle('../examples/yahoo_price.pkl')
volume = pd.read_pickle('../examples/yahoo_volume.pkl')
price.head()
```

输出：

|            | AAPL      | GOOG       | IBM        | MSFT      |
| ---------- | --------- | ---------- | ---------- | --------- |
| Date       |           |            |            |           |
| 2010-01-04 | 27.990226 | 313.062468 | 113.304536 | 25.884104 |
| 2010-01-05 | 28.038618 | 311.683844 | 111.935822 | 25.892466 |
| 2010-01-06 | 27.592626 | 303.826685 | 111.208683 | 25.733566 |
| 2010-01-07 | 27.541619 | 296.753749 | 110.823732 | 25.465944 |
| 2010-01-08 | 27.724725 | 300.709808 | 111.935822 | 25.641571 |



```python
volume.head()
```

输出：

|            | AAPL      | GOOG     | IBM     | MSFT     |
| ---------- | --------- | -------- | ------- | -------- |
| Date       |           |          |         |          |
| 2010-01-04 | 123432400 | 3927000  | 6155300 | 38409100 |
| 2010-01-05 | 150476200 | 6031900  | 6841400 | 49749600 |
| 2010-01-06 | 138040000 | 7987100  | 5605300 | 58182400 |
| 2010-01-07 | 119282800 | 12876600 | 5840600 | 50559700 |
| 2010-01-08 | 111902700 | 9483900  | 4197200 | 51197400 |


现在我们计算一下价格百分比的变化：pct_change(): 这个函数用来计算同 colnums 两个相邻的数字之间的变化率<font color=red>还有这个函数！确认下。</font>



```python
returns = price.pct_change()
returns.tail()
```

输出：

|            | AAPL      | GOOG      | IBM       | MSFT      |
| ---------- | --------- | --------- | --------- | --------- |
| Date       |           |           |           |           |
| 2016-10-17 | -0.000680 | 0.001837  | 0.002072  | -0.003483 |
| 2016-10-18 | -0.000681 | 0.019616  | -0.026168 | 0.007690  |
| 2016-10-19 | -0.002979 | 0.007846  | 0.003583  | -0.002255 |
| 2016-10-20 | -0.000512 | -0.005652 | 0.001719  | -0.004867 |
| 2016-10-21 | -0.003930 | 0.003011  | -0.012474 | 0.042096  |

series 的 corr 方法计算两个，重合的，非 NA 的，通过 index 排列好的 series 。cov 计算方差：



```python
print(returns['MSFT'].corr(returns['IBM']))
print(returns['MSFT'].cov(returns['IBM']))

```

输出：

```text
0.4997636114415116
8.8706554797035489e-05
```

因为 MSFT 是一个有效的python属性，我们可以通过更简洁的方式来选中 columns：<font color=red>为什么是一个有效的python属性？</font>


```python
returns.MSFT.corr(returns.IBM)
```

输出：

```text
0.4997636114415116
```

dataframe 的 corr 和 cov 方法，能返回一个完整的相似性或方差矩阵：<font color=red>不知道这样的矩阵有什么作用？</font>



```python
returns.corr()
```

输出：

|      | AAPL     | GOOG     | IBM      | MSFT     |
| ---- | -------- | -------- | -------- | -------- |
| AAPL | 1.000000 | 0.407919 | 0.386817 | 0.389695 |
| GOOG | 0.407919 | 1.000000 | 0.405099 | 0.465919 |
| IBM  | 0.386817 | 0.405099 | 1.000000 | 0.499764 |
| MSFT | 0.389695 | 0.465919 | 0.499764 | 1.000000 |



```python
returns.cov()
```

输出：

|      | AAPL     | GOOG     | IBM      | MSFT     |
| ---- | -------- | -------- | -------- | -------- |
| AAPL | 0.000277 | 0.000107 | 0.000078 | 0.000095 |
| GOOG | 0.000107 | 0.000251 | 0.000078 | 0.000108 |
| IBM  | 0.000078 | 0.000078 | 0.000146 | 0.000089 |
| MSFT | 0.000095 | 0.000108 | 0.000089 | 0.000215 |

用 Dataframe 的 corrwith 方法，我们可以计算 dataframe 中不同 columns 之间，或 row 之间的相似性。传递一个 series：



```python
returns.corrwith(returns.IBM)
```

输出：

```text
AAPL    0.386817
GOOG    0.405099
IBM     1.000000
MSFT    0.499764
dtype: float64
```

传入一个 dataframe 能计算匹配的 column names 质监局的相似性。这里我计算 vooumn 中百分比变化的相似性：<font color=red>为什么可以与 volume 相计算？</font>



```python
returns.corrwith(volume)
```

输出：

```text
AAPL   -0.075565
GOOG   -0.007067
IBM    -0.204849
MSFT   -0.092950
dtype: float64
```

传入 axis='columns' 能做到 row-by-row 计算。在 correlation 被计算之前，所有的数据会根据 label 先对齐。

# 2 Unique Values, Value Counts, and Membership（唯一值，值计数，会员）

这里介绍另一种从一维 series 中提取信息的方法：



```python
obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])
```

第一个函数时unique，能告诉我们 series 里 unique values 有哪些：



```python
uniques = obj.unique()
uniques
```

输出：

```text
array(['c', 'a', 'd', 'b'], dtype=object)
```

返回的 unique values 不是有序的，但我们可以排序，uniques.sort()。相对的，value_counts 能计算 series 中值出现的频率：



```python
obj.value_counts()
```

输出：

```text
a    3
c    3
b    2
d    1
dtype: int64
```

返回的结果是按降序处理的。vaule_counts 也是 pandas 中的方法，能用在任何 array 或 sequence 上：<font color=red>sequence 是什么？</font>



```python
pd.value_counts(obj.values, sort=False)
```

输出：

```text
d    1
c    3
b    2
a    3
dtype: int64
```

isin 能实现一个向量化的集合成员关系检查，能用于过滤数据集，检查一个子集，是否在 series 的 values 中，或在 dataframe 的 column 中：



```python
obj
```

输出：

```text
0    c
1    a
2    d
3    a
4    a
5    b
6    b
7    c
8    c
dtype: object
```



```python
mask = obj.isin(['b', 'c'])
mask
```

输出：

```text
0     True
1    False
2    False
3    False
4    False
5     True
6     True
7     True
8     True
dtype: bool
```



```python
obj[mask]
```

输出：<font color=red>这样的过滤一般在什么时候使用？</font>

```text
0    c
5    b
6    b
7    c
8    c
dtype: object
```

与 isin 相对的另一个方法是 Index.get_indexer，能返回一个index array，告诉我们有重复值的 values(to_match)，在非重复的 values(unique_vals) 中对应的索引值：<font color=red>这也可以！这个什么时候用到？</font>



```python
to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])
unique_vals = pd.Series(['c', 'b', 'a'])
pd.Index(unique_vals).get_indexer(to_match)
```

输出：

```text
array([0, 2, 1, 1, 0, 2])
```

Unique, value counts, and set membership methods：
![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180624/dIdD4JE2GD.png?imageslim)


在某些情况下，你可能想要计算一下 dataframe 中多个 column 的柱状图：<font color=red>什么意思？</font>



```python
data = pd.DataFrame({'Qu1': [1, 3, 4, 3, 4],
                     'Qu2': [2, 3, 1, 2, 3],
                     'Qu3': [1, 5, 2, 4, 4]})
data
```

输出：

|      | Qu1  | Qu2  | Qu3  |
| ---- | ---- | ---- | ---- |
| 0    | 1    | 2    | 1    |
| 1    | 3    | 3    | 5    |
| 2    | 4    | 1    | 2    |
| 3    | 3    | 2    | 4    |
| 4    | 4    | 3    | 4    |

把 padas.value_counts 传递给 dataframe 的 apply 函数：<font color=red>这种用法什么时候用到呢？</font>

```python
result = data.apply(pd.value_counts)
result
```

输出：

|      | Qu1  | Qu2  | Qu3  |
| ---- | ---- | ---- | ---- |
| 1    | 1.0  | 1.0  | 1.0  |
| 2    | NaN  | 2.0  | 1.0  |
| 3    | 2.0  | 2.0  | NaN  |
| 4    | 2.0  | NaN  | 2.0  |
| 5    | NaN  | NaN  | 1.0  |

每一行的 laebls(即1，2，3，4，5) 其实就是整个data里出现过的值，从 1 到 5 。而对应的每个方框里的值，则是表示该值在当前列中出现的次数。比如：
- (2, Qu1) 的值是Nan，说明 2 这个数字没有在Qu1这一列出现过。
- (2, Qu2)的值是2，说明2这个数字在Qu2这一列出现过2次。
- (2, Qu3)的值是1，说明2这个数字在Qu3这一列出现过1次。
