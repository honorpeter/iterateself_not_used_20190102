---
title: 01 以文本格式读取和写入数据
toc: true
date: 2018-08-21 18:16:23
---


input 和 output 大多可以分为几类：

- 读取文本文件或其他一些存储在磁盘上的格式，
- 从数据库加载数据，
- 利用 web API 来获取网络资源。

# 6.1 Reading and Writing Data in Text Format (以文本格式读取和写入数据)

pandas 有很多用来读取表格式数据作为 dataframe 的函数，下面列出来一些。其中read_csv 和 read_tabel 是最经常用到的：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180708/l3ag1fejgC.png?imageslim)


![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180708/ebEKL3f3Fh.png?imageslim)

这里我们给出这些函数的大致功能，就是把 test data 变为 dataframe 。这些函数的一些可选参数有以下几类：

- Indexing（索引）

能把返回的一列或多列作为一个dataframe。另外也可以选择从文件中获取列名或完全不获取列名


- Type inference and data conversion ( 类型推测和数据转换 )

这个包括用户自己定义的转换类型和缺失值转换


- Datetime parsing（日期解析）

包含整合能力，可以把多列中的时间信息整合为一列


- Iterating（迭代）

支持对比较大的文件进行迭代 <span style="color:red;">什么意思？</span>


- Unclean data issues（未清洗的数据问题）

跳过行或柱脚，评论，或其他一些小东西，比如 csv 中的逗号

因为现实中的数据非常messy（杂乱），所以有一些数据加载函数（特别是 read_csv ）的参数也变得越来越多。对于众多参数感觉不知所措是正常的（read_csv有超过50个参数 <span style="color:red;">这么多吗？</span>）。具体的可以去看pandas官网给出的例子。

一些函数，比如 pandas.read_csv 实现 type inference，因为 column data type 不是数据类型的一种。这意味着我们没有必要指定哪些 columns 是数值，哪些是整数，哪些是字符串。其他一些数据格式，比如HDF5，数据类型是在格式里的。<span style="color:red;">HDF5 是什么？</span>

先来一个CSV文件热热身（CSV文件指的是用逗号隔开数据的文件）：


```python
!cat ../examples/ex1.csv
```

```text
a,b,c,d,message
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo
```

cat 是 unix 下的一个 shell command。如果是用 windows，用 type 代替cat <span style="color:red;">可行吗？好像type不行？确认下。</span>


```python
import pandas as pd
```


```python
df = pd.read_csv('../examples/ex1.csv')
df
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>hello</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>



我们也可以用 read_table 来指定分隔符：


```python
pd.read_table('../examples/ex1.csv', sep=',')
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>hello</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>



一个文件不会总是有header row(页首行)，考虑下面的文件：


```python
!cat ../examples/ex2.csv
```

```text
 1,2,3,4,hello
 5,6,7,8,world
 9,10,11,12,foo
 ```


读取这样的文件，设定 column name:


```python
pd.read_csv('../examples/ex2.csv', header=None)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>hello</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>


也可以自己设定 headers ：

```python
pd.read_csv('../examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>hello</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>



如果想要从多列从构建一个 hierarchical index (阶层型索引)，传入一个包含列名的list：<span style="color:red;">这种阶层型索引一般什么时候会用到？</span>


```python
!cat ../examples/csv_mindex.csv
```

```text
key1,key2,value1,value2
one,a,1,2
one,b,3,4
one,c,5,6
one,d,7,8
two,a,9,10
two,b,11,12
two,c,13,14
two,d,15,16
```



```python
parsed = pd.read_csv('../examples/csv_mindex.csv',
                     index_col=['key1', 'key2'])
parsed
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>value1</th>
      <th>value2</th>
    </tr>
    <tr>
      <th>key1</th>
      <th>key2</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="4" valign="top">one</th>
      <th>a</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>b</th>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>c</th>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>d</th>
      <td>7</td>
      <td>8</td>
    </tr>
    <tr>
      <th rowspan="4" valign="top">two</th>
      <th>a</th>
      <td>9</td>
      <td>10</td>
    </tr>
    <tr>
      <th>b</th>
      <td>11</td>
      <td>12</td>
    </tr>
    <tr>
      <th>c</th>
      <td>13</td>
      <td>14</td>
    </tr>
    <tr>
      <th>d</th>
      <td>15</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>



在一些情况下，一个table可能没有固定的分隔符，用空格或其他方式来分隔。比如下面这个文件：


```python
list(open('../examples/ex3.txt'))
```




    ['            A         B         C\n',
     'aaa -0.264438 -1.026059 -0.619500\n',
     'bbb  0.927272  0.302904 -0.032399\n',
     'ccc -0.264273 -0.386314 -0.217601\n',
     'ddd -0.871858 -0.348382  1.100491\n']



可以看到区域是通过不同数量的空格来分隔的。这种情况下，可以传入一个正则表达式给read_table来代替分隔符。用正则表达式为`\s+`，我们得到：


```python
result = pd.read_table('../examples/ex3.txt', sep='\s+')
result
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>aaa</th>
      <td>-0.264438</td>
      <td>-1.026059</td>
      <td>-0.619500</td>
    </tr>
    <tr>
      <th>bbb</th>
      <td>0.927272</td>
      <td>0.302904</td>
      <td>-0.032399</td>
    </tr>
    <tr>
      <th>ccc</th>
      <td>-0.264273</td>
      <td>-0.386314</td>
      <td>-0.217601</td>
    </tr>
    <tr>
      <th>ddd</th>
      <td>-0.871858</td>
      <td>-0.348382</td>
      <td>1.100491</td>
    </tr>
  </tbody>
</table>
</div>



因为列名比行的数量少，所以 read_table 推测第一列应该是 dataframe 的 index。

这个解析器功能有很多其他参数能帮你解决遇到文件格式异常的问题（可以见之后的表格）。比如，我们要跳过第一、三、四行，使用 `skiprows` :<span style="color:red;">这个还是很好用的</span>


```python
!cat ../examples/ex4.csv
```

```text
# hey!
a,b,c,d,message
# just wanted to make things more difficult for you
# who reads CSV files with computers, anyway?
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo
```

```python
pd.read_csv('../examples/ex4.csv', skiprows=[0, 2, 3])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>hello</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>



对于缺失值，pandas 使用一些 sentinel value(标记值)来代表，比如NA和NULL：


```python
!cat ../examples/ex5.csv
```

```text
something,a,b,c,d,message
one,1,2,3,4,NA
two,5,6,,8,world
three,9,10,11,12,foo
```

```python
result = pd.read_csv('../examples/ex5.csv')
result
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>something</th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>one</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>two</td>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>three</td>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.isnull(result)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>something</th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>



`na_values` 选项能把我们传入的字符识别为 NA，导入必须是 list：


```python
result = pd.read_csv('../examples/ex5.csv', na_values=['NULL'])
result
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>something</th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>one</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>two</td>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>three</td>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>



我们还可以给不同的column设定不同的缺失值标记符，这样的话需要用到dict：


```python
sentinels = {'message': ['foo', 'NA'],
             'something': ['two']}
# 把message列中的foo和NA识别为NA，把something列中的two识别为NA

pd.read_csv('../examples/ex5.csv', na_values=sentinels)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>something</th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>one</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>three</td>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>12</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



下面是pandas.read_csv和pandas.read_table中一些常用的选项：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1d5DJ1Em9i.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/i9HHEAc3Hd.png?imageslim)

# 1 Reading Text Files in Pieces（读取一部分文本）

对于一些比较大的文件，我们想要一次读取一小部分，或者每次迭代一小部分。在我们看一个比较大的文件前，先设置一下pandas中显示的数量：


```python
pd.options.display.max_rows = 10
```


```python
result = pd.read_csv('../examples/ex6.csv')
result
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>one</th>
      <th>two</th>
      <th>three</th>
      <th>four</th>
      <th>key</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.467976</td>
      <td>-0.038649</td>
      <td>-0.295344</td>
      <td>-1.824726</td>
      <td>L</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.358893</td>
      <td>1.404453</td>
      <td>0.704965</td>
      <td>-0.200638</td>
      <td>B</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.501840</td>
      <td>0.659254</td>
      <td>-0.421691</td>
      <td>-0.057688</td>
      <td>G</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.204886</td>
      <td>1.074134</td>
      <td>1.388361</td>
      <td>-0.982404</td>
      <td>R</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.354628</td>
      <td>-0.133116</td>
      <td>0.283763</td>
      <td>-0.837063</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9995</th>
      <td>2.311896</td>
      <td>-0.417070</td>
      <td>-1.409599</td>
      <td>-0.515821</td>
      <td>L</td>
    </tr>
    <tr>
      <th>9996</th>
      <td>-0.479893</td>
      <td>-0.650419</td>
      <td>0.745152</td>
      <td>-0.646038</td>
      <td>E</td>
    </tr>
    <tr>
      <th>9997</th>
      <td>0.523331</td>
      <td>0.787112</td>
      <td>0.486066</td>
      <td>1.093156</td>
      <td>K</td>
    </tr>
    <tr>
      <th>9998</th>
      <td>-0.362559</td>
      <td>0.598894</td>
      <td>-1.843201</td>
      <td>0.887292</td>
      <td>G</td>
    </tr>
    <tr>
      <th>9999</th>
      <td>-0.096376</td>
      <td>-1.012999</td>
      <td>-0.657431</td>
      <td>-0.573315</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>10000 rows × 5 columns</p>
</div>



如果只是想要读取前几行（不读取整个文件），指定一下nrows:


```python
pd.read_csv('../examples/ex6.csv', nrows=5)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>one</th>
      <th>two</th>
      <th>three</th>
      <th>four</th>
      <th>key</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.467976</td>
      <td>-0.038649</td>
      <td>-0.295344</td>
      <td>-1.824726</td>
      <td>L</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.358893</td>
      <td>1.404453</td>
      <td>0.704965</td>
      <td>-0.200638</td>
      <td>B</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.501840</td>
      <td>0.659254</td>
      <td>-0.421691</td>
      <td>-0.057688</td>
      <td>G</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.204886</td>
      <td>1.074134</td>
      <td>1.388361</td>
      <td>-0.982404</td>
      <td>R</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.354628</td>
      <td>-0.133116</td>
      <td>0.283763</td>
      <td>-0.837063</td>
      <td>Q</td>
    </tr>
  </tbody>
</table>
</div>



读取文件的一部分，可以指定`chunksize`:


```python
chunker = pd.read_csv('../examples/ex6.csv', chunksize=1000)
chunker
```



```text
<pandas.io.parsers.TextFileReader at 0x1121558d0>
```


pandas返回的 TextParser object 能让我们根据chunksize每次迭代文件的一部分。比如，我们想要迭代ex6.csv, 计算key列的值的综合：


```python
chunker = pd.read_csv('../examples/ex6.csv', chunksize=1000)

tot = pd.Series([])
for piece in chunker:
    tot = tot.add(piece['key'].value_counts(), fill_value=0)

tot = tot.sort_values(ascending=False)
tot[:10]
```




    E    368.0
    X    364.0
    L    346.0
    O    343.0
    Q    340.0
    M    338.0
    J    337.0
    F    335.0
    K    334.0
    H    330.0
    dtype: float64



TextParser 有一个 get_chunk 方法，能返回任意大小的数据片段：


```python
chunker = pd.read_csv('../examples/ex6.csv', chunksize=1000)

chunker.get_chunk(10)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>one</th>
      <th>two</th>
      <th>three</th>
      <th>four</th>
      <th>key</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.467976</td>
      <td>-0.038649</td>
      <td>-0.295344</td>
      <td>-1.824726</td>
      <td>L</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.358893</td>
      <td>1.404453</td>
      <td>0.704965</td>
      <td>-0.200638</td>
      <td>B</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.501840</td>
      <td>0.659254</td>
      <td>-0.421691</td>
      <td>-0.057688</td>
      <td>G</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.204886</td>
      <td>1.074134</td>
      <td>1.388361</td>
      <td>-0.982404</td>
      <td>R</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.354628</td>
      <td>-0.133116</td>
      <td>0.283763</td>
      <td>-0.837063</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.817480</td>
      <td>0.742273</td>
      <td>0.419395</td>
      <td>-2.251035</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.776764</td>
      <td>0.935518</td>
      <td>-0.332872</td>
      <td>-1.875641</td>
      <td>U</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.913135</td>
      <td>1.530624</td>
      <td>-0.572657</td>
      <td>0.477252</td>
      <td>K</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.358480</td>
      <td>-0.497572</td>
      <td>-0.367016</td>
      <td>0.507702</td>
      <td>S</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-1.740877</td>
      <td>-1.160417</td>
      <td>-1.637830</td>
      <td>2.172201</td>
      <td>G</td>
    </tr>
  </tbody>
</table>
</div>



# 2 Writing Data to Text Format (写入数据到文本格式)

可以输出位csv格式：


```python
data = pd.read_csv('../examples/ex5.csv')
data
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>something</th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
      <th>d</th>
      <th>message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>one</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>two</td>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8</td>
      <td>world</td>
    </tr>
    <tr>
      <th>2</th>
      <td>three</td>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>12</td>
      <td>foo</td>
    </tr>
  </tbody>
</table>
</div>




```python
data.to_csv('../examples/out.csv')
```


```python
!cat ../examples/out.csv
```


```text
,something,a,b,c,d,message
0,one,1,2,3.0,4,
1,two,5,6,,8,world
2,three,9,10,11.0,12,foo
```


其他一些分隔符也可以使用（使用sys.stdout可以直接打印文本，方便查看效果）：


```python
import sys
data.to_csv(sys.stdout, sep='|')
```

```text
|something|a|b|c|d|message
0|one|1|2|3.0|4|
1|two|5|6||8|world
2|three|9|10|11.0|12|foo
```

缺失值会以空字符串打印出来，我们可以自己设定缺失值的指定符：


```python
data.to_csv(sys.stdout, na_rep='NULL')
```

```
,something,a,b,c,d,message
0,one,1,2,3.0,4,NULL
1,two,5,6,NULL,8,world
2,three,9,10,11.0,12,foo
```

如果不指定，行和列会被自动写入。当然也可以设定为不写入：


```python
data.to_csv(sys.stdout, index=False, header=False)
```
```
one,1,2,3.0,4,
two,5,6,,8,world
three,9,10,11.0,12,foo
```

你可以指定只读取一部分列，并按你选择的顺序读取：


```python
data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])
```

    a,b,c
    1,2,3.0
    5,6,
    9,10,11.0


series也有一个to_csv方法：


```python
import numpy as np
```


```python
dates = pd.date_range('1/1/2000', periods=7)

ts = pd.Series(np.arange(7), index=dates)

ts.to_csv('../examples/tseries.csv')
```


```python
!cat ../examples/tseries.csv
```

    2000-01-01,0

    2000-01-02,1

    2000-01-03,2

    2000-01-04,3

    2000-01-05,4

    2000-01-06,5

    2000-01-07,6



# 3 Working with Delimited Formats

对于大部分磁盘中的表格型数据，用pandas.read_table就能解决。不过，有时候一些人工的处理也是需要的。

当然，有时候，一些格式不正确的行能会把read_table绊倒。为了展示一些基本用法，这里先考虑一个小的CSV文件：


```python
!cat ../examples/ex7.csv
```

    "a","b","c"

    "1","2","3"

    "1","2","3"



对于单个字符的分隔符，可以使用python内建的csv方法。只要给csv.reader一个打开的文件即可：


```python
import csv
f = open('../examples/ex7.csv')
reader = csv.reader(f)
```

迭代这个reader:


```python
for line in reader:
    print(line)
```

    ['a', 'b', 'c']
    ['1', '2', '3']
    ['1', '2', '3']


接下来，我们可以根据自己的需要来处理数据。一步步来，首先，把文件读取成一个list of lines:


```python
with open('../examples/ex7.csv') as f:
    lines = list(csv.reader(f))
```

把lines分成header line和data lines：


```python
header, values = lines[0], lines[1:]
```

然后我们可以用一个字典表达式来构造一个有列的字典，以及用zip(\*values)反转行为列：


```python
data_dict = {h: v for h, v in zip(header, zip(*values))}
data_dict
```




    {'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}




```python
header
```




    ['a', 'b', 'c']




```python
print([x for x in zip(*values)])
```

    [('1', '1'), ('2', '2'), ('3', '3')]


CSV有很多功能。我们可以定义一个新的分隔符格式，比如字符串的引号，行结束时的回车，这里我们利用csv.Dialect来构造一个子类：


```python
class my_dialect(csv.Dialect):
    lineterminator = '\n'
    delimiter = ';'
    quotechar = '"'
    quoting = csv.QUOTE_MINIMAL

f = open('../examples/ex7.csv')
reader = csv.reader(f, dialect=my_dialect)
```


```python
for line in reader:
    print(line)
```

    ['a,"b","c"']
    ['1,"2","3"']
    ['1,"2","3"']


当然，也可以设定一个分隔符参数给csv.reader，而不用单独定义一个子类：


```python
reader = csv.reader(f, delimiter='|')
```


```python
for line in reader:
    print(line)

f.close()
```

下面是一些csv.Dialect的选项：

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/aidE0egACD.png?imageslim)

![mark](http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/ae822Fg6HI.png?imageslim)

对于一些更复杂的文件，比如用多种字符来做分隔符，就不能知网用csv模块来处理了。这种情况下，要先做string的split，或者用re.split

写入的话，可以用csv.write。它可以写入与csv.reader中设定一样的文件：


```python
with open('../examples/mydata.csv', 'w') as f:
    writer = csv.writer(f, dialect=my_dialect)
    writer.writerow(('one', 'two', 'three'))
    writer.writerow(('1', '2', '3'))
    writer.writerow(('4', '5', '6'))
    writer.writerow(('7', '8', '9'))
```


```python
!cat ../examples/mydata.csv
```

    one;two;three

    1;2;3

    4;5;6

    7;8;9



# 4 JSON Data

JSON (short for JavaScript Object Notation)已经是发送HTTP请求的标准数据格式了。这种格式比起表个性的CSV更自由一些：


```python
obj = """
{"name": "Wes",
 "places_lived": ["United States", "Spain", "Germany"],
 "pet": null, "siblings": [{"name": "Scott", "age": 30, "pets": ["Zeus", "Zuko"]},
                           {"name": "Katie", "age": 38,
                            "pets": ["Sixes", "Stache", "Cisco"]}]
}
"""
```

JSON是很接近python代码的，除了他的缺失值为null和一些其他的要求。基本的类型是object(dicts), array(lists), strings, numbers, booleans, and nulls. 所以的key必须是string。有很多读取JSON的库，这里用json，它也是python内建的库。把JSON string变为python格式，用json.loads:


```python
import json
```


```python
result = json.loads(obj)
result
```




    {'name': 'Wes',
     'pet': None,
     'places_lived': ['United States', 'Spain', 'Germany'],
     'siblings': [{'age': 30, 'name': 'Scott', 'pets': ['Zeus', 'Zuko']},
      {'age': 38, 'name': 'Katie', 'pets': ['Sixes', 'Stache', 'Cisco']}]}



使用json.dumps，可以把python object转换为JSON：


```python
asjson = json.dumps(result)
```

如何把JSON转变为DataFrame或其他一些结构呢。可以把a list of dicts（JSON object）传给DataFrame constructor而且可以自己指定传入的部分：


```python
siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])
siblings
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Scott</td>
      <td>30</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Katie</td>
      <td>38</td>
    </tr>
  </tbody>
</table>
</div>



pandas.read_json可以自动把JSON数据转变为series或DataFrame：


```python
!cat ../examples/example.json
```

    [{"a": 1, "b": 2, "c": 3},

     {"a": 4, "b": 5, "c": 6},

     {"a": 7, "b": 8, "c": 9}]



pandas.read_json假设JSON数组中的每一个Object，是表格中的一行：


```python
data = pd.read_json('../examples/example.json')
data
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>a</th>
      <th>b</th>
      <th>c</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>8</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div>



如果想要输出结果为JSON，用to_json方法：


```python
print(data.to_json())
```

    {"a":{"0":1,"1":4,"2":7},"b":{"0":2,"1":5,"2":8},"c":{"0":3,"1":6,"2":9} }



```python
print(data.to_json(orient='records'))
```

    [{"a":1,"b":2,"c":3},{"a":4,"b":5,"c":6},{"a":7,"b":8,"c":9}]


`orient='records'`表示输出的数据结构是 列->值 的形式：

`records : list like [{column -> value}, ... , {column -> value}]`

# 5 XML and HTML: Web Scraping (网络爬取)

python有很多包用来读取和写入HTML和XML格式。比如:lxml, Beautiful Soup, html5lib。其中lxml比较快，其他一些包则能更好的处理一些复杂的HTML和XML文件。

pandas有一个内建的函数，叫read_html, 这个函数利用lxml和Beautiful Soup这样的包来自动解析HTML，变为DataFrame。这里我们必须要先下载这些包才能使用read_html:


```python
conda install lxml
pip install beautifulsoup4 html5lib
```

pandas.read_html函数有很多额外选项，但是默认会搜索并试图解析含有`<tagble>`tag的表格型数据。结果是a list of dataframe:


```python
tables = pd.read_html('../examples/fdic_failed_bank_list.html')
```


```python
len(tables)
```




    1




```python
tables
```




    [                             Bank Name             City  ST   CERT  \
     0                          Allied Bank         Mulberry  AR     91
     1         The Woodbury Banking Company         Woodbury  GA  11297
     2               First CornerStone Bank  King of Prussia  PA  35312
     3                   Trust Company Bank          Memphis  TN   9956
     4           North Milwaukee State Bank        Milwaukee  WI  20364
     ..                                 ...              ...  ..    ...
     542                 Superior Bank, FSB         Hinsdale  IL  32646
     543                Malta National Bank            Malta  OH   6629
     544    First Alliance Bank & Trust Co.       Manchester  NH  34264
     545  National State Bank of Metropolis       Metropolis  IL   3815
     546                   Bank of Honolulu         Honolulu  HI  21029

                        Acquiring Institution        Closing Date  \
     0                           Today's Bank  September 23, 2016
     1                            United Bank     August 19, 2016
     2    First-Citizens Bank & Trust Company         May 6, 2016
     3             The Bank of Fayette County      April 29, 2016
     4    First-Citizens Bank & Trust Company      March 11, 2016
     ..                                   ...                 ...
     542                Superior Federal, FSB       July 27, 2001
     543                    North Valley Bank         May 3, 2001
     544  Southern New Hampshire Bank & Trust    February 2, 2001
     545              Banterra Bank of Marion   December 14, 2000
     546                   Bank of the Orient    October 13, 2000

               Updated Date
     0    November 17, 2016
     1    November 17, 2016
     2    September 6, 2016
     3    September 6, 2016
     4        June 16, 2016
     ..                 ...
     542    August 19, 2014
     543  November 18, 2002
     544  February 18, 2003
     545     March 17, 2005
     546     March 17, 2005

     [547 rows x 7 columns]]




```python
failures = tables[0]
failures.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Bank Name</th>
      <th>City</th>
      <th>ST</th>
      <th>CERT</th>
      <th>Acquiring Institution</th>
      <th>Closing Date</th>
      <th>Updated Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Allied Bank</td>
      <td>Mulberry</td>
      <td>AR</td>
      <td>91</td>
      <td>Today's Bank</td>
      <td>September 23, 2016</td>
      <td>November 17, 2016</td>
    </tr>
    <tr>
      <th>1</th>
      <td>The Woodbury Banking Company</td>
      <td>Woodbury</td>
      <td>GA</td>
      <td>11297</td>
      <td>United Bank</td>
      <td>August 19, 2016</td>
      <td>November 17, 2016</td>
    </tr>
    <tr>
      <th>2</th>
      <td>First CornerStone Bank</td>
      <td>King of Prussia</td>
      <td>PA</td>
      <td>35312</td>
      <td>First-Citizens Bank &amp; Trust Company</td>
      <td>May 6, 2016</td>
      <td>September 6, 2016</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Trust Company Bank</td>
      <td>Memphis</td>
      <td>TN</td>
      <td>9956</td>
      <td>The Bank of Fayette County</td>
      <td>April 29, 2016</td>
      <td>September 6, 2016</td>
    </tr>
    <tr>
      <th>4</th>
      <td>North Milwaukee State Bank</td>
      <td>Milwaukee</td>
      <td>WI</td>
      <td>20364</td>
      <td>First-Citizens Bank &amp; Trust Company</td>
      <td>March 11, 2016</td>
      <td>June 16, 2016</td>
    </tr>
  </tbody>
</table>
</div>



这里我们做一些数据清洗和分析，比如按年计算bank failure的数量：


```python
close_timestamps = pd.to_datetime(failures['Closing Date'])
```


```python
close_timestamps.dt.year.value_counts()
```




    2010    157
    2009    140
    2011     92
    2012     51
    2008     25
           ...
    2004      4
    2001      4
    2007      3
    2003      3
    2000      2
    Name: Closing Date, dtype: int64



## Parsing XML with lxml.objectify

XML(eXtensible Markup Language)是另一种常见的数据格式，支持阶层型、嵌套的数据。这里我们演示如何用lxml来解析一个XML格式文件。

纽约都会交通局发布了巴士和地铁的时间表。每一个地跌或巴士都有一个不同的文件（比如Performance_NNR.xml对应Metro-North Railroad）:

```
<INDICATOR>
  <INDICATOR_SEQ>373889</INDICATOR_SEQ>
  <PARENT_SEQ></PARENT_SEQ>
  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>
  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>
  <DESCRIPTION>Percent of the time that escalators are operational systemwide. The availability rate is based on physical observations performed the morning of regular business days only. This is a new indicator the agency began reporting in 2009.</DESCRIPTION>
  <PERIOD_YEAR>2011</PERIOD_YEAR>
  <PERIOD_MONTH>12</PERIOD_MONTH>
  <CATEGORY>Service Indicators</CATEGORY>
  <FREQUENCY>M</FREQUENCY>
  <DESIRED_CHANGE>U</DESIRED_CHANGE>
  <INDICATOR_UNIT>%</INDICATOR_UNIT>
  <DECIMAL_PLACES>1</DECIMAL_PLACES>
  <YTD_TARGET>97.00</YTD_TARGET>
  <YTD_ACTUAL></YTD_ACTUAL>
  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>
  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>
</INDICATOR>
```

使用lxml.objectify,我们可以解析文件，通过getroot，得到一个指向XML文件中root node的指针：


```python
from lxml import objectify

path = '../datasets/mta_perf/Performance_MNR.xml'
parsed = objectify.parse(open(path))
root = parsed.getroot()
```

root.INDICATOR 返回一个生成器，每次调用能生成一个`<INDICATOR>`XML元素。每一个记录，我们产生一个dict，tag name(比如YTD_ACTUAL)作为字典的key：


```python
data = []

skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',
               'DESIRED_CHANGE', 'DECIMAL_PLACES']

for elt in root.INDICATOR:
    el_data = {}
    for child in elt.getchildren():
        if child.tag in skip_fields:
            continue
        el_data[child.tag] = child.pyval
    data.append(el_data)
```

然后我们把这个dict变为DataFrame：


```python
perf = pd.DataFrame(data)
perf.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AGENCY_NAME</th>
      <th>CATEGORY</th>
      <th>DESCRIPTION</th>
      <th>FREQUENCY</th>
      <th>INDICATOR_NAME</th>
      <th>INDICATOR_UNIT</th>
      <th>MONTHLY_ACTUAL</th>
      <th>MONTHLY_TARGET</th>
      <th>PERIOD_MONTH</th>
      <th>PERIOD_YEAR</th>
      <th>YTD_ACTUAL</th>
      <th>YTD_TARGET</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Metro-North Railroad</td>
      <td>Service Indicators</td>
      <td>Percent of commuter trains that arrive at thei...</td>
      <td>M</td>
      <td>On-Time Performance (West of Hudson)</td>
      <td>%</td>
      <td>96.9</td>
      <td>95</td>
      <td>1</td>
      <td>2008</td>
      <td>96.9</td>
      <td>95</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Metro-North Railroad</td>
      <td>Service Indicators</td>
      <td>Percent of commuter trains that arrive at thei...</td>
      <td>M</td>
      <td>On-Time Performance (West of Hudson)</td>
      <td>%</td>
      <td>95</td>
      <td>95</td>
      <td>2</td>
      <td>2008</td>
      <td>96</td>
      <td>95</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Metro-North Railroad</td>
      <td>Service Indicators</td>
      <td>Percent of commuter trains that arrive at thei...</td>
      <td>M</td>
      <td>On-Time Performance (West of Hudson)</td>
      <td>%</td>
      <td>96.9</td>
      <td>95</td>
      <td>3</td>
      <td>2008</td>
      <td>96.3</td>
      <td>95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Metro-North Railroad</td>
      <td>Service Indicators</td>
      <td>Percent of commuter trains that arrive at thei...</td>
      <td>M</td>
      <td>On-Time Performance (West of Hudson)</td>
      <td>%</td>
      <td>98.3</td>
      <td>95</td>
      <td>4</td>
      <td>2008</td>
      <td>96.8</td>
      <td>95</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Metro-North Railroad</td>
      <td>Service Indicators</td>
      <td>Percent of commuter trains that arrive at thei...</td>
      <td>M</td>
      <td>On-Time Performance (West of Hudson)</td>
      <td>%</td>
      <td>95.8</td>
      <td>95</td>
      <td>5</td>
      <td>2008</td>
      <td>96.6</td>
      <td>95</td>
    </tr>
  </tbody>
</table>
</div>



XML数据能得到比这个例子更复杂的情况。每个tag都有数据。比如一个而HTML链接，也是一个有效的XML：


```python
from io import StringIO
tag = '<a href="http://www.google.com">Google</a>'
root = objectify.parse(StringIO(tag)).getroot()
```

我们可以访问任何区域的tag(比如href)：


```python
root
```




    <Element a at 0x114860948>




```python
root.get('href')
```




    'http://www.google.com'




```python
root.text
```




    'Google'




```python

```
