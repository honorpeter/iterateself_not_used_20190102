---
title: 08 性能剖析与优化
toc: true
date: 2018-06-27 07:24:33
---
第8章；彳

### 性能剖析与优化

“选择了脚本语言就要忍受其速度”，这句话在某种程度上说明了 Python作为脚本语言 在效率和性能方面的一个不足之处。但这并没有你想象的那么夸张，也不是说你只能坐以 待毙。性能与效率与很多方面息息相关，如软件设计、运行环境、网络带宽、更优化的代码 等。代码优化并不神秘，它建立在软件算法和硬件体系结构等知识层面之上，有一定的方法 可以遵循。本章将着重从代码层面来探讨如何提髙Python的效率和性能，并重点分析一些常 见的优化方法和技巧。

##### 建议79: 了解代码优化的基本原则

代码优化是指在不改变程序运行结果的前提下使得程序运行的效率更髙，优化的代码意 味着运行速度更快或者占有的资源更少。进行代码优化时需要记住以下几点原则。

(1 )优先保证代码是可工作的

Donald Knuth曾说过，过早优化是编程中一切“罪恶”的根源。很多人热衷优化，一开 始写代码就奔着性能这个目标。但事实真相是“让正确的程序更快要比让快速的程序正确容 易得多。”因此优化的前提是代码满足了基本的功能需求，是可工作的。过早地进行优化可 能会忽视对总体性能指标的把握，忽略可移植性、可读性、内聚性等，更何况每个模块甚至 每行优化的代码并不一定能够带来整体运行性能良好，因为性能瓶颈可能出现在意想不到的 地方，如模块与模块之间的交互和通信等，在得到整体视图之前不要主次颠倒。需要说明的 是，这并不是不鼓励你在代码实现的过程中去尝试更优的实现方式，在编码的过程中同样应 该遵循Python的哲学和本书前面章节所提倡的风格与语法，尽量选择更好的算法或者实现。

（2）    权衡优化的代价

优化是有代价的，想解决所有性能问题几乎是不可能的。从代码本身的角度来讲，可能 面临着牺牲时间换空间或者牺牲空间换时间的抉择；从项目的角度来讲，质量、时间和成本 这三者之间“铁三角”关系不会改变，如果性能是权衡质0的一个指标的话，更好的性能意 味着需要更多的时间和人力或者更强大的硬件资源；从用户的角度来看，根据80/20法则， 最终影响用户体验的可能也就是20%的性能问题。因此，优化需要权衡代价，如果在项0时 间紧迫的情况下能够仅仅通过增加硬件资源就解决主要性能问题，不妨选择更强大的部署环 境；或者在已经实现的代码上进行修修补补试图进行优化代码所耗费的精力超过a构的代价 时，重构可能是更好的选择。

（3）    定义性能指标，集中力量解决首要问题

你可能曾经听到过客户这样的声音：我希望这个功能反应更快一点。这很好，起码你明 白优化的目标所在，而不至于像个“无头苍蝇’’ 一样抓不住客户需要的方向而导致最后落个 费力不讨好的结果。但这还不够好，为什么？什么标准才符合更快一点这个说法呢？更快到 底是多快？ “一千个人心中就有一千个哈姆雷特”，我们必须制定出可以衡量快的具体指标， 比如在什么样的运行环境下（如网络速度、硬件资源等）、运行什么样的业务响应时间的范围 是多少秒。这里要着重强调的是：精确，可度量。更快、非常快这些都是描述性的词语，并 不可度景，不同的人有着不同的衡量标准，可能对于一个请求你认为2秒内能够返回结果 已经够快了，但业务人员所理解的够快可能是0.5秒内，偏差由此产生。如果你的客户并不 能提出专业精确的目标，那么相关需求人员或者技术人员也一定要引导和帮助客户（如运用 SMART法则等）最终达成契约。另外，性能优化一定要站在客户和产品本身的角度上分析 而不是开发人员的角度上。为什么？因为客户才是我们服务的主要对象，他们的想法才能代 表最终的需求。比如开发人员可能觉得安装的时间过长而花费不少精力进行优化，但客户真 正关心的可能是在系统上部署一个新的服务的响应时间。因此，在进行优化之前，一定要针 对客户关心的问题进行主次排列，并集中力量解决主要问题。

（4）    不要忽略可读性

优化不能以牺牲代码的可读性，甚至带来更多的副作用为代价。实际应用中经常运行的 代码可能只占一小部分，但几乎所有代码都是需要维护的，因此在代码的可读性、可维护性 以及更优化的性能之间需要权衡。如果优化的结果是使代码变得难以阅读和理解，可能停止 优化或者选择其他替代没计更好。

最后需要说明的是，优化无极限，不要陷人怪圈，什么时候应该优化、什么时候应该停 止优化心里得有谱，性能较优的代码确实很吸引人，但过犹不及。

##### I议80:借助性能优化工具

工欲善其事，必先利其器”，好的工具能够对性能的提升起到非常关键的作用。常见的

性能优化工具有Psyco、Pypy和cPython等。本节我们将简单讨论前两者，cPython性能优化 具体使用见建议90—节。

(1 ) Psyco

Psyco是一个just-in-time的编译器，它能够在不改变源代码的情况下提髙一定的性能, Psyco将操作编澤成部分优化的机器码，其操作分成三个不同的级别，有“运行时’、‘编译时’ 和“虚拟吋”变僮，并根据需要提髙和降低变的级别。运行时变姑只是常规Python解释器 处理的原始字节码和对象结构。一旦Psyco将操作编译成机器码，那么编译时变tt就会在机 器寄存器和可直接访问的内存位®中表示。同时Python能高速缓存已编译的机器码以备以后 重用，这样能节省一点时间。但Psyco也有其缺点，如，其本身运行所占内存较大。2012年 3月12日，Psyco项目主页上宣布Psyco停止维护并正式结束，由Pypy所接替。到结束为止， Psyco也没有提供对Python2.7版本的支持。对Psyco感兴趣的读者可以参考其主页(http:// psyco.sourceforge.net/) 了 解更多信息。

(2) Pypy

Python的动态编译器，是Psyco的后继项目。其目的是，做到Psyco没存做到的动态 编译。Pypy的实现分为两部分：第一部分“用Python实现的Python”，这里虽然是这么说， 但实际上它是使用一个名为RPython的Python子集实现的，Pypy能够将Python代码转成 C、.NET、Java等语言和平台的代码；第二部分Pypy集成了一种编译rPython的即时(JIT) 编译器，和许多编译器、解释器不同，

这种编译器不关心Python代码的同法 分析和语法树，因为它是用Python语 言写的，所以它直接利用Python语言 的Code Object (Python字节码的表示)，

00

2



01



000001

0000^

00001

08^

0001

00 一



也就是说，Pypy直接分析Python代码 所对应的字节码，这些字节码既不是

汁算规模



以字符形式也不是以某种二进制格式    图8-1 Python与Pypy运行时间比较

保存在文件中。如图8-1所示是针对同 一段代码分别使用Python和Pypy运行得到的时间消耗示意图。

从图8-1中可见看出，使用Pypy来编译和运行程序，随着运算规模的扩大，其效率显 著提高。

##### ■议81:利用cProfile定位性能瓶颈

程序运行慢的原因有很多，但真正的原因往往是一两段设计并不那么良好的不起眼的 程序，比如对一系列元素进行自定义的类型转换等。程序性能影响往往符合80/20法则，即

20%的代码的运行时间占用了 80%的总运行时间(实际上，比例要夸张得多，通常是几十行 代码占用了 95%以上的运行时间)，所以如何定位瓶颈所在很有难度，靠经验是很难找出造 成性能瓶颈的代码的。这时候，我们需要一个工具帮忙，下文通过cPrOfile分析相关的独立 模块，基本上解决了定位性能瓶颈问题、

profile是Python的标准库。可以统计程序里每一个函数的运行时间，并且提供了多样 化的报表，而cProfile则是它的C实现版本，剖析过程本身需要消耗的资源更少。所以在 Python 3中，cProfile代替了 profile,成为默认的性能剖析模块。使用cProfile来分析一个程 序很简单，以下面一个程序为例：

def foo ():

sum = 0

for i in range (100): sum += i

return sum

if name == •• main ••: foo ()

现在要用profile分析这个程序。很简单，把if程序块改为如下：

if _name_ == •’_main_••: import cProfile cProfile . run ("foo () "}

我们仅仅是import 了 cProfile这个模块，然后以程序的人口函数名为参数调用了 cProfile.run这个闲数。程序运行的输出如下：

5 function calls in 0.143 CPU seconds    Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)

| 1    | 0.000 | 0.000 | 0.000 | 0.000 | :0 (range)            |
| ---- | ----- | ----- | ----- | ----- | --------------------- |
| 1    | 0.143 | 0.143 | 0.143 | 0.143 | :0 (setprofile)       |
| 1    | 0.000 | 0.000 | 0.000 | 0.000 | <string>:1(?)         |
| 1    | 0.000 | 0.000 | 0.000 | 0.000 | prof1.py:1(foo)       |
| 1    | 0.000 | 0.000 | 0.143 | 0.143 | profile: 0 (foo ())   |
| 0    | 0.000 |       | 0.000 |       | profile: 0 (profiler) |

上面显示了 profl.py里函数调用的情况，根据数据我们可以清楚地看到foo()函数占用 了 100%的运行时间，foo()函数是这个程序里名副其实的热点。

除了用这种方式，cProfile还可以直接用Python解释器调用cProfile模块来剖析Python 程序。如在命令行界面输人如下命令：

python -m cProfile profl.py

产生的输出跟直接修改脚本调用cProfile.runO函数有一样的功效。

cProfile 的统果分为 ncalls、tottime、percall、cumtime、percall、filename:lineno(function) 等若十列，如表8-1所示。

表8-1 cProfile的统计结果以及各项意义

| 统计项                     | 意    义                                   |
| -------------------------- | ------------------------------------------ |
| ncalls                     | 函数的被调用次数                           |
| tottime                    | 函数总计运行时间，不含调用的函数运行时间   |
| percall                    | 困数运行一次的平均时间，等于tottime/ncalls |
| cumtime                    | 函数总计运行时间.含调用的函数运行时间      |
| percall                    | 困数运行一次的平均时间，等于cumtime/ncalls |
| filename: lineno(function) | 函数所在的文件名、函数的行号、函数名       |

通常情况下，cProfile的输出都直接输出到命令行，而且默认是按照文件名排序输出的。 这就给我们造成了障碍，我们有时候希望能够把输出保存到文件，并且能够以各种形式来 查看结果。cProfile简单地支持了一些需求，我们可以在cProfile.runO函数里再提供一个实 参，就是保存输出的文件名。同样，在命令行参数里，我们也可以加多一个参数，用来保存 cProfile的输出。

cProfile解决了我们的对程序执行性能剖析的需求，但还有一个需求：以多种形式査看 报表以便快速定位瓶颈。我们可以通过pstats模块的另一个类Stats来解决。Stats的构造函 数接受一个参数一就是cProfile的输出文件名。Stats提供了对cProfile输出结果进行排序、 输出控制等功能。如我们把前文的程序改为如下：

###### #…略

if _name_ == •’_main_•’： import cProfile

cProfile• run ("foo() ", "prof .txt”} import pstats

p = pstats.Stats("prof.txtu> p.sort_stats("time").print—stats()

引人pstats之后，将cProfile的输出按兩数占用的时间排序，输出如下：

Sun Jan 14 00:03:12 2007 prof.txt

5 function calls in 0.002 CPU seconds

Ordered by: internal time

ncalls tottime percall cumtime percall filename:lineno(function)

| 1    | 0.002 | 0.002 | 0.002 | 0.002 | :0 (setprofile)        |
| ---- | ----- | ----- | ----- | ----- | ---------------------- |
| 1    | 0.000 | 0.000 | 0.002 | 0.002 | profile : 0 (f oo ())  |
| 1    | 0.000 | 0.000 | 0.000 | 0.000 | G:/prof1.py:1 (foo)    |
| 1    | 0.000 | 0.000 | 0.000 | 0.000 | <string>:1 (?)         |
| 1    | 0.000 | 0.000 | 0.000 | 0.000 | :0 (range)             |
| 0    | 0.000 |       | 0.000 |       | profile : 0 (profiler) |

###### Stats有若干个函数，这些函数组合能输出不同的cProfile报表，功能非常强大，如 表8-2所示。下面简单地介绍一下这些函数。

表8-2 Stats函数以及对应作用

| 函    数                             | 函数的作用                              |
| ------------------------------------ | --------------------------------------- |
| strip dirs()                         | 用以除去文件名前名的路径信息            |
| add(filename，[".])                  | 把profile的输出文件加人Stats实例中统计  |
| dumpstals(filcnamc)                  | 把Stats的统汁结果保存到文件             |
| sort slats(kcy,[- • •])              | 最重要的一个函数，用以排序profile的输出 |
| reverse ordcr(>                      | 把Stats实例里的数据反序蜇排             |
| print stats([restriction, •••])      | 把Stats报表输出到stdout                 |
| print callers([restriction,---])     | 输出调用了指定的函数的相关信息          |
| print 一 callees([restriction，...]) | 输出指定的函数凋用过的函数的相关信息    |

###### 这里最重要的函数就是sort_stats和print_stats,通过这两个函数我们几乎可以用适当的 形式浏览所有的信息了。下面来详细介绍一下。

1) SOrt_StatS()接收一个或者多个字符串参数，如time、name等，表明要根据哪一列来 排序。这相当有用，例如我们可以通过用time为key来排序得知最消耗时间的函数；也可以 通过cumtime来排序，获知总消耗吋间最多的函数。这样我们优化的时候就有了针对性，可 以做到事半功倍了。    -

sort stats可接受的参数如表8-3所示。

表8-3 sort stats可接受参数列表

| 参    数   | 参数对应的意义                         |
| ---------- | -------------------------------------- |
| ncalls     | 被调用次数                             |
| cumulative | 函数运行的总时间                       |
| file       | 文件名                                 |
| module     | 模块名                                 |
| pcalls     | 简单调用统计(兼容旧版，未统计递归调用) |
| line       | 行号                                   |
| name       | 函数名                                 |
| nfl        | Name、file、line                       |
| stdnamc    | 标准函数名                             |
| time       | 函数内部运行时间(不计调用子函数的时间) |

2 ) print_stats输出最后一次调用sort_stats之后得到的报表。print_stats有多个可选参数， 用以筛选输出的数据。print_Stats的参数可以是数字也可以是Perl风格的正则表达式。相关 的内容通过其他渠道了解，这里就不详述啦。仅举以下3个例子：

print_stats (n • 1”，”foo: •’)

###### 这个语句表示将stats里的内容取前面的10%,然后再将包含“foo＜这个字符串的结果 输出。

print_stats("foo:• 1")

这个语句表示将stats里的包含“focr”字符串的内容的前10%输出。

print_stats (10)

这个语句表示将stats里前10条数据输出。

实际上，profile输出结果的吋候相当于如下调用了 Stats的函数： p.strip_dirs()•sort_stats(-1).print_stats()

其-中sOrt_Stats函数的参数是-1,这也是为了与旧版本兼容而保留的。sort_stats可以接 受-1、0、1、2 之一，这 4 个数分别对应 “stdname”、“calls”、“time” 和 “cumulative”。 但如果你使用了数字为参数，那么pstats只按照第一个参数进行排序，其他参数将被忽略。

除了编编程接口外，pstats还提供了友好的命令行交互环境，在命令行执行python-m pstats就可以进人交互环境，在交互环境里可以使用read或add指令读人或加载剖分结果文 件，stats指令用以査看报表，callees和callers指令用以查看特定函数的被调用者和调用者。 如图8-2所示是pstats的截图，标识了它的基本使用方法。

I冬1 8-2    pstats输出信息截图

如果我们某天心血来潮，想知道向list里添加一个元素需要多少时间，或者想知道抛出 一个异常需要多少时间，那使用profile就好像用牛刀杀鸡了。这时候一般我们先手动写如下 一段代码：

import time def profile ():

bgn = time.time ()

for i in xrange(100000):

[】.append⑴

return time.time() - bgn print profile ()

为了测定一条语句，写了好几条代码，真的让人汗颜。更好的选择是timeit模块。 timeit除了有非常友好的编程接口，也同样提供了友好的命令行接口。首先来看看编程

接口。timeit模块包含一个类Timer,它的构造函数如下：

class Timer( (stmt=,pass * [, setup=fpass1 [, timer=<timer function〉】】】>

stmt参数是字符串形式的一个代码段，这个代码段将被评测运行时间；setup参数用以 设置stmt的运行环境；timer可以由用户使用自定义精度的计时函数。

timeit.Timer有3个成员函数，简单介绍如下：

timeit( [number=1000000])

timeit()执行一次Timer构造函数中的setup语句之后，就重复执行number次stmt语句， 然后返回总计运行消耗的时间。

repeat ( [repeat=3 [, number=1000000]])

repeat()函数以number为参数调用timeit函数repeat次，并返回总计运行消耗的时间。

print_exc ( [file=None])

print_exc()函数用以代替标准的tracback，原因在于print_exc()会输出错行的源代 码。如：

〉>> t = timeit.Timer(nt = foo()/n;print t")

\>>> t.timeit()

Traceback (most recent call last):

File K<pyshell#12>w/ line 1, in -toplevel-

t.timeit()

File ,fE: / Python27/lib/timeit .pyr\ line 158, in timeit return self.inner (it, self.timer)

File H<timeit-src>,,/ line 6, in inner foo U

NameError: global name 1 foo1 is not defined

在这里NameError有点让人迷惑，foo未定义到底是来自被timeit的那段代码还是调用 timeit的代码本身呢？这个场景就是print_exc()函数的用武之地了。

»> try:

t.timeit ()

except:

t.print_exc()

Traceback (most recent call last):

File ”<pyshell#17>", line 2, in ?

File WE: /Python27/lib/timeit .py11 # line 158, in timeit return self.inner (it, self.timer)

File w<timeit-src>", line 6, in inner t = foo<)

NameError: global name 1 foo* is not defined

可以看到traceback里原来的foo()变成了整行代码t=foo(),这样丰富的信息能够加速定 位错误。

除了可以使用timeit的编程接口外，我们也可以在命令行里使用timeit,非常方便。 python -m timeit ［-n N】［-r N］卜s S］卜t］ ［一c】［一h］ ［statement ."】

其中参数的定义如下：

□    -n N/—number=N, statement 语句执行的次数，

□    -r N/-repeat=N,重复多少次调用timeit(),默认为3,

□    -s S/—setup=S,用以设置statement执行环境的语句，默认为“pass”。

□    汁时函数，除了 Windows平台外默认使用time.time()函数c

□    -c/—clock,计时闲数，Windows平台默认使用time.clock()函数c

□    -vMverbose,输出更大精度的计时数值。

□    -h/-help,简单的使用帮助。

小巧实用的timeit蕴藏了无限的潜能等待你去发掘。如本节开始的例子可以使用一句命 令行命令搞定。

$ python -m timeit ••［】 .append ⑴"

1000000 loops, best of 3: 0.187 usee per loop

##### 旨议82:使用memory_profiler和objgraph剖析内存使用

Python还提供了一些工具可以用来査看内存的使用情况以及追踪内存泄露（如memory, profiler、objgraph、cProfile、PySizer及Heapy等），或者可视化地显示对象之间的引用（如 objgraph）,从而为发现内存问题提供更直接的证据。本节最后我们再来看看memory_profiler 和objgraph这两个工具的使用。

（1 ） memory_profiler

安装 memory_profiler 可以使用命令 easy_install-U memory_profiler 或者 pipinstall-U memory_profiler,也可进行源码安装。需要注意的是，在Windows平台上需要先安装依赖包

psutil。memory』rofiler的使用非常简单，在需要进行内存分析的代码之前用@profile进行装 饰，然后运行命令python-m memory_profiler文件名，便可以输出毎一行代码的内存使用以 及增长情况。

memory_profiler test.py 代码片段如下：

import memory一profiler ^profile

def fibonacci (n):

以代码memory_profiler_test.py为例，输出列分别对应为行号、内存使用情况、内存增 长情况以及行所对应的内容。如下所示：

| Line #   | Mem usage | Increment | Line Contents     |        |              |
| -------- | --------- | --------- | ----------------- | ------ | ------------ |
|          |           |           | @profile          |        |              |
|          | 8.648 MB  | 0.000 MB  | def fibonacci (n) | ••     |              |
|          | 11.500 MB | 2.852 MB  | if n <            | 0:     |              |
|          |           |           |                   | return | -1           |
|          | 11.500 MB | 0.000 MB  | elif n            | <=1:   |              |
|          | 11.500 MB | 0.000 MB  |                   | return | 1            |
|          | 11.500 MB | 0.000 MB  | else:             |        |              |
|          | 11.500 MB | 0.000 MB  |                   | return | fibonacci (n |
| nacci (n | -2)       |           |                   |        |              |

更多关于 memory_profiler 的信息可以参考 <https://pypi.python.org/pypi/memory_profilerc>

(2) Objgraph

Objgraph的安装非常简单，可以使用命令pip install objgraph,或者直接从<https://pypi>. python.org/pypi/objgraph下载进行源码安装3 Objgraph的功能大致可以分为以下3类：

□统计。如objgraph.count(typename[, objects])表示根据传入的参数显示被gc跟踪的对 象的数目；objgraph.show_most_common_types([limit= 10, objects])表示显示常用类型 对应的对象的数目。

□定位和过滤对象。如objgraph.by_type(typename[, objects])表示根据传人的参数显示 被gc跟踪的对象信息；objgraph.at(addr)表示根据给定的地址返回对象。

□遍历和显示对象阳。如 objgraph.show_refs(objs[，max_depth=3, extra_ignore=0, filter=None, too many=10, highlight=None, filename=None, extra info=None, refcounts=False])表示从 对象 objs 开始 示对象引用关系图；objgraph.show_backrcfs(objs[, max_dcpth=3, extra_ ignore=(), filter=None, too many=10, highlight=None, filename=None, extra」nfo=None, refcounts=False])表示显示以objs的引用作为结朿的对象关系图。

更多关于 objgraph 使用的 API 文档参见 [http://mg.pov.lt/objgraph/objgraph.html](http://mg.pov.lt/objgraph/objgraph.html%e3%80%82%e4%b8%8b%e9%9d%a2%e6%9d%a5)[。下面来](http://mg.pov.lt/objgraph/objgraph.html%e3%80%82%e4%b8%8b%e9%9d%a2%e6%9d%a5) 看使用objgraph的两个简单的例子。其中第一个例子生成对象的引用关系图，第二个显示不 同类型对象的数目。

1)生成对象x的引用关系图。生成的关系图如图8-3所示。具体代码如下:

\>>> import objgraph »> x = [.a.,，1\ [2,3】]

list

3 items

图8-3对象x的引用关系图



\>>> ob jgraph. show_ref s ([x] z filename= * test. png *)

2)显示常用类型不同类型对象的数目，限制 输出前3行。代码如下：

»> objgraph•show_mo5t_coinnon_types (limit = 3> wrapper_descriptor    1031

function    975

builtin_function_or_method 615

##### 建议83:努力降低算法复杂度

同一问题可用不同算法解决，而一个算法的优劣将直接影响程序的效率和性能。算法的 评价主要从时间复杂度和空间复杂度来考虑。空间复杂度的分析相对来说要简单，并且在当 前的计界硬件资源发展形势下，对空间复杂度的关注远没有时间复杂度高。因此降低算法的 复杂度主要集中在对其时间复杂度的考量，本章侧重考虑时间复杂度。算法的时间复杂度是 指算法需要消耗的时间资源，常使用大写字母表示。如插人排序的时间复杂度为O(z;2)， 快速排序的最坏运行时间是O(A/2)，但是平均运行时间则是log 77)。同一算法对应的不同 代码实现的性能差异可能仅仅体现在其系数上，但数量级上仍然在同一水平，但不同吋间复 杂度的算法随着计算规模的扩大带来的性能差别则较为明ffl。下面是算法时间复杂度大0的 排序比较：

0(l)<0(log* n)<O{n)<O(n log n)<O(n2)<O(cn)<O(nl)<O(nn)

因此对算法改进的目的是尽量往吋间复杂度较低的O靠近。要降低算法的复杂度，首

先要对算法复杂度进行分析。算法分析建立在一定的假设前提上：即一台给定的计算机执 行每一条指令的时间是确定的，因此，对于获取字典中某个key对应的值，其时间复杂度为 0(1)，査找列表中某个元素，其时间复杂度最优为0(1)，最坏的情况为O⑻。下面的示例中 用于求两个列表交集，即使函数中存在条件分支，虽然if部分运算的时间复杂度为0(1)，但 else部分需要循环遍历两个列表，其时间复杂度为<9(«2)，因此最终的算法复杂度为O(z22)。

def intersectionl(listl,list2): result = {}

if len(listl)<5:    # 时间复杂度为 0(l>

print listl

else:    #时间复杂度为0 (n2)

for item in listl:

if item in list2:

result[item] = True

return result.keys()

需要特别说明，算法的复杂度分析的粒度非常重要，其前提一定是粒度相同的指令执行 时间近似，千万不能将任意一行代码直接当做0(1)进行分析。例如上面的例子中如果有其他 函数再调用intersection 1,纵然在调用函数中只有一行代码，该行代码的时间复杂度仍然要 按照O(n2)计算。另外，算法复杂度分析建立在同一级别语言实现的基础上，如果Python代 码中含有C实现的代码，千万不能将两者混在一起进行评估。关于更多算法分析的思想和方 法，读者可以査看数据结构与算法相关资料。

Python常见数据结构基本操作的时间复杂度如表8-4所示。

表84常见数据结构基本操作的时间复杂度

| 数据结构                         | 操    作                   | 平均时间复杂度     | 最差时间复杂度 |
| -------------------------------- | -------------------------- | ------------------ | -------------- |
| list                             | 笈制                       | 0⑻                 | O(n)           |
| 追加、取元素的値，给某个元素赋值 | 6>(1)                      | 0(1)               |                |
| 插人、刪除某个元素，迭代操作     | O{n)                       | O(n)               |                |
| 切片操作                         | O(k}                       | O(k)               |                |
| set                              | x in                       | 0(1)               |                |
| 并巾                             | (?(len ⑺+len(/))           |                    |                |
| 交s&t                            | O(min(len(s), len(z))      | O(len(j) • len(z)) |                |
|                                  | O(len ⑺)                   |                    |                |
| diet                             | 获取修改元素的值，删除     | 0(1)               | ⑽              |
| 迭代操作                         | O(n)                       | O{n)               |                |
| collcctions.dcquc                | 人列、出列(包括左边出入列) | 0(1)               | 0(1)           |
| 扩大队列                         | O(k)                       | 0(k)               |                |
| 刪除元素                         | 0(n)                       | 0⑻                 |                |

##### 建议84:掌握循环优化的基本技巧

循环的优化应遵循的原则是尽泉减少循环过程中的计算量，多重循环的情形下尽量将内 层的计算提到上一层。

1)减少循环内部的计算。下面两个示例实现的是同一功能，但提倡使用第二种循环 实现，因为第一种循环中d=math.Sqrt(y)位于循环内部，每次循环过程中都会重新计算一 遍，无形中增加了系统开销。测试结果表明，第二种运算的计算速率比第一种运算的速率快 40% 〜60%。

示例一：

for i in range (iter): d=math.sqrt(y) j+=i*d

示例二：

d=math.sqrt(y)

for i in range(iter):

j+=i*d

2)    将显式循环改为隐式循环。假设求等差数列1, 2……，n的和，可以直接通过如下 循环来计算：

sum = 0

for i in xrange(n+1): sum = sum+i

也可以直接写出得到计算结果的值：M…+1)/2。显然直接计算表达式的值效率更髙，程 序中如果有类似的情形，可以将显式循环改为隐式。当然这可能会带来另一个负面影响：牺 牲了代码的可读性。因此这种情况下清晰、恰当的注释是非常必要的。

3)    在循环中尽量引用局部变量。在命名空间中局部变量优先搜索，因此局部变量的查 询会比全局变景要快，当在循环中需要多次引用某一个变量的时候，尽量将其转换为局部变 量。下面的例子中如果使用示例二代替示例一，性能将提高10% ~ 15%。

示例一：

x = [10,34,56,78】 def f(x):

for i in xrange (len(x)):

x[i] = math.sin(x(i])

return x

示例二： def g(x):

loc_sin = math.sin for i in xrange (len (x)):

x[i】=loc一sin(x【i】｝    r

return x

4)关注内层嵌套循环。在多层嵌套循环中，重点关注内层嵌套循环，尽量将内层循环 的什算往上层移。如下面的示例一中，vl[i]在第二层循环for j in range(len(v2))时针对每个i 其值保持不变，因此可以在外层循环中使用临时变量替代而不是每次都重新计算，如示例二 所示。’

示例一：

for i in range(len(vl)):

for j in range (len(v2)):

x ■ vl[i] + v2[j]

示例二：

for i in range(len(vl)):

vli = vl[i]

for j in range (len(v2)):

x = vli + v2【j 】

##### 建议85:使用生成器提高效率

斐波那契数列相信大家都不陌生，这是常见的编程题目，也是很多书籍中喜欢引用 的例子。我们这里也不免落于俗套，就以这个例子开场吧。斐波那契是一个简单的递归 数列，数列满足这样的规律：除了前两个数，任何其他数都可以由其前面两个数相加得 到，即n>2o Python中有多种方法实现这个数列，我们来看其中的 一种。

»> def fab (n):

...    i, a, b = 0, 0, 1

."    foblist =[]

...    while i < n:

...    foblist.append(b)

...    a,b » b,a+b    #不借助中间变货交换两个变i的方法

…    i = i+1

• "    return foblist

\>» print fab (4 )

[1, 1, 2, 3]

想一想，上面的例子有没有更好的实现方法呢？显然有！在介绍具体实现之前我们先来 了解生成器的有关知识。

生成器的语法在Python2.2中就引人 了，但实际应用过程中还是有人不会选择 使用它，特别是有过其他语言基础的，主 要是思维上难以转换过来。生成器的概念 其实非常简单，如果一个函数体中包含 有yield语句，则称为生成器(generator),

Itcrables:包含有getrtcm^O或者」仿_0方法的 數提容器对象





它是一种特殊的迭代器(iterator),也可以 称为可迭代对象(iterable)。可迭代对象、

Generator：包含有yidd语句的函麪， 讲有与迭代器类似的^为



迭代器、生成器这三者之间的关系可以简 单地表示成如图8-4所示的形式。

图8-4可迭代对象、迭代器、生成器三者之间的关系



对生成器的调用会返回一个迭代器，使用next()方法可以获取下一个元素或者抛出 Stoplteration 异常。

»> def mygen(x):

•    ..    for i in range(x):

...    yield i

•    • •

»> d = mygen (1)

»> d    #生成器对象，拥有iter()和next<)方法

〈generator object mygen at 0x005162B0>

\>>> d.next()

0

\>>> d.next()

Traceback (most recent call last):

File "<stdin>", line 1, in <module>

Stoplteration

\>>>

实际上当需要在循环过程中依次处理一个序列中的元素的时候，就应该考虑生成器。当 然，要深人理解生成器，必须透彻理解yield语句。yield语句与return语句相似，当解释器 执行遇到yield的时候，函数会自动返回yield语句之后的表达式的值。不过与return不同的 是，yield语句在返回的同时会保存所有的局部变量以及现场信息，以便在迭代器调用next() 或者Send()方法的时候还原，而不是直接交给垃圾回收器(returnO方法返回后这些信息会被 垃圾回收器处理)。这样就能够保证对生成器的每一次迭代都会返回一个元素，而不是一次 性在内存中生成所有的元素。自Python2.5开始，yield语句变为表达式，可以直接将其值赋 给其他变量，如x=(yieldy)。结合一个例子来看yield语句在生成器函数调用的时候执行状 态，下面的函数代表数列1，-3, 5, -7, 9,……。

\>» def series ():

| print "begin: ••                  |      |
| --------------------------------- | ---- |
| m=l.0; n - 1 print "while begin1' | ①    |
| whiled):                          | ②    |
| print "yield a data" yield m/n    | ③    |
| m = m+2                           | ④    |

n = n* -1 print "end"

上面的代码用状态机可以表示为如图8-5所示的形式。状态机中状态1表示从函数定义 到标注1处的所有语句的集合，状态2表示标注2的语句，状态3表示标注2到3之间的所 有语句的集合，状态4表示从标注4后到结束的语句。

遇到yield语句函数

运行上面的程序你会惊讶地发现，即使while中的条件永远为真，代码也不会陷人无限 循环的状态，而是每调用一次next()方法产生一个数，这样生成器的优势就体现出来了。

\>>> d = series()

»> d.next ()    #第一次调用next执行到状态2,循环条件为1,转到状态3,遇到yield语句返回对

\#应的值并保留现场

begin: while begin yield a data 1-0

»> d.next ()    #之后每次调用d.next (>从状态3开始转向状态4,也就是yield语句之后的第一句

\#语句开始执行，4直接转到2,循环条件永远为真，从而再次转到状态3

end

yield a data -3.0

生成器的优点总体来说有如下几条：

□生成器提供了一种更为便利的产生迭代器的方式，用户一般不需要自己实现iter 和next方法，它默认返回一个迭代器。

□代码更为简洁、优雅。

□充分利用了延迟评估(Lazy evaluation)的特性，仅在需要的时候才产生对应的元素， 而不是一次生成所有的元素，从而节省了内存空间，提髙了效率，理论上无限循环成 为可能而不会导致MemoryError,这在大数据处理的情形下尤为重要。

□使得协同程序更为容易实现。协同程序是有多个进人点，可以挂起恢复的函数，这 基本就是yield的工作方式。Python2.5之后生成器的功能更加完善，加入了 sendG、 clOSe()和thmw()方法。其中sendO不仅可以传递值给yield语句，而且能够恢复生成 器，因此生成器能大大简化协同程序的实现。

现在我们回过头来看看本节开头的例子，使用生成器来实现是不是更为简洁呢？

\>>> def fib(n):

...    a = b =1

for i in range(n): yield a azb = b,a+b

##### 建议86:使用不同的数据结构优化性能

在解决性能问题的时候，如果已经到了非改代码不可的情况，考虑到Python中的査找、 排序常用算法都已经优化到极点(虽然对sort()使用key参数比使用cmp参数畚更髙的性能仍 然值得一提)，那么首先应当想到的是使用不同的数据结构优化性能。

首先来看最常用的数据结构一一list,它的内存管理类似C++的stcLvector，即先预分配一 定数量的“车位”，当预分配的内存用完时，又继续往里插人元素，就会启动新一轮的内存分配。 list对象会根据内存增长算法申清一块更大的内存，然后将原有的所有元素拷贝过去，销毁之前 的内存，再插人新元素。当删除元素时，也是类似，删除后发现已用空间比预分配空间的一半 还少时，list会另外申请一块小内存，再做一次元素拷贝，然后销毁原有的大内存。可见，如果 list对象经常有元素数量的“巨变”，比如膨账、收缩得很频繁，那么应当考虑使用deque。

deque就是双端队列，同时具备栈和队列的特性，能够提供在两端插人和删除时复 杂度为0(1)的操作。相对于list，它最大的优势在于内存管理方面。如果不熟悉C++的 std::deque,那么可以把deque想象为多个list连在一起(仅为比喻，非精确描述)，“像火车 一样，每一节车厢可以载客”，它的每一个“list”也可以存储多个元素。它的优势在插入时， 已有空间已经用完，那么它会申请一个“车厢”来容纳新的元素，并将其与已有的其他“车 膊’串接起来，从而避免元素拷贝；在删除元素时也类似，某个“车厮’空了，就“丢莽’掉， 无需移动元素。所以当出现元素数* “巨变”时，它的性能比list要好上许多倍。

对于list这种序列容器来说，除了 pop(O)和insert(O, v)这种插入操作非常耗时之外，査 找一元素是否在其中，也是＜9(«)的线性复杂度。在C语言中，标准库函数bsearchO能够通 过二分查找算法在有序队列中快速査找是否存在某一元素。在Python中，对保持list对象有 序以及在有序队列中查找元素有非常好的支持，这是通过标准库bisect来实现的。

bisect并没有实现一种新的“数据结构”，其实它是用来维护“有序列表”的一组函数，可以 兼容所有能够随机存取的序列容器，比如list。它可使在有序列表中査找某一元素变得非常简单。

def index(a, x):

i = bisect_left(ar x)

if i != len(a) and a[i] == x:

return i

raise ValueError

保持列表有序需要付出额外的维护工作，但如果业务需要在元素较多的列表中频繁查找 某些元素是否存在或者需要频繁地有序访问这些元素，使用bisect则相当值得。

对于序列容器，除了插入、删除、査找之外，还有一种很常见的需求是就获取其中的极 大值或极小值元素，比如在查找最短路径的A*算法中就需要在Open表中快速找到预估值最 小的元素。这时候，可以使用heapq模块。类似bisect, heapq也是维护列表的一组函数，其 中最先接触的必然是heapify(),它的作用是把一个序列容器转化为一个堆。

\>>> import heapq

\>>> import random

\>>> alist = [random.randint (Q, 100) for i in xrange (10))

\>>> alist

[59, 62, 38, 18, 26, 92, 9, 57, 52, 97】

\>» heapq.heapify (alist)

\>» alist

[9, 18, 38, 52, 26, 92, 59, 57, 62, 97]

可以看到转化为堆后，alist的第一个元素alist[O]是整个列表中最小的元素，heapq将保 证这一点，从而保证从列表中获取最小值元索的时间复杂度是0(1)。

»> heapq.heappop (alist)

9

\>>> alist

[18, 26, 38, 52, 97, 92, 59, 57, 62]

除了通过heapify()函数将一个列表转换为堆之外，也可以通过heappush()、heappop() pR 数插人、删除元素，针对常见的先插人新元素再获取最小元素、先获取最小元素再插人新元 素的需求，还有heappushpop(heap, item)和heapreplace(heap,item)函数可以快速完成。从上 例可以看出，每次元素增减之后序列的变化都很大，可以想象维护堆结构需要付出许多额外 计算，所以千万不要“提前优化”乱用heapq,以免带来性能问题。

除此之前，heapq还有3个通用函数值得介绍，其中rneigeO能够把多个有序列表归并为一个 有序列表(返冋迭代器，不占用内存)，而nlaigestO和nsmallestO类似于C++中的std::nth_elementO, 能够返回无序列表中最大或最小的《个元素，并且性能比sorted(iterable4(ey=key)[:n]要髙。

除了对容器的操作可能会出现性能问题外，容器中存储的元素也有很大的优化空间，这 是因为在很多业务中，容器存储的元素往往是同一类型的，比如都是整数，而且整数的取值 范围也确定，那么就可以使用array优化程序性能。

array实例化的时候需要指定其存储的元素类型，如’c\表示存储的每个人元素都相当 于C语言中的char类型，占用内存大小为1字节。

\>>> import array

»> a = array. array (1 c1 z 1 string *)

»> a

array(*cf, •string*}

»> a(0] = •c1

»> print a

array c\ •ctring*)

从上例可以看出，array对象与str不同，它是可变对象，可以随意修改某一元素的值。

###### 不过它最大的优势在于更小的内容占用。

\>>> import sys »> a

array(1c1r •ctring*)

»> sys .getsizeof (a)

62L

\>>> 1 = list (1cstring•)

\>>> sys.getsizeof(1)

152

###### 看，内存占用只有使用了 list的40%左右，这个优化效果在元素数量巨大的时候会更加 明显。此外，还有性能方面的提升。

»> t = timeit .Timer ("••. join <a) ", na = list (•cstring1) M)

\>>> t.timeit()

0.21462702751159668

»> t = timeit. Timer ("a . tost ring () ", "import array; a=array .array    cstring1) w)

»> t. timeit ()

0.1419069766998291

###### 从容器到字符串的转变可以看出array的性能提升是比较大的，但也不能认为array在什 么方面都有更好的性能。

\>>> t = timeit.Timer(na.reverse()", "import array;a=array.array(•c* z •cstring*)n) »> t. timeit ()

0.15056395530700684

»> t = timeit .Timer (wa. reverse () nr "a - list (* cstring1) n)

\>>> t.timeit()

0.08988785743713379

###### 看，在这里list的性能要好些。所以性能优化一定要根据profiler的剖析结果来进行，经 验往往靠不住，这和“不要提前优化” 一样是性能优化的基本原则。

##### 建议87:充分利用set的优势

###### 假设有这么个需求，希望能找出两个不同的给定目录下相同的文件名。该怎么实现呢？ 一 个可行的方法是列出两个目录下所有的文件名，然后逐一比较找出相同的项。实现代码如下：

import os

def ListFilename (dirname^filesuffix,: filelist =[] os.chdir(dirname) for files in os • listdir (" •"):

if files .endswith (filesuffix):

filelist. append (files)    #找出满足条件的后缀条件的文件加入到列表中

return filelist

filelistA = ListFilename ("C:log")

filelistB - ListFilename ("C:\\temp\\", " .log")

filelistA. sort ()    #对列表进行排序

filelistB.sort ()

samefilelist= [ 1    #用来存效相同文件的列表

for a in filelistA:    #对列表进行梅环比较

for b in filelistB:

if a == b:

samefilelist.append (a)

print samefilelist

示例程序选择的数据结构为列表，首先对列表进行排序，然后进行逐项比较，其算法的 复杂度为其中w分别为两个列表的长度。那么清读者思考一下，有没有更好 的选择呢？我们来了解一下集合(set)的基本知识点。Python中集合是通过Hash算法实现 的无序不重复的元索集。创建集合通过set()方法来实现。

\>>> set < whello") set([.h.,    T, 'o1])

»> a = [1,2,M34' (5,6)]

»> set (a)    #方便地将列表转换为set

set([(5, 6), 1、 2, ,34,])

###### 集合中常见的操作以及对应的吋间复杂度如表8-5所示。

表8-5集合常见操作及时间复杂度

| 操    作                  | 说    明                                                   | 图形表示       | 示    例                                                     | 时间复杂度        |                    |            |
| ------------------------- | ---------------------------------------------------------- | -------------- | ------------------------------------------------------------ | ----------------- | ------------------ | ---------- |
| 平                        | 均                                                         | 最    差       |                                                              |                   |                    |            |
| s.union(t)                | s和f的并集.s U t                                           | CZ®            | »> s.union(t) set([l,2,3,4, 5,6])                            | (?(len(5)+Icn(/)) |                    |            |
| s.intersection(t)         | s和f的交集，                                               |                | »> s.intersection(t)                                         | O(min(            | len(j)，           | 0(len(5) * |
| sC\t                      |                                                            | set([l,2,3,4]) | len(/))                                                      |                   | Ien(O)             |            |
| s.difTerence(t)           | 叉和f的差集， s-t,在J中存在 但在f中不存在的 元索组成的集合 |                | »> s.difference(t) SCt([])»> t.difTerence(s) set([5, 6])     | O(ien ⑺)          |                    |            |
| s.symmctric^difference(t) | 5% S和的并 集减去和*的交 集                                |                | »> s = sct([ 1,2,3]) »>t = sct([3,4,5,6]) >» s.symmetric_difference(t) sct([l,2, 4, 5, 6])»> s.union(t) set([l,2,3,4, 5, 6]) »> s.intersection(t) sct([3])>>> s.union(t)-s. intersection(t)set([l，2,4,5,6]) | (?(len(^))        | O(len(j) * len(')) |            |

对表8-5时间复杂度这列仔细分析会发现，集合操作的复杂度基本为<?(〃)，最差的情况 下吋间复杂度才为O(«A2)。回过头来看本节开头的例子，你是不是会有这么一个想法：如 果能够将对列表的操作改为对集合的操作，性能将会明显提高？那么事实是不是如我们所料 呢？我们先来基于一些基本操作测试一下这两种数据结构在性能上的表现。

###### 1)对list求相同的元素，set求并集。当元素规模为100的吋候测试结果显示，list的耗 时大约为set操作的15倍。

Python -m timeit -n 1000 "【x for x in xrange (100) if x in xrange(60, 100)]"

1000 loops, best of 3: 133 usee per loop

Python -m timeit -n 1000 ’.set (xrange (100) ). intersection (xrange <60, 100) )11

1000 loops, best of 3: 8.99 usee per loop

###### 当元素规模为1000时的测试结果显示，list耗吋大约为set操作的144倍，如表8-6所示。

Python -m timeit -n 1000 "[x for x in xrange (1000) if x in xrange (600' 1000}

1000 loops, best of 3: 9.93 msec per loop

Python -m timeit -n 1000 ,f set (xrange (1000) ) . intersection (xrange (600, 1000""

1000 loopsr best of 3: 68.9 usee per loop

表8-6 list和set在求相同元素操作时的性能比较

| 操    作       | 元素数目 | 时间(usee) |
| -------------- | -------- | ---------- |
| list求相同兀索 | 100      | 133        |
| 1000           | 9930     |            |
| set求交集      | 100      | 8.99       |
| 1000           | 68.9     |            |

###### 1 )向list和set中添加元素，当元素规模为100的时候，list的耗时为set的9倍。

Python -m timeit -s "testset=set ()" -s "for x in xrange (100) : testset.add(x) "for x in xrange (100) : x in testset’.

100000 loops, best of 3: 11.5 usee per loop

Python -m timeit -s ntmp - list () 11 -s "for x in xrange (100): tmp. append (x)11 "for x in xrange(100): x in tmp"

10000 loops, best of 3: 104 usee per loop

###### 当元素规模为1000的时候，list的耗时约为set的89倍，如表8-7所示o

表8-7往list和set中添加元索的情况下性能比较

| 操    作         | 元素数目 | 时间(usee) |
| ---------------- | -------- | ---------- |
| 向set中添加元素  | 100      | 11.5       |
| 1000             | 105      |            |
| 向list中添加元索 | 100      | 104        |
| 1000             | 9410     |            |

Python -m timeit -s "testset=set () •• -s ”for x in xrange (1000) : testset. add(x) •’ Hfor x in xrange (1000) : x in testset"

10000 loops, best of 3: 105 usee per loop

Python -m timeit -s "tmp = list () n -s .’for x in xrange (1000) : tmp• append (x)" "for x in xrange (1000) : x in tmpn

100 loops, best of 3: 9.41 msec per loop

从表8-7所示的测试数据中可以看出，set在某些操作上明显比list更髙效，实际上set 的union、intersection、difference等操作要比list的迭代要快。因此如果涉及求list交集、并 集或者差等问题可以转换为set来操作。因此本节最初的例子将list转换为set再求交集会更 为简洁髙效，可修改为printset(filelistA)&set(filelist)。修改后测试结果表明，即使规模在10 左右，set的效率仍然比list髙了将近3倍(读者可以自行验证)。

##### 建议88:使用multiprocessing克服GIL的缺陷

众所周知，GIL的存在使得Python中的多线程无法充分利用多核的优势来提高性能，这 个问题困扰着很多开发者，也使很多人备受打击。一些人甚至提出质疑要求移去GIL,但由 于种种原因目前还没有明确的迹象表明GIL会在随后的版本中消失。为了能够充分利用多核 优势，Python的专家们提供了另外一个解决方案：多进程。Multiprocessing由此而生，它是 Python中的多进程管理包，在Python2.6版本中引进的，主要用来帮助处理进程的创建以及 它们之间的通信和相互协调。它主要解决了两个问题：一是尽量缩小平台之间的差异，提供 髙层次的API从而使得使用者忽略底层IPC的问题；二是提供对复杂对象的共享支持，支持 本地和远程并发。

类Process是multiprocessing中较为重要的一个类，用丁•创建进程，其构造函数如下：

Process([group [, target [, name [, args [, kwargs]]】]】＞

其中，参数target表示可调用对象；args表示调用对象的位置参数元组；kwargs表示调 用对象的字典；name为进程的名称；group—般设置为None。该类提供的方法与属性基本 上与 threading.Thread 类一致，包括 is_alive()、join([timeout])、run()、start()、terminate()、 daemon (要通过 start()设置)、exitcode、name、pid等。

不同于线程，每个进程都有其独立的地址空间，进程间的数据空间也相互独立，因此进 程之间数据的共享和传递不如线程来得方便。庆幸的是multiprocessing模块中都提供了相应 的机制：如进程间同步操作原语Lock、Event、Condition、Semaphore,传统的管道通信机 制 pipe 以及队列 Queue,用于共享资源的 multiprocessing.Value 和 multiprocessing.Array 以及 Manager 等。

Multiprocessing模块在使用上需要注意以下几个要点：

1)进程之间的通信优先考虑Pipe和Queue，而不是Lock、Event、Condition、Semaphore

等同步原语。进程中的类Queue使用pipe和一些locks、semaphores原语来实现，是进程安 全的。该类的构造函数返回一个进程的共享队列，其支持的方法和线程中的Queue基本类 似，除『方法task_done()和join()是在其子类JoinableQueue中实现的以外。需要注意的是， 由于底层使用pipe来实现，使用Queue进行进程之间的通信的时候，传输的对象必须是可以 序列化的，否则put操作会导致PicklingError。此外，为了提供put方法的超时控制，Queue 并不是直接将对象写到管道中而是先写到一个本地的缓存中，再将其从缓存中放人pipe中， 内部有个专门的线程feeder负责这项工作。由于feeder的存在，Queue还提供了以下特殊方 法来处理进程退出时缓存中仍然存在数据的问题。

□    closeO:表明不再存放数据到queue中。一旦所有缓冲的数据刷新到管道，后台线程 将退出。

□    join_thread():--般在close方法之后使用，它会阻止直到的后台线程退出，确保所有 缓冲区中的数据已经刷新到管道中。

□    canccljoin_thread():需要立即退出当前进程，而无需等待排队的数据刷新到底层的 管道的时候可以使用该方法，表明无需阻止到后台线程的退出。

Multiprocessing中还有个SimpleQueue队列，它是实现了锁机制的pipe,内部去掉了 buffer,但没有提供put和get的超时处理，两个动作都是阻塞的。

除了 multiprocessing.Queue之外，另一种很重要的通信方式是multiprocessing.Pipe。它 的构造闲数为multiprocessing.Pipe([duplex]),其中duplex默认为True，表示为双向管道，否 则为单向。它返回一个Connection对象的组(connl，conn2 )，分别代表管道的两端。Pipe不 支持进程安全，因此当有多个进程同时对管道的一端进行读操作或者写操作的时候可能会导 致数据丢失或者损坏。因此在进程通信的时候，如果是超过2个以上进程，可以使用queue, 但对于两个进程之间的通信而言Pipe性能更快。下面看一个示例：

from multiprocessing import Process, Pipe,Queue import time

def reader_pipe(pipe):

output_p, input_p = pipe    #返回赞道的两端

input_p.close() while True:

try:

msg = output_p. reev ()    ♦从 pipe 中读取消息

except EOFError:

break

def writer_pipe(count/ input_p):

\#写消息到管道中

\#发送消息



\#利用队列来发送消息



for i in xrange(0, count):

input一p•send(i)

def reader_queue(queue): while True:

msg = queue.get ()    #从队列中获取元素

if (msg == 1 DONE1):

break

def writer_queue《count, queue)2

for ii in xrange (0/ count):

queue.put (ii)    #放入消息队列中

queue.put(* DONE 1)

if _name_==•_main_•:

print Mtesting for pipe:M

for count in [10**3, 10**4, 10**5]:

output_p, input—p = Pipe()

reader』=Process(target=reader^pipe, args=((output_p, input_p),)} reader start <)    # 启动进程

output_p.close ()

_start = time.time()

writer_pipe (count，input_p)    # 写消息到管道中

input_p.close()

reader j). join ()    #等待进程处理完毕

print "Sending %s numbers to Pipe () took %s seconds'1 % (count,

(time.time() - _start))

print "testing for queue:"

for count in [10**3, 10**4, 10**5]:

queue = Queue ()    # 利用 queue 进行通信

reader_p = Process(target=reader—queue, args=((queue)," reader一p.daemon = True

reader_p.start()

_start = time.time()

writer_queue (county queue)    # 写消息到 queue 中

reader^p.join()

print "Sending %s numbers to Queue() took %s seconds" % (count, (time.time() - _start))

榆出比较：

testing for pipe:

Sending 1000 numbers to Pipe() took 0.15299987793 seconds Sending 10000 numbers to Pipe() took 0.384999990463 seconds Sending 100000 numbers to Pipe() took 2.09099984169 seconds testing for queue:

Sending 1000 numbers to Queue () took 0.169000148773 seconds Sending 10000 numbers to Queue() took 0.555000066757 seconds Sending 100000 numbers to Queue() took 3.0790002346 seconds

上面的代码分别用来测试两个多线程的情况下使用pipe和queue进行通信发送相同数据 的时候的性能，其中与pipe相关的兩数为readerjpipe()和writer_pipe()，而与queue相关的 主要函数为writer_queue()和reader_queue()。从函数输出可以看出，pipe所消耗的时间较小，

###### 性能更好。

2)尽量避免资源共享。相比于线程，进程之间资源共享的开销较大，因此要尽a避免 资源共享。但如果不可避免，可以通过multiprocessing.Value和multiprocessing.Array或者 multiprocessing.sharedctypes来实现内存共享，也可以通过服务器进程管理器Manager()来实 现数据和状态的共享。这两种方式各有优势，总体来说共享内存的方式更快，效率更髙，但 服务器进程管理器ManagerO使用起来更加方便，并且支持本地和远程内存共享。我们通过 几个例子来看一下各自使用需要注意的问题。

示例一：使用Value进行内存共享。

import time

from multiprocessing import Process, Value

def func(val):    #多个进程同时修改val

for i in range(10):

time.sleep(0.1) val.value += 1

if _name_ == 1_main_1 :

v = Value Ci1, 0)    #使用value来共享内存

processList = [Process (target-func, args=(v,)) for i in range(10)】 for p in processList: p.start()

for p in processList: p.join() print v.value

上面的程序输出是多少？ 100对吗？你可以运行看看。Python官方文档中有个容易让人 迷惑的描述：在 Value 的构造涵数 multiprocessing.Value(typecode_or_type,*args[,lock])中，如 果lock的值为True会创建一个锁对象用于同步访问控制，该值默认为True。因此很多人会 以为Value是进程安全的，实际上要真正控制同步访问，需要实现获取这个锁。因此上面的 例子要保证每次运行都输出100,需要将函数func修改如下：

def func(val):

for i in range(10):

time.sleep(0.1)

with val .get_lock () :    #仍然甭要使用get_lock方法来获取锁对象

val.value += 1

###### 示例二：使用Manager进行内存共享。

import multiprocessing def f (ns):

ns.x.append(1} ns.y.append (*a1) if _name_ == •__main_

manager = multiprocessing.Manager() ns = manager.Namespace() ns.x =[】

\#manager内部包括可变对象



ns.y =[]

print •before process operation:*/ ns p = multiprocessing.Process(target=fz args=(ns,)) p.start()

p. join ()

print •after process operation、ns    # 傪改根本不会生效

本意是希望x=[l], y=[ ‘a’ ],程序输出是不是期望的结果呢？答案是否定的。这乂 是为什么呢？这是因为manager对象仅能传播对一个可变对象本身所做的修改，如有一个 manager.listO对象，管理列表本身的任何更改会传播到所有其他进程。但是，如果容器对象 内部还包括可修改的对象，则内部可修改对象的任何更改都不会传播到其他进程。因此，正 确的处理方式应该是下面这种形式：

import multiprocessing def f(ns,x,y>:

x.    append(1)

y.    append('a 1)

ns.x= x    #将可变对象也作为参数传入

ns.y = y

if _name_ == ■_main_•:

manager = multiprocessing.Manager() ns = manager.Namespace()

ns.x = 【】 ns.y =［】

print 1 before process operation:1r ns

p = multiprocessing.Process(target=f# args=(ns,ns.x,ns.y," p.start () p. join ()

print *after process operation1, ns

3)注意平台之间的差异。由于Linux平台使用fork()函数来创建进程，因此父进程中所 有的资源，如数据结构、打开的文件或者数据库的连接都会在子进程中共享，而Windows平 台中父子进程相对独立，因此为了更好地保持平台的兼容性，最好能够将相关资源对象作为 子进程的构造函数的参数传递进去。因此要避免如下方式：

f = None def child(f):

\# do something if _name == *_main_•:

f = open (filename, mode) p = Process(target=child) p.start ()

p. join()

###### 而推荐使用如下方式:



def child(f): print f



main



_name_ == •_main_■: f = open (filename,mode} p = Process(target=child,args= (f,)} p.start ()

p. join ()



\#将资源对象作为构造函数参数传入



需要注意的是，Linux平台上multiprocessing的实现是基于C库中的fork(),所有子进 程与父进程的数据是完全相同，因此父进程中所有的资源，如数据结构、打开的文件或者数 据库的连接都会在子进程共享。但Windows平台上由于没有fork()函数，父子进程相对独 立，因此为了保持平台的兼容性，最好在脚本中加上“if_name_=“_main_”:”的判断。 这样可以避免有可能出现的RuntimeError或者死锁。

4)尽量避免使用terminate()方式终止进程，并且确保pool.map中传入的参数是可以 序列化的。在下面的例子中，如果直接将一个方法作为参数传人map中，会抛出cPickle. PicklingError异常，这是因为函数和方法是不可序列化的。



class calculate(object): def run(self):

def f(x): return x*x



p - Pool ()

return p.map(fr [1,2,3】)



\#直接传入函数f



cl = calculate () print cl.run ()



\# 抛出 cPickle. PicklingError 异常



###### 一个可行的正确做法如下:



import multiprocessing

def unwrap_self_f<arg, **kwarg):



return calculate.f(*argr **kwarg)



![img](12 1699d828cfe301 3984Python0b8f84912afaae-78.jpg)



class calculate(object): def f(selfr x):



def run(self):



return x*x



p = multiprocessing.Pool()

return p.map(unwrap_self_f, zip([self]*3Z[1,2,3]))



cl = calculate() print cl.run ()



##### 建议89:使用线程池提高效率

我们知道线程的生命周期分为5个状态：创建、就绪、运行、阻塞和终止。自线程创建 到终止，线程便不断在运行、就绪和阻塞这3个状态之间转换直至销毁。而真正占有CPU 的只有运行、创建和销毁这3个状态。一个线程的运行时间由此可以分为3部分：线程的启 动时间(Ts)、线程体的运行时间(Tr)以及线程的销毁时间(Td)。在多线程处理的情景中， 如果线程不能够被重用，就意味着每次创建都需要经过启动、销毁和运行这3个过程。这必 然会增加系统的相应时间，降低效率。而线程体的运行时间Tr不可控制，在这种情况下如何 提髙线程运行的效率呢？线程池便是一个解决方案。

线程池的基本原理如图8-6所示，它通过将事先创建多个能够执行任务的线程放人池 中，所需要执行的任务通常被安排在队列中。通常情况下，需要处理的任务比线程数目要 多，线程执行完当前任务后，会从队列中取下一个任务，直到所有的任务已经完成。

由于线程预先被创建并放人线程池中，同时处理完当前任务之后并不销毁而是被安排处 理下一个任务，因此能够避免多次创建线程，从而节省线程创建和销毁的开销，带来更好的 性能和系统稳定性。线程池技术适合处理突发性大量请求或者需要大量线程来完成任务、但 任务实际处理时间较短的应用场景，它能有效避免由于系统中创建线程过多而导致的系统性 能负荷过大、响应过慢等问题。

在Python中利用线程池有两种解决方案：一是自己实现线程池模式，二是使用线程池模 块。我们先来看一个线程池模式的简单实现。

import Queue,sys,threading import urllib2,os

\#处理request的工作线程

class Worker(threading.Thread):

def _init_( self, workQueue, resultQueue, **kwds): threading.Thread. init ( self, * *kwds )

self.setDaemon( True )

self.workQueue = workQueue self.resultQueue = resultQueue



def run ( self ): while True:



try:

callable r args, kwds = self .workQueue.get (False) # 从队列中取出一个任务



res ■ callable(*argsz **kwds) self.resultQueue.put( res )

except Queue.Empty: break

class WorkerManager:

def _init_( self, num一of_workers=10):

self.workQueue = Queue.Queue() self.resultQueue = Queue.Queue() self.workers =[]

self._recruitThreads( num_of_workers )



\#存放处理结果到队列中



\#线程池管理器

\#请求队列 #输出结果的队列



def _recruitThreads( self, num_of_workers ): for i in range( num一of_workers ):

worker = Worker( self • workQueue, self.resultCXieue )# 创建工作线程 self .workers .append (worker)    # 加入线程队列中



def start (self) :    #启动线程

for w in self.workers:

w.start ()



def wait_for_complete( self): while len(self.workers):

worker = self .workers .pop ()    #从施中取出一个线程处理请求

worker.join()

if worker.isAlive() and not self.workQueue.empty():

self.workers.append( worker )    # 重新加入线程池中

print "All jobs were completed.f,



def add」ob( self, callable, *args, **kwds ):

self .workQueue.put ( (callable，args，kwds) )    #往工作队列中加入请求

def get_result ( self, *args, **kwds ) :    # 获取处理结果

return self.resultQueue.get( *argsz **kwds )



def download」ile (url):

print "begin download**, url urlhandler = urllib2.urlopen(url) fname = os.path.basename(url)•htmlH with open(fnamer ”wb") as f:

while True:

chunk = urlhandler.read(1024)



if not chunk: break f.write(chunk)

urls = ["<http://wiki.python.org/moin/WebProgrammingM>,

"https : / / www. create space . com/3 611970 ••/

••http: //wiki .python .org/moin/Documentation"

]

wm = WorkerManager (2)    禅创建线程地

for i in urls:

wm.add_job ( download」ile, i )    #将所有请求加入队列中

wm.start()

wm.wait_for_complete()

自行实现线程池，需要定义一个Workei•处理工作请求，定义WorkerManage来进行 线程池的管理和创建，它包含一个工作请求列队和执行结果列队，具体的下载工作通过 download_file()方法来实现。

相比自己实现的线程池模型，使用现成的线程池模块往往更简单。Python中线程池模块 的卜载地址为：https://pypi.python.org/pypi/threadpoolo该模块提供了以下基本类和方法。

1 ) threadpool.ThreadPool：线程池类，主要的作用是用来分派任务请求和收集运行结果。 主要有以下方法。

□    _init_(self, num workers, q_size=O, resq_size=O, poll_timeout=5):建立线程池，并启 动对应num_workers的线程；q_size表示任务请求队列的大小，resq_size表示存放运 行结果队列的大小。

□    createWorkers(self, num workers, poll_timeout=5):将 num workers 数量对应的线程加 人线程池中。

□    dismissWorkers(self，num workers, doJoin=Falsc):告诉 num workers 数ft的 I:作线程 在执行完当前任务后退出。

□    joinAllDismissedWorkers(self):在设置为退出的线程上执行 Thread.join。

□    putRequest(self, request, block=True, timeout=None):将工作请求放人队列中。

□    poll(self，b!ock=FaIse)：处理任务队列中新的请求。

□    wait(self):阻塞用于等待所有执行结果。注意当所有执行结果返回后，线程池内部 的线程并没有销毁，而在等待新的任务。因此，wait()之后仍然可以再次调用pool. putRequests()往其中添加任务。

2) threadpool.WorkRequest:包含有具体执行方法的丁•作请求类。

3    )threadpool.WorkerThread:处理任务的工作线程，主要有run()方法以及dismiss()方法。

4    ) makeRequests(callable_，args_list，callback=None，exc_callback=_handle_thread exception)：主要的函数，作用是创建具有相同的执行函数但参数不同的一系列工作请求。

最后看一个例子，将上一节多线程下载的例子改为用线程池来实现。

import urllib2 import os import time import threadpool

def download一file (url):

print "begin download",url urlhandler = urllib2.urlopen(url) fname = os .path .basename <url)+•’.html" with open(fnamer "wb") as f:

while True:

chunk - urlhandler.read(1024) if not chunk: break f.write(chunk)

urls = ["<http://wiki.python.org/moin/WebProgramming>",

"<https://www.createspace.com/3611970>",

•’http: / /wiki .python .org/moin/Document at ion M

]

pool_size = 2

pool = threadpool .ThreadPool (pool_size)    # 创建浅程池，大小为 2

requests = threadpool .makeRequests (download」ile, urls) # 创建工作请求 [pool.putRequest(req) for req in requests J

print "putting request to pool"

pool.putRequest(threadpool.WorkRequest(download file,args=["<http://chrisarndt>. de/projects/threadpool/api/M, ]) )    #将具体的请求故入线程池

pool.putRequest(threadpool.WorkRequest(download」ile,args=("<https://pypi.python>, org/pypi/threadpool"，]))

pool .poll ()    #赴理任务吹列中的新的请求

pool.wait()

print Mdestory all threads before exist”

pool .dismissWorkers (pool_sizer do」oin=True)    # 完成后退出

![img](12 1699d828cfe301 3984Python0b8f84912afaae-81.jpg)



##### I议90:使用C/C++模块扩展提高性能



Python具有良好的可扩展性，利用Python提供的API,如宏、类型、函数等，可以让 Python方便地进行C/C++扩展，从而获得较优的执行性能。所有这些API却包含在Python.h 的头文件中，在编写C代码的时候引人该头文件即可。来看一个简单的扩展例子。

###### 1)先用C实现相关函数：以实现素数判断为例，文件命名为testextend.c。也可以直接 使用C语言实现相关函数功能后再使用Python进行包装。

include "Python.h"

static PyObject *pr_isprime(PyObject *self, PyObject *args){ int n, num;

if (! PyArg^ParseTuple (argsr ”i”，&num) )    # 解析参数

return NULL?

if (num < 1)[

return Py BuildValue ("i", 0) ; #C类型的数据结构转换成Python对象

}

n ■ num - 1;

while (n > 1){

if (num%n == 0) return PyBuildValue(ni", 0);;

return Py^BuildValue("i", 1);

}

static PyMethodDef PrMethods[】={

{•• isPr ime, pr—isprime, METH^VARARGS,

"check if an input number is prime



or not."}.

{NULL, NULL, Q, NULL}

}；

void initpr(void){

(void) Py_InitModule("pr", PrMethods);

}

上面的代码包含以下3部分。

□导出函数：C模块对外暴露的接口函数pr isprime,带有self和args两个参数，其中 参数args中包含了 Python解释器要传递给C函数的所有参数，通常使用函数PyArg_ ParseTuple()来获得这些参数值。

□初始化函数：以便Python解释器能够对模块进行正确的初始化.初始化时要以init开 头，如 initp。

□方法列表:提供给外部的Python程序使用的一个C模块函数名称映射表PrMethodSo 它是一个PyMethodDef结构体，其中成员依次表示方法名、导出函数、参数传递方 式和方法描述。看下面这个例子。

struct PyMethodDef { char* misname; PyCFunction ml_meth;

\#方法名 #导出函数 #参数传递方法 #方法描述



int ml_flags;

char* ml^doc;

参数传递方法一般设置为meth_varargs,如果想传入关键字参数，则可以将其 与METH KEYWORDS进行或运算。若不想接受任何参数，则可以将其设置为METH NOARGS。该结构体必须以{NULL,NULL,0,NULL}所表示的一条空记录来结尾。

2 )编写setup.py脚本。

from distutils.core import setup. Extension

module = Extension(*pr1r sources = [•testextend.c1J)

setup(name ■ 1Pr test/, version = 11.0•z ext_modules = [module]}

3 ）使用python setup.py build进行编译，系统会在当前目录下生成一个build p目录, 里面包含pr.so和pr.o文件，如图8-7所示。

running build running build_ext pailding fpr• extension jcreacing build

creating build/temp. linux-x86_64-2.4

gcc -pthread -fno-stxict-aliasing -DNDEBUG -02 -g -pipe -Wall -Wp,-D一FORTIFY一SOU RCE=2 -fexceptions -fstack-protector —param=ssp-buffer-size=4 -m64 -ratune=genex ic -D 一GNU一SOURCE -1PIC -fPIC -I/usr/lnclude/pychon2.4 *c cescexcend.c -o bulld/c eiup. linux—x36一64—2 • 4/testexcend. o

creating build/llb.Iinux-x36_64*2.4

gcc -pthread -shared build/temp.linux-xS6_64-2.4/testexcend.o -o build/lib.linux -xS6 64-2.4/pr.so

图8-7使用Python进行编译

4 ）将生成的文件pr.so复制到Python的site_packages目录下，或者将pr.so所在目录的 路径添加到sys.path中，就可以使用C扩展的模块了，如图8-8所示。

»> import pr »> dir(pr)

I • _doc_•, ■_file_■ r _name_•, ■ isPrime •】 »> pr . isPrinie (2)

1

图8-8导人编译后的模块

更多关于C模块扩展的内容请读者参考<http://docs.python.Org/2/c-api/index.htmlo>

##### 建议91:使用Cython编写扩展模块

Python-API让大家可以方便地使用C/C++编写扩展模块/从而通过重写应用中的瓶颈 代码获得性能提升。但是，这种方式仍然有几个问题让Pythonistas非常头疼：

1 ）掌握C/C++编程语言、工具链有巨大的学习成本，如果没有这方面的技术积累，就 无法快速编写代码，解决性能瓶颈。

2）即便是C/C++熟手，重写代码也有非常多的工作，比如编写特定数据结构、算法的 C/C+4•版本，费时费力还容易出错。

所以整个Python社区都在努力实现一个“编译器”，它可以把Python代码直接编译成等 价的C/C++代码，从而获得性能提升。通过开发人员的艰苦工作，涌现出了一批这类工具， 如Pyrex、Py2C和Cython等。而从Pyrcx发展而来的Cython是其中的集大成者。

Cython通过给Python代码增加类型声明和直接调用C函数，使得从Python代码中转换 的C代码能够有非常髙的执行效率。它的优势在于它几乎支持全部Python特性，也就是说， 基本上所有的Python代码都是有效的Cython代码，这使得将Cython技术引人项目的成本降 到最低。除此之外，Cython支持使用decorator语法声明类型，甚至支持专门的类型声明文 件，以使原有的Python代码能够继续保持独立，这些特性都使它得到广泛应用，如PyAMF、 PyYAML等库都使用它编写自己的髙效率版本。

安装Cython非常简单，使用pip能够很方便地安装。

pip install —U cython

编译时间有点漫长，稍作等待，Cython就自动安装好了。然后我们可以尝试拿之前的 arithmetic.py尝试一下，执行命令cython arithmetic.py,很快就完成了，但其实生成了一个 arithmetic.c文件，它非常巨大，大概会有两三千行。是的，你没有看错，只有8行有效代码 的arithmetic.py文件生成的C代码有两三千行。它的部分代码（subtract函数对应的代码的一 分部）如下：

/* Python wrapper */

static PyObject *_pyx_pw_10arithmetic_7subtract(PyObject *_pyx_self, PyObject

\* pyx一args, PyObject * pyx一kwds); /*proto*/

static PyMethodDef _pyx一mdef一lOarithmetic一7subtract = {_Pyx一NAMESTR("subtract”)r (PyCFunction)^yxj^30aritJ¥retJ.c_7subtract/ METH一VARARGSIMETH一KEYWORDS, _Pyx一DOCSTR(O)};

static PyObject *_pyx_pw_10arithmetic_7subtract(PyObject * pyx^selfr PyObject

*_pyx—args, PyObject *_pyx一kwds) {

PyObject *_pyxvx = 0;

PyObject *_pyxvy = 0;

int _pyx__lineno « 0;

const char *_pyx」ilename » NULL;

int _pyx clineno - 0;

PyObject *_pyx_r ■ 0;

_Pyx_RefNannyDeclarations Pyx RefNannySetupContext (.’subtract (wrapper) 0);

{

static PyObject **_pyx一pyargnames[] = {&_pyx—n一s_x,&_pyx一n一s_y,0}; PyObject* values[2] = {0,0};

• • •

###### 看不懂？没有关系，机器生成的代码本来就不是为了给人看的，还是把它交给编译器吧。

$ gcc -shared -pthread -fPIC -fwrapv -02 -Wall -fno-strict-aliasing \ -I/usr/include/python2.7 -o arithmetic.so arithmetic.c

又是一阵等待，编译、链接工作完成后，arithinethic.so文件就生成了。这时候可以像 import普通的Python模块一样使用它。

$ python

\>>> import arithmetic »> arithmetic.subtract (2, 1) 1

###### 每一次都需要编译、等待未免麻烦，所以Cython很体贴地提供了无需显式编译的方案: pyximport。只要将原有的Python代码后缀名从.py改为.pyx即可。

$ cp arithmetic.py arithmetic.pyx

$ cd -

$ python

\>» import arithmetic

Traceback (most recent call last):

File    line    1, in <module>

ImportError: No module named arithmetic

\>>> import pyximport; pyximport.install()

(None, 〈pyximport.pyximport.Pyxlmporter object at 0xl0fbd05d0>)

»> import arithmetic

\>» arithmetic._file__

•/Users/apple/.pyxbld/lib.macosx-10.8-x86_64-2.7/arithmetic.so1

从_6^_属性可以看出，这个.pyx文件已经被编译链接为共享库了，pyximport的确 方便啊！掌握了 Cylhon的基本使用方法之后，就可以更进一步学习了。接下来要谈的是如 何通过Cython把原有代码的性能提升许多倍，是的，Cython就是这么快！

在GIS中，经常需要计算地球表面上两点之间的距离。

import math

def great_circle(lonl,latl,lon2,lat2): radius = 3956 tmiles x = math.pi/180.0 a = (90.0-latl)*(x) b = (90.0-lat2)*(x) theta = (lon2-lonl)*(x)

c = math.acos((math.cos(a)*math.cos(b)) + (math.sin(a)*math.sin(b)*math.cos(theta))) return radius*c

这段Python代码的执行效率可以通过timeit来确定。

import timeit

lonl, latl, lon2r lat2 = -72.345, 34.323, -61.823, 54.826 num = 500000

t = timeit.Timer(npl.great_circle% (lonl,latl,lon2,lat2),

’•import piw)

print "Pure python function", t. timeit (num) , "sec•’

执行50万次大概需要：2.2秒，太慢了。接下来尝试使用Cython进行改写，为了避免 一下子代码变化太大，只使用Cython的类型声明“技能”，看看能达到什么效果。    -

import math

def great^circle (float lonl,float latl,float lon2,float lat2): cdef float radius ■ 3956.0

cdef float cdef float cdef float



pi » 3.14159265 x = pi/180.0 a,b,theta,c

a - (90.0-latl)*(x) b ■ (90.0-lat2)*(x) theta = (lon2-lonl)*(x)

c 篇 math.acos((math.cos(a)*math.cos(b)) + (math.sin(a)*math.sin(b)*math.cos(theta))) return radius*c

通过给great circle函数的参数、中间变量增加类型声明，Cython代码看起来跟原有的 Python代码并无很大不同，业务逻辑代码一行没改。使用timeit的测定结果是大概1.8秒, 提速将近二成，说明类型声明对性能提升非常有帮助。这时候，还有一个性能瓶颈需要解 决，那就是：调用的math库是一个Python库，性能较差。解决这个问题，需要用到Cython 的另一个技能：直接调用C函数。

cdef extern from "math.h": float cosf (float theta) float sinf (float theta) float acosf (float theta)

def great_circle (float lonl,float latl,float lon2,float lat2): cdef float radius - 3956.0 cdef float pi = 3.14159265

cdef float x = pi/180.0    .

cdef float a, b, theta, c a = (90.0-latl)*(x) b - (90.0-lat2)*(x) theta - (lon2-lonl)*(x)

c - acosf ( (cosf(a)*cosf(b)) + (sinf (a)*sinf(b)*cosf(theta))) return radius*c

Cython使用cdef extern from语法，将math.h这个C语言库头文件里声明的cofs、sinf、 acosf等函数导人代码中。因为减少了 Python函数调用和调用时产生的类型转换开销，使用 timeit测定这个版本的代码的效率仅需要大概0.4秒的时间，性能提升了 5倍有余。

通过这个例子，可以掌握Cython的两大技能：类型声明和直接调用C函数。只要再进 一步参考Cython的文档，就可以尝试在项目中使用了。比起直接使用C/C++编写扩展模块， 使用Cython的方法方便得多，成本也更低。

除了使用Cython编写扩展模块提升性能之外，Cython也可用来把之前编写的C/C++ 代码封装成.so模块给Python调用(类似boost.python/SWIG的功能)，Cython社区已 经开发了许多自动化工具。
