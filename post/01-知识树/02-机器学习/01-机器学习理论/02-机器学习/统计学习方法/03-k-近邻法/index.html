<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>03 k 近邻法 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="第3章ft近邻法 灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/03-k-%E8%BF%91%E9%82%BB%E6%B3%95/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="03 k 近邻法" />
<meta property="og:description" content="第3章ft近邻法 灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/03-k-%E8%BF%91%E9%82%BB%E6%B3%95/" /><meta property="article:published_time" content="2018-06-26T19:22:05&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-26T19:22:05&#43;00:00"/>
<meta itemprop="name" content="03 k 近邻法">
<meta itemprop="description" content="第3章ft近邻法 灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的">


<meta itemprop="datePublished" content="2018-06-26T19:22:05&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-26T19:22:05&#43;00:00" />
<meta itemprop="wordCount" content="5733">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="03 k 近邻法"/>
<meta name="twitter:description" content="第3章ft近邻法 灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/recent/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/recent/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">03 k 近邻法</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-26 </span>
        
        <span class="more-meta"> 5733 words </span>
        <span class="more-meta"> 12 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#第3章ft近邻法">第3章ft近邻法</a>
<ul>
<li><a href="#3-1-近邻算法">3.1    近邻算法</a></li>
<li><a href="#3-2-a近邻模型">3.2 A近邻模型</a></li>
<li><a href="#p-k-x-i-p-r">P(K*/(x»=i-p(r=/(^))</a></li>
<li><a href="#3-3-a近邻法的实现-似树">3.3 A近邻法的实现：似树</a></li>
<li><a href="#本章概要">本章概要</a></li>
<li><a href="#继续阅读">继续阅读</a></li>
<li><a href="#参考文献">参考文献</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h5 id="第3章ft近邻法">第3章ft近邻法</h5>

<p>灸近邻法(是-nearestneighbor, i-NN)是一种基本分类与回JQ方法.本书只讨 论分类问题中的灸近邻法.近邻法的输入为实例的特征向量，对应于特征空间 的点；输出为实例的类别，可以取多类.*近邻法假设给定一个训练数据集，其 中的实例类别已定.分类时，对新的实例，根据其A个最近邻的训练实例的类别， 通过多数表决等方式进行预测.因此，近邻法不具有显式的学习过程.*近邻 法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型灸 值的选择、距离度量及分类决策规则是*近邻法的三个基本要素.灸近邻法1968年 由Cover和Hart提出.</p>

<p>本章首先叙述近邻算法，然后讨论*近邻法的模型及三个基本要素，最后 讲述A近邻法的一个实现方法一JW树，介绍构造妃树和搜索松树的算法.</p>

<h6 id="3-1-近邻算法">3.1    近邻算法</h6>

<p>先近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数 据集中找到与该实例最邻近的*个实例，这A个实例的多数属于某个类，就把该 输入实例分为这个类.下面先叙述*近邻算法，然后再讨论其细节.</p>

<p>算法3.1 (々近邻法)</p>

<p>输入：训练数据集</p>

<p>^={(Xi»yi)Xx2，y2),—,(xN,yN&rsquo;)}</p>

<p>其中，为实例的特征向量，_W = {cpc2，…，〜}为实例的类别，f = 1，2,…，況；实例特征向量X;</p>

<p>输出：实例;c所属的类</p>

<p>(1)    根据给定的距离度量，在训练集r中找出与x最邻近的*个点，涵盖这 个点的;t的邻域记作M(xJ:</p>

<p>(2)    在M(;r)中根据分类决策规则(如多数表决)决定*的类别</p>

<p>少= argraax [ I{y,=cJ), i = l,2,-,Nt j = \,2,-,K    (3.1)</p>

<p>1</p>

<p>式(3.1)中，J为指示函数，即当时J为1，否则7为0.    ■</p>

<p>k近邻法的特殊情况是* = 1的情形，称为最近邻算法.对于输入的实例点(特</p>

<p>征向量)；C,最近邻法将训练数据集中与最邻近点的类作为X的类.</p>

<p>是近邻法没有显式的学习过程.</p>

<h6 id="3-2-a近邻模型">3.2 A近邻模型</h6>

<p>k近邻法使用的模型实际上对应于对特征空间的划分.模型由三个基本要素—— 距离度量、A值的选择和分类决策规则决定.</p>

<p>3^.1模型</p>

<p>*近邻法中，当训练集、距离度量（如欧氏距离）、值及分类决策规则（如 多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定.这相 当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的 类.这一事实从最近邻算法中可以看得很清楚.</p>

<p>特征空间中，对每个训练实例点*,距离该点比其他点更近的所有点组成一 个区域，叫作单元（cell）.每个训练实例点拥有一个单元，所有训练实例点的单 元构成对特征空间的_个划分.最近邻法将实例&amp;的类y,作为其单元中所有点的 类标记（class label）.这样，每个单元的实例点的类别是确定的.图3.1是二维 特征空间划分的一个例子.</p>

<p>o    x（l）</p>

<p>图3.1    *近邻法的模型对应特征空间的一个划分</p>

<p>3.2.2距离度量</p>

<p>特征空间中两个实例点的距离是两个实例点相似程度的反映.*近邻模型的特 征空间一般是n维实数向量空间R”.使用的距离是欧氏距离,但也可以是其他距离， 如更一般的 Zy距离（2^ distance）或 Minkowski 距离（Minkowski distance）.</p>

<p>设特征空间A•是n维实数向量空间R&rdquo;，x„XjeX, Xl=(^,x}2),-,x^)T, Xj=^,x^,-,x^)\ xp〜的么距离定义为</p>

<p>Lp(x„x}) = ^1 x&lt;° - l^y    (3.2)</p>

<p>这里.当p = 2时，称为欧氏距离(Euclideandistance)，即 i</p>

<p>咕，少⑽</p>

<p>当p = 1时，称为曼哈顿距离(Manhattan distance)，即</p>

<p>A(x(,x7) = Jl^,-xJ/)|    (3.4)</p>

<p>1=1</p>

<p>当p=oo时，它是各个坐标距离的最大值，即</p>

<p>A.(-K,»x/) = max|x&lt;(0 -x^\    (3.5)</p>

<p>图3.2给出了二维空间中p取不同值时，与原点的~距离为1 (~=1)的 点的图形.</p>

<p>下面的例子说明，由不同的距离度量所确定的最近邻点是不同的.</p>

<p>例3.1已知二维空间的3个点$ = (1，1)T，&amp; = (5,1)T，X, = (4,4)T，试求在p</p>

<p>取不同值时，距离下x,的最近邻点</p>

<p>解因为<em>和七只有第二维上值不同，所以p为任何值时，.而 £1(x,,x3) = 6 , I2(x,,x3) = 4.24, Z3(x1&gt;x3) = 3.78, I4(</em>,,j^) = 3.57</p>

<p>于是得到：p等于1或2时，x2是X,的最近邻点：p大于等于3时，x,是*的 最近邻点.    ■</p>

<p>3.2 J A值的选择</p>

<p>k值的选择会对*近邻法的结果产生重大影响.</p>

<p>如果选择较小的&amp;值，就相当于用较小的邻域中的训练实例进行预测，“学 习”的近似误差(approximationerror)会减小，只有与输入实例较近的(相似的) 训练实例才会对预测结果起作用.但缺点是“学习”的估计误差(estimation error) 会增大，预测结果会对近邻的实例点非常敏感［2\如果邻近的实例点恰巧是噪声， 预测就会出错.换句话说，A值的减小就意味着整体模型变得复杂，容易发生过 拟合.</p>

<p>如果选择较大的*值，就相当于用较大邻域中的训练实例进行预测.其优点 是可以减少学习的估计误差.但缺点是学习的近似误差会增大.这时与输入实例 较远的(不相似的)训练实例也会对预测起作用，使预测发生错误.1值的增大 就意味着整体的模型变得简单.</p>

<p>如果A = 那么无论输入实例是什么，都将简单地预测它属于在训练实例 中最多的类.这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不 可取的.</p>

<p>在应用中，*值_般取一个比较小的数值.通常采用交叉验证法来选取最优 的女值.</p>

<p>3.2.4分触策规则</p>

<p>k近邻法中的分类决策规则往往是多数表决，即由输入实例的*个邻近的训 练实例中的多数类决定输入实例的类.</p>

<p>多数表决规则(majority voting rule)有如下解释：如果分类的损失函数为0-1 损失函数，分类函数为</p>

<p>/:R”</p>

<p>那么误分类的概率是</p>

<h6 id="p-k-x-i-p-r">P(K*/(x»=i-p(r=/(^))</h6>

<p>对给定的实例xe    其最近邻的*个训练实例点构成集合y/x).如果涵盖％(x)</p>

<p>的区域的类别是那么误分类率是</p>

<p>要使误分类率最小即经验风险最小，就要使/(乃=9)旱大，所以多数表决 规则等价于经验风险最小化.</p>

<h6 id="3-3-a近邻法的实现-似树">3.3 A近邻法的实现：似树</h6>

<p>实现*近邻法时,主要考虑的问题是如何对训练数据进行快速*近邻捜索.这 点在特征空间的维数大及训练数据容量大时尤其必要.</p>

<p>灸近邻法最简单的实现方法是线性扫描（linear scan）.这时要计算输入实例与 每一个训练实例的距离.当训练集很大时，计算非常耗时，这种方法是不可行的.</p>

<p>为了提髙*近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少 计算距离的次数.具体方法很多，下面介绍其中的树（fa/tree）方法 3.3.1构造W树</p>

<p>kd树是一种对*维空间中的实例点进行存储以便对其进行快速检索的树形 数据结构.紐树是二叉树，表示对A维空间的一个划分（partition）.构造记树相 当于不断地用垂直于坐标轴的超平面将A维空间切分，构成一系列的i维超矩形区 域.fa/树的每个结点对应于一个*维超矩形区域.</p>

<p>构造fc/树的方法如下：构造根结点，使根结点对应于Jfc维空间中包含所有实 例点的超矩形区域；通过下面的递归方法，不断地对A维空间进行切分，生成子结 点.在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确 定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩 形区域切分为左右两个子区域（子结点）：这时，实例被分到两个子区域.这个 过程直到子区域内没有实例时终止（终止时的结点为叶结点）.在此过程中，将 实例保存在相应的结点上.</p>

<p>通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位 数（median）®为切分点，这样得到的W树是平衡的.注意，平衡的似树搜索时 的效率未必是最优的.</p>

<p>下面给出构造紀树的算法.</p>

<p>算法3.2 （构造平衡树）</p>

<p>输入：维空间数据集r={x,，ww}，</p>

<p>其中七二^1），;^2），…，^））7, z = l，2,…，况；</p>

<p>输出：记树.</p>

<p>（1）开始：构造根结点，根结点对应于包含r的龙维空间的超矩形区域.</p>

<p>选择x（1＞为坐标轴，以r中所有实例的坐标的中位数为切分点，将根结点 对应的超矩形区域切分为两个子区域.切分由通过切分点并与坐标轴x（1＞垂直的 超平面实现.</p>

<p>①    W树是存储*■维空间数据的树结构，这里的々与龙近邻法的A•意义不同，为了与习质一致，本书仍用虹树的名称.</p>

<p>②    一组败据按大小顾序排列起来，处在中间位a的一个数或最中间两个数的平均值.</p>

<p>由根结点生成深度为1的左、右子结点：左子结点对应坐标/^小于切分点 的子区域，右子结点对应于坐标#＞大于切分点的子区域.</p>

<p>将落在切分超平面上的实例点保存在根结点.</p>

<p>〈2)重复：对深度为_/的结点，选择;^为切分的坐标轴，/ = y(mod幻+1, 以该结点的区域中所有实例的;^坐标的中位数为切分点，将该结点对应的超 矩形区域切分为两个子区域.切分由通过切分点并与坐标轴垂直的超平面 实现.</p>

<p>由该结点生成深度为_/ + 1的左、右子结点：左子结点对应坐标/0小于切分 点的子区域，右子结点对应坐标^＞大于切分点的子区域.</p>

<p>将落在切分超平面上的实例点保存在该结点.</p>

<p>(3)直到两个子区域没有实例存在时停止.从而形成fa/树的区域划分.■</p>

<p>例3.2给定一个二维空间的数据集：</p>

<p>r = {(2,3)(5,4)(9,6)(4,7)(8,1)(7,2/}</p>

<p>构造一个平衡fo/树®.</p>

<p>解根结点对应包含数据集r的矩形，选择x(1)轴，6个数据点的X(1)坐标的</p>

<p>图3.3特征空间划分</p>

<p>33.2搜索W树</p>

<p>下面介绍如何利用fe/树进行A近邻搜索.可以看到，利用记树可以省去对</p>

<p>大部分数据点的搜索，从而减少搜索的计算量.这里以最近邻为例加以叙述，同 样的方法可以应用到*近邻.</p>

<p>给定一个目标点，搜索其最近邻.首先找到包含目标点的叶结点；然后从该叶 结点出发，依次回退到父结点；不断査找与目标点最邻近的结点，当确定不可能存 在更近的结点时终止.这样搜索就被限制在空间的局部区域上，效率大为提髙.</p>

<p>包含目标点的叶结点对应包含目标点的最小超矩形区域.以此叶结点的实例 点作为当前最近点.目标点的最近邻一定在以目标点为中心并通过当前最近点的 超球体的内部(参阅图3.8).然后返回当前结点的父结点，如果父结点的另一子 结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例 点.如果存在这样的点，将此点作为新的当前最近点.算法转到更上一级的父结 点，继续上述过程.如果父结点的另一子结点的超矩形区域与超球体不相交，或 不存在比当前最近点更近的点，则停止搜索.</p>

<p>下面叙述用松树的最近邻捜索算法.</p>

<p>算法3«3 (用/W树的最近邻搜索)</p>

<p>输入：已构造的fc/树；目标点</p>

<p>输出：x的最近邻.</p>

<p>(1)    在fc/树中找出包含目标点的叶结点：从根结点出发，递归地向下访问 fa/树.若目标点;V当前维的坐标小于切分点的坐标，则移动到左子结点，否则移 动到右子结点.直到子结点为叶结点为止.</p>

<p>(2)    以此叶结点为“当前最近点</p>

<p>(3)    递归地向上回退，在每个结点进行以下操作：</p>

<p>(a)    如果该结点保存的实例点比当前最近点與离目标点更近，则以该实例点 为“当前最近点”.</p>

<p>(b)    当前最近点一定存在于该结点一个子结点对应的区域.检查该子结点的 父结点的另一子结点对应的区域是否有更近的点.具体地，检査另一子结点对应 的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超 球体相交.</p>

<p>如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动 到另~个子结点.接着，递归地进行最近邻搜索；</p>

<p>如果不相交，向上回退.</p>

<p>（4）当回退到根结点时，搜索结束.最后的“当前最近点”即为的最近 邻点.    ■</p>

<p>如果实例点是随机分布的，紀树搜索的平均计算复杂度是，这里况 是训练实例数.奴树更适用于训练实例数远大于空间维数时的ft近邻捜索.当空 间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描.</p>

<p>下面通过一个例题来说明搜索方法.</p>

<p>例3.3给定一个如图3.5所示的似树，根结点为W，其子结点为5 , C等.树 上共存储7个实例点；另有一个输入目标实例点S,求5的最近邻.</p>

<p>解首先在fc/树中找到包含点S的叶结点D （图中的右下区域），以点Z）作为 近似最近邻.真正最近邻~定在以点S为中心通过点D的圆的内部.然后返回结 点Z）的父结点在结点5的另一子结点F的区域内搜索最近邻.结点F的区域 与圆不相交，不可能有最近邻点.继续返回上一级父结点J，在结点d的另一子结 点C的区域内搜索最近邻.结点C的区域与圆相交;该区域在圆内的实例点有点£， 点£:比点D更近，成为新的最近邻近似.最后得到点£是点5的最近邻.    ■</p>

<h6 id="本章概要">本章概要</h6>

<p>1.女近邻法是基本且简单的分类与回归方法.*近邻法的基本做法是：对给 定的训练实例点和输入实例点，首先确定输入实例点的个最近邻训练实例点，然</p>

<p>后利用这A个训练实例点的类的多数来预测输入实例点的类.</p>

<p>\2.    *近邻模型对应于基于训练数据集对特征空间的一个划分.*近邻法中， 当训练集、距离度量、it值及分类决策规则确定后，其结果唯一确定.</p>

<p>\3.    Jfc近邻法三要素：距离度量、&amp;值的选择和分类决策规则.常用的距离度 量是欧氏距离及更一般的距离.A值小时，*近邻模型更复杂：A值大时，是近 邻模型更简单.A值的选择反映了对近似误差与估计误差之间的权衡，通常由交 叉验证选择最优的常用的分类决策规则是多数表决，对应于经验风险最小化.</p>

<p>\4.    女近邻法的实现需要考虑如何快速搜索*个最近邻点.fa/树是一种便于 对*维空间中的数据进行快速检索的数据结构.树是二叉树，表示对A维空间 的一个划分，其每个结点对应于A维空间划分中的一个超矩形区域.利用紀树可 以省去对大部分数据点的搜索，从而减少搜索的计算量.</p>

<h6 id="继续阅读">继续阅读</h6>

<p>k近邻法由Cover与Hart提出近邻法相关的理论在文献［2,3］中己有论 述.*近邻法的扩展可参考文献［4］. W树及其他快速搜索算法可参见文献［5］.关 于走近邻法的介绍可参考文献［2］.</p>

<p>习 题</p>

<p>3.1参照图3.1，在二维空间中给出实例点，画出*为1和2时的灸近邻法构成的 空间划分，并对其进行比较，体会女值选择与模型复杂度及预测准确率的关系.</p>

<p>3.2利用例题3.2构造的奴树求点* = (3,4.5/的最近邻点.</p>

<p>3.3参照算法3.3,写出输出为的是近邻的算法.</p>

<h6 id="参考文献">参考文献</h6>

<p>[1]    Cover T, HartP. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 1967</p>

<p>[2]    Hastie T, TibshiraniR, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2001 (中译本：统计学习基础一数据挖掘、推理与预測.范明，柴玉梅， 昝红英等译.北京：电子工业出版社，2004)</p>

<p>[3]    Friedman J. Flexible metric nearest neighbor classification. Technical Rqx)rt, 1994</p>

<p>[4]    Weinberger KQ, Blitzer J, Saul LK. Distance metric learning for large margin nearest neighbor classification. In： Proceedings of the NIPS. 2005</p>

<p>[5]    Samet H. The Design and Analysis of Spatial Data Structures. Reading, MA: Addison-Wesley, 1990</p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/02-%E6%84%9F%E7%9F%A5%E6%9C%BA/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">02 感知机</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/05-%E5%86%B3%E7%AD%96%E6%A0%91/">
            <span class="next-text nav-default">05 决策树</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
