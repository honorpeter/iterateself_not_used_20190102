<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>05 机器学习基础 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第五章 机器学习基础 深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="05 机器学习基础" />
<meta property="og:description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第五章 机器学习基础 深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" /><meta property="article:published_time" content="2018-06-26T19:35:09&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-26T19:35:09&#43;00:00"/>
<meta itemprop="name" content="05 机器学习基础">
<meta itemprop="description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第五章 机器学习基础 深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的">


<meta itemprop="datePublished" content="2018-06-26T19:35:09&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-26T19:35:09&#43;00:00" />
<meta itemprop="wordCount" content="29021">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="05 机器学习基础"/>
<meta name="twitter:description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第五章 机器学习基础 深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">iterate self</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">iterate self</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">05 机器学习基础</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-26 </span>
        
        <span class="more-meta"> 29021 words </span>
        <span class="more-meta"> 58 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="original">ORIGINAL</h1>

<ol>
<li>《深度学习》Ian Goodfellow</li>
</ol>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>第五章 机器学习基础
深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的理解。本章将探讨贯穿本书其余部分的一些机器学习重要 原理。我们建议新手读者或是希望更全面了解的读者参考一些更全面覆盖基础知识 的机器学习参考书，例如 Murphy (2012) 或者 Bishop (2006)。如果你已经熟知机器 学习，可以跳过前面的部分，前往第5.11节。第5.11节涵盖了一些传统机器学习技 术观点，这些技术对深度学习的发展有着深远影响。</p>

<p>首先，我们将介绍学习算法的定义，并介绍一个简单的示例：线性回归算法。接</p>

<p>下来，我们会探讨拟合训练数据与寻找能够泛化到新数据的模式存在哪些不同的挑</p>

<p>战。大部分机器学习算法都有超参数(必须在学习算法外设定)；我们将探讨如何使</p>

<p>用额外的数据设置超参数。机器学习本质上属于应用统计学，更多地关注于如何用</p>

<p>计算机统计地估计复杂函数，不太关注为这些函数提供置信区间；因此我们会探讨</p>

<p>两种统计学的主要方法：频率派估计和贝叶斯推断。大部分机器学习算法可以分成监</p>

<p>督学习和无监督学习两类；我们将探讨不同的分类，并为每类提供一些简单的机器</p>

<p>学习算法作为示例。大部分深度学习算法都是基于被称为随机梯度下降的算法求解</p>

<p>的。我们将介绍如何组合不同的算法部分，例如优化算法、代价函数、模型和数据</p>

<p>集，来建立一个机器学习算法。最后在第5.11节，我们会介绍一些限制传统机器学</p>

<p>习泛化能力的因素。这些挑战促进了解决这些问题的深度学习算法的发展。</p>

<p>5.1 学习算法
机器学习算法是一种能够从数据中学习的算法。然而，我们所谓的 ‘‘学习&rdquo; 是什</p>

<p>么意思呢？ Mitchell (1997)提供了一个简洁的定义：“对于某类任务T和性能度量 P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任 87</p>

<p>务T上由性能度量P衡量的性能有所提升。”经验E，任务T和性能度量P的定</p>

<p>义范围非常宽广，在本书中我们并不会试图去解释这些定义的具体意义。相反，我</p>

<p>们会在接下来的章节中提供直观的解释和示例来介绍不同的任务、性能度量和经验</p>

<p>这些将被用来构建机器学习算法。</p>

<p>5.1.1 任务 T
机器学习可以让我们解决一些人为设计和使用确定性程序很难解决的问题。从</p>

<p>科学和哲学的角度来看，机器学习受到关注是因为提高我们对机器学习的认识需要</p>

<p>提高我们对智能背后原理的理解。</p>

<p>从 ‘‘任务&rdquo; 的相对正式的定义上说，学习过程本身不能算是任务。学习是我们所</p>

<p>谓的获取完成任务的能力。例如，我们的目标是使机器人能够行走，那么行走便是</p>

<p>任务。我们可以编程让机器人学会如何行走，或者可以人工编写特定的指令来指导</p>

<p>机器人如何行走。</p>

<p>通常机器学习任务定义为机器学习系统应该如何处理样本(example。。样本是 指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征 (feature。的集合。我们通常会将样本表示成一个向量x e Rn，其中向量的每一个元 素 xi 是一个特征。例如，一张图片的特征通常是指这张图片的像素值。</p>

<p>机器学习可以解决很多类型的任务。一些非常常见的机器学习任务列举如下：</p>

<p>•    分类：在这类任务中，计算机程序需要指定某些输入属于 k 类中的哪一类</p>

<p>为了完成这个任务，学习算法通常会返回一个函数f : Rn 4{1，&hellip;，k}。当 y = f(x)时，模型将向量x所代表的输人分类到数字码y所代表的类别。还有 一些其他的分类问题，例如， f 输出的是不同类别的概率分布。分类任务中有 一个任务是对象识别，其中输人是图片(通常由一组像素亮度值表示。，输出 是表示图片物体的数字码。例如， Willow Garage PR2 机器人能像服务员一样 识别不同饮料，并送给点餐的顾客 (Goodfellow et al., 2010)。目前，最好的对 象识别工作正是基于深度学习的 (Krizhevsky et al., 2012a; Ioffe and Szegedy, 2015)。对象识别同时也是计算机识别人脸的基本技术，可用于标记相片合辑中 的人脸 (Taigman et al., 2014)，有助于计算机更自然地与用户交互。</p>

<p>•    输入缺失分类：当输人向量的每个度量不被保证的时候，分类问题将会变得更 有挑战性。为了解决分类任务，学习算法只需要定义一个从输人向量映射到输</p>

<p>出类别的函数。当一些输人可能丢失时，学习算法必须学习一组函数，而不是 单个分类函数。每个函数对应着分类具有不同缺失输人子集的x。这种情况在 医疗诊断中经常出现，因为很多类型的医学测试是昂贵的，对身体有害的。有 效地定义这样一个大集合函数的方法是学习所有相关变量的概率分布，然后通 过边缘化缺失变量来解决分类任务。使用 n 个输人变量，我们现在可以获得每 个可能的缺失输人集合所需的所有 2n 个不同的分类函数，但是计算机程序仅 需要学习一个描述联合概率分布的函数。参见 Goodfellow et al. (2013d) 了解 以这种方式将深度概率模型应用于这类任务的示例。本节中描述的许多其他任 务也可以推广到缺失输人的情况; 缺失输人分类只是机器学习能够解决的问题 的一个示例。</p>

<p>•    回归：在这类任务中，计算机程序需要对给定输人预测数值。为了解决这个任</p>

<p>务，学习算法需要输出函数f : Rn    R。除了返回结果的形式不一样外，这类</p>

<p>问题和分类问题是很像的。这类任务的一个示例是预测投保人的索赔金额(用</p>

<p>于设置保险费)，或者预测证券未来的价格。这类预测也用在算法交易中。</p>

<p>•    转录：这类任务中，机器学习系统观测一些相对非结构化表示的数据，并转 录信息为离散的文本形式。例如，光学字符识别要求计算机程序根据文本图片</p>

<p>返回文字序列(ASCII码或者Unicode码)。谷歌街景以这种方式使用深度学 习处理街道编号(Goodfellow et al., 2014d)。另一个例子是语音识别，计算机 程序输人一段音频波形，输出一序列音频记录中所说的字符或单词 ID 的编码。 深度学习是现代语音识别系统的重要组成部分，被各大公司广泛使用，包括微 软， IBM 和谷歌 (Hinton et al., 2012b)。</p>

<p>•    机器翻译：在机器翻译任务中，输人是一种语言的符号序列，计算机程序必须 将其转化成另一种语言的符号序列。这通常适用于自然语言，如将英语译成 法语。最近，深度学习已经开始在这个任务上产生重要影响 (Sutskever et al., 2014; Bahdanau et al., 2015)。</p>

<p>•    结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构 并且构成输出的这些不同元素间具有重要关系。这是一个很大的范畴，包括上 述转录任务和翻译任务在内的很多其他任务。例如语法分析——映射自然语言 句子到语法结构树，并标记树的节点为动词、名词、副词等等。参考 Collobert (2011) 将深度学习应用到语法分析的示例。另一个例子是图像的像素级分割 将每一个像素分配到特定类别。例如，深度学习可用于标注航拍照片中的道路</p>

<p>位置 (Mnih and Hinton, 2010)。在这些标注型的任务中，输出的结构形式不 需要和输入尽可能相似。例如，在为图片添加描述的任务中，计算机程序观察 到一幅图，输出描述这幅图的自然语言句子 (Kiros et al., 2014a,b; Mao et al., 2014; Vinyals et al., 2015b; Donahue et al., 2014; Karpathy and Li, 2015; Fang et al., 2015; Xu et al., 2015)。这类任务被称为结构化输出任务是因为输出值之 间内部紧密相关。例如，为图片添加标题的程序输出的单词必须组合成一个通 顺的句子。</p>

<p>•    异常检测：在这类任务中，计算机程序在一组事件或对象中筛选，并标记不正 常或非典型的个体。异常检测任务的一个示例是信用卡欺诈检测。通过对你的 购买习惯建模，信用卡公司可以检测到你的卡是否被滥用。如果窃贼窃取你的 信用卡或信用卡信息，窃贼采购物品的分布通常和你的不同。当该卡发生了不 正常的购买行为时，信用卡公司可以尽快冻结该卡以防欺诈。参考 Chandola et al. (2009) 了解欺诈检测方法。</p>

<p>•    合成和采样：在这类任务中，机器学习程序生成一些和训练数据相似的新样本。 通过机器学习，合成和采样可能在媒体应用中非常有用，可以避免艺术家大量 昂贵或者乏味费时的手动工作。例如，视频游戏可以自动生成大型物体或风景</p>

<p>的纹理，而不是让艺术家手动标记每个像素(Luo et al., 2013)。在某些情况下，</p>

<p>我们希望采样或合成过程可以根据给定的输入生成一些特定类型的输出。例如</p>

<p>在语音合成任务中，我们提供书写的句子，要求程序输出这个句子语音的音频</p>

<p>波形。这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的</p>

<p>条件，并且我们明确希望输出有很多变化，这可以使结果看上去更加自然和真</p>

<p>实。</p>

<p>•缺失值填补：在这类任务中，机器学习算法给定一个新样本xeRn，x中某些</p>

<p>元素 xi 缺失。算法必须填补这些缺失值。</p>

<p>•去噪：在这类任务中，机器学习算法的输人是，干净样本xeRn经过未知损 坏过程后得到的损坏样本x e Rn。算法根据损坏后的样本i预测干净的样本 x，或者更一般地预测条件概率分布p(x I x)。</p>

<p>•    密度估计或概率质量函数估计：在密度估计问题中，机器学习算法学习函数 Pmodel : Rn 4 R，其中Pmodel(x)可以解释成样本采样空间的概率密度函数(如 果 x 是连续的)或者概率质量函数(如果 x 是离散的)。要做好这样的任务</p>

<p>(当我们讨论性能度量P时，我们会明确定义任务是什么)，算法需要学习观测</p>

<p>到的数据的结构。算法必须知道什么情况下样本聚集出现，什么情况下不太可</p>

<p>能出现。以上描述的大多数任务都要求学习算法至少能隐式地捕获概率分布的</p>

<p>结构。密度估计可以让我们显式地捕获该分布。原则上，我们可以在该分布上</p>

<p>计算以便解决其他任务。例如，如果我们通过密度估计得到了概率分布 p(x)</p>

<p>我们可以用该分布解决缺失值填补任务。如果 xi 的值是缺失的，但是其他的变</p>

<p>量值x-i已知，那么我们可以得到条件概率分布p(Xi I x-i)。实际情况中，密</p>

<p>度估计并不能够解决所有这类问题，因为在很多情况下 p(x) 是难以计算的。</p>

<p>当然，还有很多其他同类型或其他类型的任务。这里我们列举的任务类型只是</p>

<p>用来介绍机器学习可以做哪些任务，并非严格地定义机器学习任务分类。</p>

<p>5.1.2 性能度量 P
为了评估机器学习算法的能力，我们必须设计其性能的定量度量。通常性能度 量P是特定于系统执行的任务T而言的。</p>

<p>对于诸如分类、缺失输人分类和转录任务，我们通常度量模型的 准确率( accuracy 。。准确率是指该模型输出正确结果的样本比率。我们也可以通过错误率( error rate。得到相同的信息。错误率是指该模型输出错误结果的样本比率。我们通常把错 误率称为 0-1损失的期望。在一个特定的样本上，如果结果是对的，那么 0-1损 失是 0；否则是 1。但是对于密度估计这类任务而言，度量准确率，错误率或者其他 类型的 0-1损失是没有意义的。反之，我们必须使用不同的性能度量，使模型对每 个样本都输出一个连续数值的得分。最常用的方法是输出模型在一些样本上概率对 数的平均值。</p>

<p>通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为这将决 定其在实际应用中的性能。因此，我们使用测试集(test set。数据来评估系统性能， 将其与训练机器学习系统的训练集数据分开。</p>

<p>性能度量的选择或许看上去简单且客观，但是选择一个与系统理想表现对应</p>

<p>的性能度量通常是很难的。</p>

<p>在某些情况下，这是因为很难确定应该度量什么。例如，在执行转录任务时，我</p>

<p>们是应该度量系统转录整个序列的准确率，还是应该用一个更细粒度的指标，对序</p>

<p>列中正确的部分元素以正面评价？在执行回归任务时，我们应该更多地惩罚频繁犯 一些中等错误的系统，还是较少犯错但是犯很大错误的系统？这些设计的选择取决</p>

<p>于应用。</p>

<p>还有一些情况，我们知道应该度量哪些数值，但是度量它们不太现实。这种情</p>

<p>况经常出现在密度估计中。很多最好的概率模型只能隐式地表示概率分布。在许多</p>

<p>这类模型中，计算空间中特定点的概率是不可行的。在这些情况下，我们必须设计</p>

<p>一个仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。</p>

<p>5.1.3 经验 E
根据学习过程中的不同经验，机器学习算法可以大致分类为无监督(unsupervised) 算法和 监督(supervised )算法。</p>

<p>本书中的大部分学习算法可以被理解为在整个数据集(dataset)上获取经验。 数据集是指很多样本组成的集合，如第5.1.1节所定义的。有时我们也将样本称为数 据点( data point)。</p>

<p>Iris (鸾尾花卉)数据集(Fisher, 1936)是统计学家和机器学习研究者使用了很 久的数据集。它是 150 个鸢尾花卉植物不同部分测量结果的集合。每个单独的植物 对应一个样本。每个样本的特征是该植物不同部分的测量结果：萼片长度、萼片宽 度、花瓣长度和花瓣宽度。这个数据集也记录了每个植物属于什么品种，其中共有 三个不同的品种。</p>

<p>无监督学习算法(unsupervised learning algorithm)训练含有很多特征的数据 集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通常要学习生 成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。 还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。</p>

<p>监督学习算法(supervised learning algorithm )训练含有很多特征的数据集，不 过数据集中的样本都有一个标签(label)或目标(target)。例如，Iris数据集注明 了每个鸢尾花卉样本属于什么品种。监督学习算法通过研究 Iris 数据集，学习如何 根据测量结果将样本划分为三个不同品种。</p>

<p>大致说来，无监督学习涉及到观察随机向量x的好几个样本，试图显式或隐式 地学习出概率分布p(x)，或者是该分布一些有意思的性质；而监督学习包含观察随 机向量x及其相关联的值或向量y，然后从x预测y，通常是估计p(y | x)。术语监 督学习(supervised learning )源自这样一个视角，教员或者老师提供目标y给机器</p>

<p>学习系统，指导其应该做什么。在无监督学习中，没有教员或者老师，算法必须学会</p>

<p>在没有指导的情况下理解数据。</p>

<p>无监督学习和监督学习不是严格定义的术语。它们之间界线通常是模糊的。很</p>

<p>多机器学习技术可以用于这两个任务。例如，概率的链式法则表明对于向量xeRn， 联合分布可以分解成</p>

<p>n</p>

<p>p(x) = p(xi I x1, .. .,xi-1).    (5.1)</p>

<p>i=1</p>

<p>该分解意味着我们可以将其拆分成n个监督学习问题，来解决表面上的无监督学习 p(x)。另外，我们求解监督学习问题p(y |x)时，也可以使用传统的无监督学习策略 学习联合分布p(x,y)，然后推断</p>

<p>p(y | x) =</p>

<p>P(x, y) Ey，p(x,y;)</p>

<p>(5.2)</p>

<p>尽管无监督学习和监督学习并非完全没有交集的正式概念，它们确实有助于粗略分</p>

<p>类我们研究机器学习算法时遇到的问题。传统地，人们将回归、分类或者结构化输</p>

<p>出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。</p>

<p>学习范式的其他变种也是有可能的。例如，半监督学习中，一些样本有监督目 标，但其他样本没有。在多实例学习中，样本的整个集合被标记为含有或者不含有 该类的样本，但是集合中单独的样本是没有标记的。参考Kotzias et al. (2015) 了解</p>

<p>最近深度模型进行多实例学习的示例。</p>

<p>有些机器学习算法并不是训练于一个固定的数据集上。例如， 强化学习( reinforcement learning )算法会和环境进行交互，所以学习系统和它的训练过程会有反 馈回路。这类算法超出了本书的范畴。请参考 Sutton and Barto (1998) 或 Bertsekas and Tsitsiklis (1996) 了解强化学习相关知识， Mnih et al. (2013) 介绍了强化学习方 向的深度学习方法。</p>

<p>大部分机器学习算法简单地训练于一个数据集上。数据集可以用很多不同方式</p>

<p>来表示。在所有的情况下，数据集都是样本的集合，而样本是特征的集合。</p>

<p>表示数据集的常用方法是设计矩阵(design matrix)。设计矩阵的每一行包含 一个不同的样本。每一列对应不同的特征。例如， Iris 数据集包含 150 个样本，每 个样本有4个特征。这意味着我们可以将该数据集表示为设计矩阵X e R150&rdquo;4,其</p>

<p>中 Xi,1 表示第 i 个植物的萼片长度， Xi,2 表示第 i 个植物的萼片宽度等等。我们在</p>

<p>本书中描述的大部分学习算法都是讲述它们是如何运行在设计矩阵数据集上的。</p>

<p>当然，每一个样本都能表示成向量，并且这些向量的维度相同，才能将一个数 据集表示成设计矩阵。这一点并非永远可能。例如，你有不同宽度和高度的照片的 集合，那么不同的照片将会包含不同数量的像素。因此不是所有的照片都可以表示 成相同长度的向量。第9.7节和第十章将会介绍如何处理这些不同类型的异构数据。 在上述这类情况下，我们不会将数据集表示成 m 行的矩阵，而是表示成 m 个元素 的结合：｛x(1), x(2),&hellip;, x(m)｝。这种表示方式意味着样本向量x⑴和x⑴可以有不 同的大小。</p>

<p>在监督学习中，样本包含一个标签或目标和一组特征。例如，我们希望使用学 习算法从照片中识别对象。我们需要明确哪些对象会出现在每张照片中。我们或许 会用数字编码表示，如 0 表示人、1 表示车、2 表示猫等等。通常在处理包含观测特</p>

<p>征的设计矩阵X的数据集时，我们也会提供一个标签向量y，其中yi表示样本i 的标签。</p>

<p>当然，有时标签可能不止一个数。例如，如果我们想要训练语音模型转录整个</p>

<p>句子，那么每个句子样本的标签是一个单词序列。</p>

<p>正如监督学习和无监督学习没有正式的定义，数据集或者经验也没有严格的区</p>

<p>分。这里介绍的结构涵盖了大多数情况，但始终有可能为新的应用设计出新的结构。</p>

<p>5.1.4 示例：线性回归
我们将机器学习算法定义为，通过经验以提高计算机程序在某些任务上性能的 算法。这个定义有点抽象。为了使这个定义更具体点，我们展示一个简单的机器学 习示例：线性回归(linear regression。。当我们介绍更多有助于理解机器学习特性的 概念时，我们会反复回顾这个示例。</p>

<p>顾名思义，线性回归解决回归问题。换言之，我们的目标是建立一个系统，将向</p>

<p>量x e Rn作为输人，预测标量y e R作为输出。线性回归的输出是其输人的线性函 数。令y表示模型预测y应该取的值。我们定义输出为</p>

<p>y = wTx,    (5.3)</p>

<p>其中w e Rn是参数(parameter。向量。</p>

<p>参数是控制系统行为的值。在这种情况下， wi 是系数，会和特征 xi 相乘之 后全部相加起来。我们可以将 w 看作是一组决定每个特征如何影响预测的权重</p>

<p>(weight。。如果特征xi对应的权重wi是正的，那么特征的值增加，我们的预测值</p>

<p>y也会增加。如果特征对应的权重％是负的，那么特征的值增加，我们的预测 值y会减少。如果特征权重的大小很大，那么它对预测有很大的影响；如果特征权 重的大小是零，那么它对预测没有影响。</p>

<p>因此，我们可以定义任务T:通过输出y = wTx从x预测y。接下来我们需要 定义性能度量一p。</p>

<p>假设我们有 m 个输人样本组成的设计矩阵，我们不用它来训练模型，而是评 估模型性能如何。我们也有每个样本对应的正确值y组成的回归目标向量。因为这 个数据集只是用来评估性能，我们称之为测试集(test set)。我们将输人的设计矩 阵记作X(test)，回归目标向量记作y(test)。</p>

<p>度量模型性能的一种方法是计算模型在测试集上的均方误差(mean squared error)。如果y(test)表示模型在测试集上的预测值，那么均方误差表示为：</p>

<p>MSEtest = - V(y(test) - y(test))2.    (5.4)</p>

<p>m</p>

<p>i</p>

<p>直观上，当y(test) = y(test)时，我们会发现误差降为0。我们也可以看到</p>

<p>MSEtest = m^ ||y(test) - y(te叫(5.5)</p>

<p>所以当预测值和目标值之间的欧几里得距离增加时，误差也会增加。</p>

<p>为了构建一个机器学习算法， 我们需要设计一个算法， 通过观察训练集 (X(train), y(train))获得经验，减少MSEtest以改进权重w。一种直观方式(我们 将在后续的第5.5.1节说明其合法性)是最小化训练集上的均方误差，即 MSEtrain。</p>

<p>最小化MSEtrain，我们可以简单地求解其导数为0的情况：</p>

<p>Vw (wT x(train)TX(train)w - 2wTX(train)Ty(train) + y(train)Ty(train^ = 0 (5.10) 今 2x(train)Tx(train)w - 2X(train)Ty(train) = 0    (5.11)</p>

<p>w = (^m ]=e</p>

<p>(5.38)</p>

<p>(5.39)</p>

<p>回到式(5.37)，我们可以得出&lt;&gt;m的偏差是-a2/m。因此样本方差是有偏估计 无偏样本方差( unbiased sample variance )估计</p>

<p>1</p>

<p>m1</p>

<p>m</p>

<p>(x(i)</p>

<p>i=1</p>

<p>(5.40)</p>

<p>提供了另一种可选方法。正如名字所言，这个估计是无偏的。换言之，我们会发现</p>

<p>E[am] =a2:</p>

<p>E[am]</p>

<p>m</p>

<p>=E m_ 丨    (xW _ Am)</p>

<p>i=1</p>

<p>E[aAm2 ]</p>

<p>m- 1 ( m m 1</p>

<p>2</p>

<p>a</p>

<p>= a2.</p>

<p>(5.41)</p>

<p>(5.42)</p>

<p>(5.43)</p>

<p>(5.44)</p>

<p>我们有两个估计量：一个是有偏的，另一个是无偏的。尽管无偏估计显然是令 人满意的，但它并不总是 ‘‘最好&rdquo; 的估计。我们将看到，经常会使用其他具有重要性 质的有偏估计。</p>

<p>5.4.3 方差和标准差
我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程 度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的 方差。估计量的方差(variance)就是一个方差</p>

<p>Var(0)    (5.45)</p>

<p>其中随机变量是训练集。另外，方差的平方根被称为标准差(standard error)，记作 SE0)。</p>

<p>估计量的方差或标准差告诉我们，当独立地从潜在的数据生成过程中重采样数</p>

<p>据集时，如何期望估计的变化。正如我们希望估计的偏差较小，我们也希望其方差</p>

<p>较小。</p>

<p>当我们使用有限的样本计算任何统计量时，真实参数的估计都是不确定的，在</p>

<p>这个意义下，从相同的分布得到其他样本时，它们的统计量也会不一样。任何方差</p>

<p>估计量的期望程度是我们想量化的误差的来源。</p>

<p>均值的标准差被记作</p>

<p>SE(/tm)=</p>

<p>Var</p>

<p>i=1</p>

<p>a</p>

<p>(5.46)</p>

<p>其中a2是样本x(i)的真实方差。标准差通常被记作a。可惜，样本方差的平方根和 方差无偏估计的平方根都不是标准差的无偏估计。这两种计算方法都倾向于低估真 实的标准差，但仍用于实际中。相较而言，方差无偏估计的平方根较少被低估。对于 较大的m，这种近似非常合理。</p>

<p>均值的标准差在机器学习实验中非常有用。我们通常用测试集样本的误差均值 来估计泛化误差。测试集中样本的数量决定了这个估计的精确度。中心极限定理告 诉我们均值会接近一个高斯分布，我们可以用标准差计算出真实期望落在选定区间 的概率。例如，以均值Am为中心的95%置信区间是</p>

<p>(沁-1.96SE(/!m),/im + 1.96SE(/!m)),    (5.47)</p>

<p>以上区间是基于均值Am和方差SE(/!m)2的高斯分布。在机器学习实验中，我们通 常说算法 A 比算法 B 好，是指算法 A 的误差的 95% 置信区间的上界小于算法 B 的误差的 95% 置信区间的下界。</p>

<p>示例：伯努利分布我们再次考虑从伯努利分布(回顾P(X⑴;0) = 0x(i) (1 - 0)1-x(i)。 中独立同分布采样出来的一组样本忖⑴^一^^⑦这次我们关注估计么二</p>

<p>mzm=ix⑴的方差:</p>

<p>Var(L)=Var(mgx⑴)</p>

<p>(5.48)</p>

<p>Var</p>

<p>x</p>

<p>(i)</p>

<p>i=1</p>

<p>i=1</p>

<p>(5.49)</p>

<p>i=1</p>

<p>mi</p>

<p>m2</p>

<p>(5.50) (5.51)</p>

<p>— 0(1 - 0) m</p>

<p>(5.52)</p>

<p>估计量方差的下降速率是关于数据集样本数目 m 的函数。这是常见估计量的普遍性</p>

<p>质，在探讨一致性(参考第5.4.5节)时，我们会继续讨论。</p>

<p>5.4.4 权衡偏差和方差以最小化均方误差
偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参</p>

<p>数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差。</p>

<p>当我们可以在一个偏差更大的估计和一个方差更大的估计中进行选择时，会发</p>

<p>生什么呢？我们该如何选择？例如，想象我们希望近似图5.2中的函数，我们只可以</p>

<p>选择一个偏差较大的估计或一个方差较大的估计，我们该如何选择呢？</p>

<p>判断这种权衡最常用的方法是交叉验证。经验上，交叉验证在真实世界的许多任 务中都非常成功。另外，我们也可以比较这些估计的均方误差( mean squared error,</p>

<p>MSE)：</p>

<p>MSE = E[(0m _ 0)2]</p>

<p>= Bias(0m)2 + Var(0m)</p>

<p>(5.53)</p>

<p>(5.54)</p>

<p>MSE度量着估计和真实参数0之间平方误差的总体期望偏差。如式(5.54)所示， MSE估计包含了偏差和方差。理想的估计具有较小的MSE或是在检查中会稍微约 束它们的偏差和方差。</p>

<p>偏差和方差的关系和机器学习容量、欠拟合和过拟合的概念紧密相联。用MSE度 量泛化误差(偏差和方差对于泛化误差都是有意义的)时，增加容量会增加方差，降 低偏差。如图5.6所示，我们再次在关于容量的函数中，看到泛化误差的 U 形曲线。</p>

<p>图5.6:当容量增大(x轴)时，偏差(用点表示)随之减小，而方差(虚线)随之增大，使得泛</p>

<p>化误差(加粗曲线)产生了另一种 U 形。如果我们沿着轴改变容量，会发现最佳容量，当容量小 于最佳容量会呈现欠拟合，大于时导致过拟合。这种关系与第5.2节以及图 5.3中讨论的容量、欠 拟合和过拟合之间的关系类似。</p>

<p>5.4.5 一致性
目前我们已经探讨了固定大小训练集下不同估计量的性质。通常，我们也会关</p>

<p>注训练数据增多后估计量的效果。特别地，我们希望当数据集中数据点的数量m增 加时，点估计会收敛到对应参数的真实值。更形式地，我们想要</p>

<p>plimm^m0m = 0.    (5.55)</p>

<p>符号plim表示依概率收敛，即对于任意的e &gt; 0，当m 4⑺时，有P(|0m — 0| &gt; e)    0。式(5.55)表示的条件被称为一致性(consistency。。有时它是指弱一致性，</p>

<p>强一致性是指几乎必然(almost sure。从0收敛到0。几乎必然收敛(almost sure convergence。是指当 p(limm^m x(m) = z) = 1 时，随机变量序列 x(1)，x(2)，…收 敛到 z。</p>

<p>一致性保证了估计量的偏差会随数据样本数目的增多而减少。然而，反过来是 不正确的——渐近无偏并不意味着一致性。例如，考虑用包含 m 个样本的数据集 {x(1)x(m)}估计正态分布N (x; A, a2)的均值参数A。我们可以使用数据集的第</p>

<p>一个样本x(1)作为无偏估计量：6 = x⑴。在该情况下，E(0m) = 0，所以不管观测 到多少数据点，该估计量都是无偏的。然而，这不是一个一致估计，因为它不满足当 m —日寸，6m ~6。</p>

<p>5.5 最大似然估计
之前，我们已经看过常用估计的定义，并分析了它们的性质。但是这些估计是</p>

<p>从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到特定函数作为好的</p>

<p>估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。</p>

<p>最常用的准则是最大似然估计。</p>

<p>考虑一组含有m个样本的数据集X = {a⑴，&hellip;，a(m)}，独立地由未知的真实数</p>

<p>据生成分布 pdata(x) 生成。</p>

<p>令 pmodel(x; 0) 是一族由 0 确定在相同空间上的概率分布。换言之， pmodel(a; 0) 将任意输人 a 映射到实数来估计真实概率 pdata(a)。</p>

<p>对0的最大似然估计被定义为：</p>

<p>0ML = argmaxpmodel(X; 0)，    (5.56)</p>

<p>Q</p>

<p>m</p>

<p>= arg max pmodel(a(i); 0).    (5.57)</p>

<p>° i=1</p>

<p>多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现数值 下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对数不会改变其 argmax 但是将乘积转化成了便于计算的求和形式：</p>

<p>m</p>

<p>0ML = argmax    logpmodel(a(i); 0).    (5.58)</p>

<p>°    i=1</p>

<p>因为当我们重新缩放代价函数时argmax不会改变，我们可以除以m得到和训练数</p>

<p>据经验分布 pdata 相关的期望作为准则：</p>

<p>0ml = arg maxEx^pd^ logPmodei(a; 0).    (5.59)</p>

<p>°</p>

<p>一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布Pdata和模</p>

<p>型分布之间的差异，两者之间的差异程度可以通过KL散度度量。KL散度被定义为</p>

<p>DKL(pdata||pmodel) = Ex^p)data [log pdata» _ log pmodel    (5.60)</p>

<p>左边一项仅涉及到数据生成过程，和模型无关。这意味着当我们训练模型最小化 KL</p>

<p>散度时，我们只需要最小化</p>

<p>_ Ex^pdata [logpmodel»]，    (5.61)</p>

<p>当然，这和式(5.59)中最大化是相同的。</p>

<p>最小化KL散度其实就是在最小化分布之间的交叉熵。许多作者使用术语“交</p>

<p>叉熵&rdquo; 特定表示伯努利或 softmax 分布的负对数似然，但那是用词不当的。任何一</p>

<p>个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率</p>

<p>分布之间的交叉熵。例如，均方误差是经验分布和高斯模型之间的交叉熵。</p>

<p>我们可以将最大似然看作是使模型分布尽可能地和经验分布 pdata 相匹配的尝 试。理想情况下，我们希望匹配真实的数据生成分布Pdata，但我们没法直接知道这 个分布。</p>

<p>虽然最优0在最大化似然或是最小化KL散度时是相同的，但目标函数值是不 一样的。在软件中，我们通常将两者都称为最小化代价函数。因此最大化似然变成 了最小化负对数似然(NLL)，或者等价的是最小化交叉熵。将最大化似然看作最小 化KL散度的视角在这个情况下是有帮助的，因为已知KL散度最小值是零。当 取实数时，负对数似然是负值。</p>

<p>5.5.1 条件对数似然和均方误差
最大似然估计很容易扩展到估计条件概率P(y | x;0)，从而给定x预测y。实 际上这是最常见的情况，因为这构成了大多数监督学习的基础。如果 X 表示所有的 输入， Y 表示我们观测到的目标，那么条件最大似然估计是</p>

<p>0ml = arg maxP( Y | X; 0).    (5.62)</p>

<p>e</p>

<p>如果假设样本是独立同分布的，那么这可以分解成</p>

<p>m</p>

<p>0ML = arg max log P ( y(i) | x(i); 0).    (5.63)</p>

<p>e</p>

<p>i=1</p>

<p>示例：线性回归作为最大似然 第5.1.4节介绍的线性回归，可以被看作是最大似然 过程。之前，我们将线性回归作为学习从输人a映射到输出y的算法。从a到y的 映射选自最小化均方误差(我们或多或少介绍的一个标准)。现在，我们以最大似然 估计的角度重新审视线性回归。我们现在希望模型能够得到条件概率P(y| a)，而不 只是得到一个单独的预测y。想象有一个无限大的训练集，我们可能会观测到几个训 练样本有相同的输人a但是不同的y。现在学习算法的目标是拟合分布P(y I a)到和 a相匹配的不同的y。为了得到我们之前推导出的相同的线性回归算法，我们定义 p(y | a) = N(y; y(a; w)，ct2)。函数y(a; w)预测高斯的均值。在这个例子中，我们假 设方差是用户固定的某个常量ct2。这种函数形式p(y | a)会使得最大似然估计得出 和之前相同的学习算法。由于假设样本是独立同分布的，条件对数似然(式(5.63)) 如下</p>

<p>m</p>

<p>log p(y(i) | a(i); 0)</p>

<p>i=1</p>

<p>E
i=1</p>

<p>2ct2</p>

<p>y(i)||2</p>

<p>(5.64)</p>

<p>(5.65)</p>

<p>其中y(i)是线性回归在第i个输人a(i)上的输出，m是训练样本的数目</p>

<p>对比均方</p>

<p>误差和对数似然，</p>

<p>MSEtrain =</p>

<p>2</p>

<p>i=1</p>

<p>(5.66)</p>

<p>我们立刻可以看出最大化关于w的对数似然和最小化均方误差会得到相同的参数估 计w。但是对于相同的最优w，这两个准则有着不同的值。这验证了 MSE可以用 于最大似然估计。正如我们将看到的，最大似然估计有几个理想的性质。</p>

<p>5.5.2 最大似然的性质
最大似然估计最吸引人的地方在于，它被证明当样本数目m o时，就收敛 率而言是最好的渐近估计。</p>

<p>在合适的条件下，最大似然估计具有一致性(参考第5.4.5节) ，意味着训练样 本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。这些条件是：</p>

<p>•真实分布Pdata必须在模型族PmodelG;0)中。否则，没有估计可以还原Pdata。</p>

<p>•真实分布Pdata必须刚好对应一个0值。否则，最大似然估计恢复出真实分布</p>

<p>Pdata后，也不能决定数据生成过程使用哪个0。</p>

<p>除了最大似然估计，还有其他的归纳准则，其中许多共享一致估计的性质。然 而，一致估计的统计效率（statistic efficiency。可能区别很大。某些一致估计可能会 在固定数目的样本上获得一个较低的泛化误差，或者等价地，可能只需要较少的样 本就能达到一个固定程度的泛化误差。</p>

<p>统计效率通常用于有参情况（parametric case。的研究中（例如线性回归。。有 参情况中我们的目标是估计参数值（假设有可能确定真实参数），而不是函数值。一 种度量我们和真实参数相差多少的方法是计算均方误差的期望，即计算m个从数据 生成分布中出来的训练样本上的估计参数和真实参数之间差值的平方。有参均方误 差估计随着m的增加而减少，当m较大时，Cramer-Rao下界（Rao, 1945; Cramer, 1946） 表明不存在均方误差低于最大似然估计的一致估计。</p>

<p>因为这些原因（一致性和统计效率），最大似然通常是机器学习中的首选估计。</p>

<p>当样本数目小到会发生过拟合时，正则化策略如权重衰减可用于获得训练数据有限</p>

<p>时方差较小的最大似然有偏版本。</p>

<p>5.6 贝叶斯统计
至此我们已经讨论了频率派统计（frequentist statistics。方法和基于估计单一 值0 的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有 可能的0。后者属于贝叶斯统计（Bayesian statistics。的范畴。</p>

<p>正如第5.4.1节中讨论的，频率派的视角是真实参数0是未知的定值，而点估计 0 是考虑数据集上函数（可以看作是随机的）的随机变量。</p>

<p>贝叶斯统计的视角完全不同。贝叶斯用概率反映知识状态的确定性程度。数据 集能够被直接观测到，因此不是随机的。另一方面，真实参数 0 是未知或不确定的 因此可以表示成随机变量。</p>

<p>在观察到数据前，我们将0的已知知识表示成先验概率分布（prior probability distribution）， P（0） （有时简单地称为 ‘‘先验&rdquo;）。一般而言，机器学习实践者会选择 一个相当宽泛的（即，高熵的）先验分布，反映在观测到任何数据前参数 0 的高度 不确定性。例如，我们可能会假设先验 0 在有限区间中均匀分布。许多先验偏好于</p>

<p>‘‘更简单&rdquo; 的解(如小幅度的系数，或是接近常数的函数)。</p>

<p>现在假设我们有一组数据样本｛x(1)，&hellip;，x(m)｝。通过贝叶斯规则结合数据似然 p(x(1)，&hellip;，x(m)| 0) 和先验，我们可以恢复数据对我们关于0信念的影响：</p>

<p>p(0 | x(1)</p>

<p>(m)) = P(X⑴，&hellip;，X(m) | 0)p(0) x    p(x⑴，…，x(m))</p>

<p>(5.67)</p>

<p>在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测</p>

<p>数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。</p>

<p>相对于最大似然估计，贝叶斯估计有两个重要区别。第一，不像最大似然方法预 测时使用0的点估计，贝叶斯方法使用0的全分布。例如，在观测到m个样本后， 下一个数据样本 x(m+1) 的预测分布如下：</p>

<p>p(x(m+1) | x(1)，&hellip;，x(m)) =    p(x(m+1) | 0)p(0| x(1)，&hellip;，x(m)) d0.</p>

<p>(5.68)</p>

<p>这里，每个具有正概率密度的 0 的值有助于下一个样本的预测，其中贡献由后验密 度本身加权。在观测到数据集｛x(1)，&hellip;，x(m)｝之后，如果我们仍然非常不确定0的 值，那么这个不确定性会直接包含在我们所做的任何预测中。</p>

<p>在第5.4节中，我们已经探讨频率派方法解决给定点估计 0 的不确定性的方法 是评估方差，估计的方差评估了观测数据重新从观测数据中采样后，估计可能如何 变化。对于如何处理估计不确定性的这个问题，贝叶斯派的答案是积分，这往往会 防止过拟合。当然，积分仅仅是概率法则的应用，使贝叶斯方法容易验证，而频率 派机器学习基于相当特别的决定构建了一个估计，将数据集里的所有信息归纳到一 个单独的点估计。</p>

<p>贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成的。先</p>

<p>验能够影响概率质量密度朝参数空间中偏好先验的区域偏移。实践中，先验通常表</p>

<p>现为偏好更简单或更光滑的模型。对贝叶斯方法的批判认为先验是人为主观判断影</p>

<p>响预测的来源。</p>

<p>当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大</p>

<p>时，通常会有很大的计算代价。</p>

<p>示例：贝叶斯线性回归 我们使用贝叶斯估计方法学习线性回归的参数。在线性回</p>

<p>归中，我们学习从输人向量e Rn预测标量y e R的线性映射。该预测由向量</p>

<p>w e Rn参数化：</p>

<p>y = wTa.    (5.69)</p>

<p>给定一组m个训练样本(X(train)，y(train))，我们可以表示整个训练集对y的预测：</p>

<p>^(train) = ^(train)w.</p>

<p>(5.70)</p>

<p>表示为 y(train) 上的高斯条件分布，我们得到</p>

<p>P(y(train)|X(train)，w)=N(y(train);X(train)w，I)    (5.71)</p>

<p>a exp    1(y(train) - X(train)w)T(y(train) 一 X(train)w))，</p>

<p>(5.72)</p>

<p>其中，我们根据标准的MSE公式假设y上的高斯方差为1。在下文中，为减少符号 负担，我们将(X(train)，y(train))简单表示为(X，y)。</p>

<p>为确定模型参数向量 w 的后验分布，我们首先需要指定一个先验分布。先验应 该反映我们对这些参数取值的信念。虽然有时将我们的先验信念表示为模型的参数 很难或很不自然，但在实践中我们通常假设一个相当广泛的分布来表示0的高度不 确定性。实数值参数通常使用高斯作为先验分布：</p>

<p>p(w) = N(w; Mo, Ao) a exp (- 1(w - Mo)TA-1 (w - &ldquo;。))，    (5.73)</p>

<p>其中，M。和A。分别是先验分布的均值向量和协方差矩阵。1 确定好先验后，我们现在可以继续确定模型参数的后验分布。</p>

<p>P(w | X， y) a P(y | X， w)P(w) 1 2</p>

<p>a exp (- 1(y- Xw)T(y- Xw)) exp (-|(w- Mo)TA-\w_ m。)</p>

<p>(5.74)</p>

<p>a exp (—* (-2yTXw + wTXTXw+ wTA-1 w — 2m(TA-1w^ .</p>

<p>(5.75)</p>

<p>(5.76)</p>

<p>除非有理由使用协方差矩阵的特定结构，我们通常假设其为对角协方差矩阵A。= diag(入0)。</p>

<p>现在我们定义Am = (XTX+ A-1)-1和Mm = Am(XTy+ A-1M0)。使用这些新的 变量，我们发现后验可改写为高斯分布：</p>

<p>P(w 1 X, y)    exP (-2(w— Mm)TAm1(w — Mm) + 2&rdquo;m^m^(5.77)</p>

<p>exp (- j(w- Mm)TAm1(w- Mm)) .    (5.78)</p>

<p>分布的积分必须归一这个事实意味着要删去所有不包括参数向量w的项。式(3.23)显</p>

<p>示了如何标准化多元高斯分布。</p>

<p>检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉。大多数情况下， 我们设置M0 = 0。如果我们设置A。= 那么Am对w的估计就和频率派带权重 衰减惩罚awTw的线性回归的估计是一样的。一个区别是若a设为0则贝叶斯估 计是未定义的——我们不能将贝叶斯学习过程初始化为一个无限宽的 w 先验。更重 要的区别是贝叶斯估计会给出一个协方差矩阵，表示w所有不同值的可能范围，而 不仅是估计 Am。</p>

<p>5.6.1 最大后验 (MAP) 估计
原则上，我们应该使用参数 0 的完整贝叶斯后验分布进行预测，但单点估计 常常也是需要的。希望使用点估计的一个常见原因是，对于大多数有意义的模型而 言，大多数涉及到贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似 解。我们仍然可以让先验影响点估计的选择来利用贝叶斯方法的优点，而不是简单 地回到最大似然估计。一种能够做到这一点的合理方式是选择 最大后验( Maximum A Posteriori, MAP。点估计。MAP估计选择后验概率最大的点(或在0是连续值 的更常见情况下，概率密度最大的点)：</p>

<p>0MAP = argmaxP(0 | z) = argmax log P(z | 0) + logP(0).    (5.79)</p>

<p>e    e</p>

<p>我们可以认出上式右边的 logP(z| 0) 对应着标准的对数似然项， logP(0) 对应着先 验分布。</p>

<p>例如，考虑具有高斯先验权重w的线性回归模型。如果先验是N(w； 0, {12), 那么式(5.79)的对数先验项正比于熟悉的权重衰减惩罚AwTw，加上一个不依赖于 w也不会影响学习过程的项。因此，具有高斯先验权重的MAP贝叶斯推断对应着权 重衰减。</p>

<p>正如全贝叶斯推断，MAP贝叶斯推断的优势是能够利用来自先验的信息，这些 信息无法从训练数据中获得。该附加信息有助于减少最大后验点估计的方差(相比 于ML估计)。然而，这个优点的代价是增加了偏差。</p>

<p>许多正规化估计方法，例如权重衰减正则化的最大似然学习，可以被解释为贝 叶斯推断的MAP近似。这个适应于正则化时加到目标函数的附加项对应着logp(0)。 并非所有的正则化惩罚都对应着MAP贝叶斯推断。例如，有些正则化项可能不是一 个概率分布的对数。还有些正则化项依赖于数据，当然也不会是一个先验概率分布。</p>

<p>MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例 如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一个单独的高斯</p>

<p>分布 (Nowlan and Hinton, 1992)。</p>

<p>5.7 监督学习算法
回顾第5.1.3节，粗略地说，监督学习算法是给定一组输人 x 和输出 y 的训练</p>

<p>集，学习如何关联输人和输出。在许多情况下，输出y很难自动收集，必须由人来 提供‘‘监督&rdquo;，不过该术语仍然适用于训练集目标可以被自动收集的情况。</p>

<p>5.7.1 概率监督学习
本书的大部分监督学习算法都是基于估计概率分布P(y | 4的。我们可以使用最 大似然估计找到对于有参分布族P(y | % 0)最好的参数向量0。</p>

<p>我们已经看到，线性回归对应于分布族</p>

<p>p(y | X； 0) = N(y; 0Tx，I).    (5.80)</p>

<p>通过定义一族不同的概率分布，我们可以将线性回归扩展到分类情况中。如果我们 有两个类，类0 和类1，那么我们只需要指定这两类之一的概率。类 1 的概率决定 了类0的概率，因为这两个值加起来必须等于 1。</p>

<p>我们用于线性回归的实数正态分布是用均值参数化的。我们提供这个均值的任 何值都是有效的。二元变量上的分布稍微复杂些，因为它的均值必须始终在 0和 1 之间。解决这个问题的一种方法是使用 logistic sigmoid 函数将线性函数的输出压缩</p>

<p>进区间 (0，1)。该值可以解释为概率：</p>

<p>p(y = 1 | a; 0) = CT(0Ta).    (5.81)</p>

<p>这个方法被称为逻辑回归(logistic regression )，这个名字有点奇怪，因为该模型用 于分类而非回归。</p>

<p>线性回归中，我们能够通过求解正规方程以找到最佳权重。相比而言，逻辑回</p>

<p>归会更困难些。其最佳权重没有闭解。反之，我们必须最大化对数似然来搜索最优</p>

<p>解。我们可以通过梯度下降算法最小化负对数似然来搜索。</p>

<p>通过确定正确的输人和输出变量上的有参条件概率分布族，相同的策略基本上</p>

<p>可以用于任何监督学习问题。</p>

<p>5.7.2 支持向量机
支持向量机(support vector machine, SVM )是监督学习中最有影响力的方法 之一 (Boser et al., 1992; Cortes and Vapnik, 1995)。类似于逻辑回归，这个模型也 是基于线性函数 wTa+b 的。不同于逻辑回归的是，支持向量机不输出概率，只输 出类别。当wTa + b为正时，支持向量机预测属于正类。类似地，当wTa+b为负 时，支持向量机预测属于负类。</p>

<p>支持向量机的一个重要创新是核技巧(kernel trick )。核技巧观察到许多机器学 习算法都可以写成样本间点积的形式。例如，支持向量机中的线性函数可以重写为</p>

<p>m</p>

<p>wTa+b= b+    aiaTa(i)，    (5.82)</p>

<p>i=1</p>

<p>其中，a(i)是训练样本，a是系数向量。学习算法重写为这种形式允许我们将a替 换为特征函数4(a)的输出，点积替换为被称为核函数(kernel function)的函数 k(a，a(i)) = 4(a) • 4(a(i))。运算符•表示类似于^(a)T^(a(i))的点积。对于某些特 征空间，我们可能不会书面地使用向量内积。在某些无限维空间中，我们需要使用 其他类型的内积，如基于积分而非加和的内积。这种类型内积的完整介绍超出了本 书的范围。</p>

<p>使用核估计替换点积之后，我们可以使用如下函数进行预测</p>

<p>f (a) = b + ^2 aik(a，a⑴).    (5.83)</p>

<p>i</p>

<p>这个函数关于z是非线性的，关于我勾是线性的。a和八勾之间的关系也是线性 的。核函数完全等价于用预处理所有的输人，然后在新的转换空间学习线性模 型。</p>

<p>核技巧十分强大有两个原因。首先，它使我们能够使用保证有效收敛的凸优化 技术来学习非线性模型(关于的函数。。这是可能的，因为我们可以认为是固 定的，仅优化a，即优化算法可以将决策函数视为不同空间中的线性函数。其二，核 函数k的实现方法通常有比直接构建我⑹再算点积高效很多。</p>

<p>在某些情况下，利勾甚至可以是无限维的，对于普通的显式方法而言，这将是 无限的计算代价。在很多情况下，即使我勾是难算的，k(d9却会是一个关于z 非线性的、易算的函数。举个无限维空间易算的核的例子，我们构建一个作用于非 负整数x上的特征映射＜Xx)。假设这个映射返回一个由开头x个1,随后是无限个 0的向量。我们可以写一个核函数k(x,x(i)) = min(x,x(i))，完全等价于对应的无限 维点积。</p>

<p>最常用的核函数是高斯核( Gaussian kernel)，</p>

<p>k(u, v) = N(u — v; 0, a21),    (5.84)</p>

<p>其中N (x; m, S)是标准正态密度。这个核也被称为径向基函数(radial basis function, RBF。核，因为其值沿v中从u向外辐射的方向减小。高斯核对应于无限维空 间中的点积，但是该空间的推导没有整数上最小核的示例那么直观。</p>

<p>我们可以认为高斯核在执行一种模板匹配(template matching)。训练标签y相 关的训练样本z变成了类别y的模版。当测试点到z的欧几里得距离很小，对 应的高斯核响应很大时，表明zz和模版z非常相似。该模型进而会赋予相对应的训 练标签 y 较大的权重。总的来说，预测将会组合很多这种通过训练样本相似度加权 的训练标签。</p>

<p>支持向量机不是唯一可以使用核技巧来增强的算法。许多其他的线性模型也 可以通过这种方式来增强。使用核技巧的算法类别被称为核机器(kernel machine。 或核方法(kernel method。(Williams and Rasmussen, 1996; Scholkopf et al., 1999)。</p>

<p>核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。 因为第i个样本贡献aik(z,z(i))到决策函数。支持向量机能够通过学习主要包含零 的向量a，以缓和这个缺点。那么判断新样本的类别仅需要计算非零ai对应的训 练样本的核函数。这些训练样本被称为支持向量(support vector。。</p>

<p>当数据集很大时，核机器的计算量也会很大。我们将会在第5.9节回顾这个想</p>

<p>法。带通用核的核机器致力于泛化得更好。我们将在第5.11节解释原因。现代深 度学习的设计旨在克服核机器的这些限制。当前深度学习的复兴始于 Hinton et al. (2006b) 表明神经网络能够在 MNIST 基准数据上胜过 RBF 核的支持向量机。</p>

<p>5.7.3 其他简单的监督学习算法
我们已经简要介绍过另一个非概率监督学习算法，最近邻回归。更一般地，k-最 近邻是一类可用于分类或回归的技术。作为一个非参数学习算法，k-最近邻并不局 限于固定数目的参数。我们通常认为k-最近邻算法没有任何参数，而是使用训练数 据的简单函数。事实上，它甚至也没有一个真正的训练阶段或学习过程。反之，在 测试阶段我们希望在新的测试输人X上产生y，我们需要在训练数据X上找到x的 k-最近邻。然后我们返回训练集上对应的y值的平均值。这几乎适用于任何类型可 以确定y值平均值的监督学习。在分类情况中，我们可以关于one-hot编码向量c 求平均，其中 cy = 1，其他的 i 值取 ci = 0。然后，我们可以解释这些 one-hot 编码 的均值为类别的概率分布。作为一个非参数学习算法，k-近邻能达到非常高的容量。 例如，假设我们有一个用 0-1 误差度量性能的多分类任务。在此设定中，当训练样 本数目趋向于无穷大时， 1-最近邻收敛到两倍贝叶斯误差。超出贝叶斯误差的原因 是它会随机从等距离的临近点中随机挑一个。而存在无限的训练数据时，所有测试 点 X 周围距离为零的邻近点有无限多个。如果我们使用所有这些临近点投票的决策 方式，而不是随机挑选一个，那么该过程将会收敛到贝叶斯错误率。k-最近邻的高容 量使其在训练样本数目大时能够获取较高的精度。然而，它的计算成本很高，另外 在训练集较小时泛化能力很差。k-最近邻的一个弱点是它不能学习出哪一个特征比 其他更具识别力。例如，假设我们要处理一个的回归任务，其中 Xe R100 是从各向 同性的高斯分布中抽取的，但是只有一个变量 x1 和结果相关。进一步假设该特征直 接决定了输出，即在所有情况中y = xi。最近邻回归不能检测到这个简单模式。大 多数点 X 的最近邻将取决于 x2 到 x100 的大多数特征，而不是单独取决于特征 x1。 因此，小训练集上的输出将会非常随机。</p>

<p>决策树(decision tree)及其变种是另一类将输人空间分成不同的区域，每个区 域有独立参数的算法 (Breiman et al., 1984)。如图 5.7所示，决策树的每个节点都与 输人空间的一个区域相关联，并且内部节点继续将区域分成子节点下的子区域(通 常使用坐标轴拆分区域)。空间由此细分成不重叠的区域，叶节点和输人区域之间形</p>

<p>成一一对应的关系。每个叶结点将其输入区域的每个点映射到相同的输出。决策树</p>

<p>通常有特定的训练算法，超出了本书的范围。如果允许学习任意大小的决策树，那</p>

<p>么它可以被视作非参数算法。然而实践中通常有大小限制，作为正则化项将其转变</p>

<p>成有参模型。由于决策树通常使用坐标轴相关的拆分，并且每个子节点关联到常数</p>

<p>输出，因此有时解决一些对于逻辑回归很简单的问题很费力。例如，假设有一个二 分类问题，当 x2 &gt; x1 时分为正类，则决策树的分界不是坐标轴对齐的。因此，决策</p>

<p>树将需要许多节点近似决策边界，坐标轴对齐使其算法步骤不断地来回穿梭于真正</p>

<p>的决策函数。</p>

<p>正如我们已经看到的，最近邻预测和决策树都有很多的局限性。尽管如此，在</p>

<p>计算资源受限制时，它们都是很有用的学习算法。通过思考复杂算法和k-最近邻或 决策树之间的相似性和差异，我们可以建立对更复杂学习算法的直觉。</p>

<p>读者可以参考 Murphy (2012); Bishop (2006); Hastie et al. (2001) 或其他机器 学习教科书了解更多的传统监督学习算法。</p>

<p>图5.7: 描述一个决策树如何工作的示意图。 （上） 树中每个节点都选择将输人样本送到左子节点 （0） 或者右子节点 （1）。内部的节点用圆圈表示，叶节点用方块表示。每一个节点可以用一个二值的 字符串识别并对应树中的位置，这个字符串是通过给起父亲节点的字符串添加一个位元来实现的 （0 表示选择左或者上， 1 表示选择右或者下）。 （下） 这个树将空间分为区域。这个二维平面说明决</p>

<p>策树可以分割R2。这个平面中画出了树的节点，每个内部点穿过分割线并用来给样本分类，叶节</p>

<p>点画在样本所属区域的中心。结果是一个分块常数函数，每一个叶节点一个区域。每个叶需要至</p>

<p>少一个训练样本来定义，所以决策树不可能用来学习一个局部极大值比训练样本数量还多的函数。</p>

<p>5.8 无监督学习算法
回顾第 5.1.3节，无监督算法只处理 “特征&rdquo;，不操作监督信号。监督和无监督 算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值 是特征还是目标。通俗地说，无监督学习的大多数尝试是指从不需要人为注释的样 本的分布中抽取信息。该术语通常与密度估计相关，学习从分布中采样、学习从分 布中去噪、寻找数据分布的流形或是将数据中相关的样本聚类。</p>

<p>一个经典的无监督学习任务是找到数据的 ‘‘最佳&rdquo; 表示。 ‘‘最佳&rdquo; 可以是不同的 表示，但是一般来说，是指该表示在比本身表示的信息更简单或更易访问而受到一</p>

<p>些惩罚或限制的情况下，尽可能地保存关于 z 更多的信息。</p>

<p>有很多方式定义较简单的表示。最常见的三种包括低维表示、稀疏表示和独立 表示。低维表示尝试将 z 中的信息尽可能压缩在一个较小的表示中。稀疏表示将数 据集嵌人到输人项大多数为零的表示中 (Barlow, 1989; Olshausen and Field, 1996; Hinton and Ghahramani, 1997)。稀疏表示通常用于需要增加表示维数的情况，使得 大部分为零的表示不会丢失很多信息。这会使得表示的整体结构倾向于将数据分布 在表示空间的坐标轴上。独立表示试图分开数据分布中变化的来源，使得表示的维 度是统计独立的。</p>

<p>当然这三个标准并非相互排斥的。低维表示通常会产生比原始的高维数据具有</p>

<p>较少或较弱依赖关系的元素。这是因为减少表示大小的一种方式是找到并消除冗余</p>

<p>识别并去除更多的冗余使得降维算法在丢失更少信息的同时显现更大的压缩。</p>

<p>表示的概念是深度学习核心主题之一，因此也是本书的核心主题之一。本节会</p>

<p>介绍表示学习算法中的一些简单示例。总的来说，这些示例算法会说明如何实施上</p>

<p>面的三个标准。剩余的大部分章节会介绍额外的表示学习算法，它们以不同方式处</p>

<p>理这三个标准或是引人其他标准。</p>

<p>5.8.1 主成分分析
在第2.12节中，我们看到PCA算法提供了一种压缩数据的方式。我们也可以 将 PCA 视为学习数据表示的无监督学习算法。这种表示基于上述简单表示的两个标 准。PCA学习一种比原始输人维数更低的表示。它也学习了一种元素之间彼此没有 线性相关的表示。这是学习表示中元素统计独立标准的第一步。要实现完全独立性 表示学习算法也必须去掉变量间的非线性关系。</p>

<p>如图5.8所示，PCA将输人a投影表示成z，学习数据的正交线性变换。在 第2.12节中，我们看到了如何学习重建原始数据的最佳一维表示（就均方误差而 言），这种表示其实对应着数据的第一个主要成分。因此，我们可以用PCA作为保</p>

<p>留数据尽可能多信息的降维方法（再次就最小重构误差平方而言）。在下文中，我们</p>

<p>将研究PCA表示如何使原始数据表示X去相关的.</p>

<p>图5.8: PCA学习一种线性投影，使最大方差的方向和新空间的轴对齐。f左J原始数据包含了 a 的样本。在这个空间中，方差的方向与轴的方向并不是对齐的。（右」变换过的数据z= aT研在 轴 z1 的方向上有最大的变化。第二大变化方差的方向沿着轴 z2。</p>

<p>假设有一个m x n的设计矩阵X，数据的均值为零，E[a] = 0。若非如此，通 过预处理步骤使所有样本减去均值，数据可以很容易地中心化。</p>

<p>X 对应的无偏样本协方差矩阵给定如下</p>

<p>Var [a] = ^^ XTX.    （5.85）</p>

<p>m - 1</p>

<p>PCA通过线性变换找到一个Var[z]是对角矩阵的表示z= WTa。</p>

<p>在第2.12节，我们已知设计矩阵X的主成分由XTX的特征向量给定。从这个 角度，我们有</p>

<p>XTX= WAWt.    （5.86）</p>

<p>本节中，我们会探索主成分的另一种推导。主成分也可以通过奇异值分解（SVD）得 到。具体来说，它们是X的右奇异向量。为了说明这点，假设W是奇异值分解 X = USWT的右奇异向量。以W作为特征向量基，我们可以得到原来的特征向量</p>

<p>方程：</p>

<p>X X = (USWt)T USWt= W^2Wt.    (5.87)</p>

<p>SVD有助于说明PCA后的Var[z]是对角的。使用X的SVD分解，X的方差 可以表示为</p>

<p>Var [x] = ^^ XTX</p>

<p>(5.88)</p>

<p>(5.89)</p>

<p>(5.90)</p>

<p>(5.91)</p>

<p>m-1</p>

<p>=^^ (ux wT)T UX wT</p>

<p>m-1</p>

<p>=^^ wxTuTuswT</p>

<p>m-1</p>

<p>=^^ WX2 wT,</p>

<p>m1</p>

<p>其中，我们使用UTU=I，因为根据奇异值的定义矩阵U是正交的。这表明Z的 协方差满足对角的要求：</p>

<p>Var [z] = ^^ Z Z    (5.92)</p>

<p>m-1</p>

<p>=^^ WTXTXW    (5.93)</p>

<p>m-1</p>

<p>=^^ WT WX2 WT W    (5.94)</p>

<p>m-1</p>

<p>=^^X2，    (5.95)</p>

<p>m1</p>

<p>其中，再次使用SVD的定义有WTW = /。</p>

<p>以上分析指明当我们通过线性变换 W 将数据 x 投影到 z 时，得到的数据表示 的协方差矩阵是对角的(即X2 )，立刻可得z中的元素是彼此无关的。</p>

<p>PCA这种将数据变换为元素之间彼此不相关表示的能力是PCA的一个重要性 质。它是消除数据中未知变化因素的简单表示示例。在PCA中，这个消除是通过寻 找输人空间的一个旋转(由W确定)，使得方差的主坐标和z相关的新表示空间的</p>

<p>基对齐。</p>

<p>虽然相关性是数据元素间依赖关系的一个重要范畴，但我们对于能够消除更复</p>

<p>杂形式的特征依赖的表示学习也很感兴趣。对此，我们需要比简单线性变换更强的</p>

<p>工具。</p>

<p>5.8.2 k-均值聚类
另外一个简单的表示学习算法是k-均值聚类。k-均值聚类算法将训练集分成k 个靠近彼此的不同样本聚类。因此我们可以认为该算法提供了 k-维的one-hot编码 向量h以表示输人a:。当z属于聚类i时，有hi = 1，h的其他项为零。</p>

<p>k-均值聚类提供的one-hot编码也是一种稀疏表示，因为每个输人的表示中大 部分元素为零。之后，我们会介绍能够学习更灵活的稀疏表示的一些其他算法(表 示中每个输人a不只一个非零项)。one-hot编码是稀疏表示的一个极端示例，丢失 了很多分布式表示的优点。 one-hot 编码仍然有一些统计优点(自然地传达了相同聚 类中的样本彼此相似的观点)，也具有计算上的优势，因为整个表示可以用一个单独 的整数表示。</p>

<p>k-均值聚类初始化k个不同的中心点｛M⑴M(k)｝，然后迭代交换两个不同 的步骤直到收敛。步骤一，每个训练样本分配到最近的中心点M(i)所代表的聚类i。 步骤二，每一个中心点M(i)更新为聚类i中所有训练样本a(j)的均值。</p>

<p>关于聚类的一个问题是聚类问题本身是病态的。这是说没有单一的标准去度量</p>

<p>聚类的数据在真实世界中效果如何。我们可以度量聚类的性质，例如类中元素到类</p>

<p>中心点的欧几里得距离的均值。这使我们可以判断从聚类分配中重建训练数据的效</p>

<p>果如何。然而我们不知道聚类的性质是否很好地对应到真实世界的性质。此外，可</p>

<p>能有许多不同的聚类都能很好地对应到现实世界的某些属性。我们可能希望找到和</p>

<p>一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。例</p>

<p>如，假设我们在包含红色卡车图片、红色汽车图片、灰色卡车图片和灰色汽车图片</p>

<p>的数据集上运行两个聚类算法。如果每个聚类算法聚两类，那么可能一个算法将汽</p>

<p>车和卡车各聚一类，另一个根据红色和灰色各聚一类。假设我们还运行了第三个聚</p>

<p>类算法，用来决定类别的数目。这有可能聚成了四类，红色卡车、红色汽车、灰色卡</p>

<p>车和灰色汽车。现在这个新的聚类至少抓住了属性的信息，但是丢失了相似性信息。</p>

<p>红色汽车和灰色汽车在不同的类中，正如红色汽车和灰色卡车也在不同的类中。该</p>

<p>聚类算法没有告诉我们灰色汽车和红色汽车的相似度比灰色卡车和红色汽车的相似</p>

<p>度更高。我们只知道它们是不同的。</p>

<p>这些问题说明了一些我们可能更偏好于分布式表示(相对于 one-hot 表示而言) 的原因。分布式表示可以对每个车辆赋予两个属性——一个表示它颜色，一个表示</p>

<p>它是汽车还是卡车。目前仍然不清楚什么是最优的分布式表示(学习算法如何知道</p>

<p>我们关心的两个属性是颜色和是否汽车或卡车，而不是制造商和车龄？，但是多个</p>

<p>属性减少了算法去猜我们关心哪一个属性的负担，允许我们通过比较很多属性而非</p>

<p>测试一个单一属性来细粒度地度量相似性。</p>

<p>5.9 随机梯度下降
几乎所有的深度学习算法都用到了一个非常重要的算法：随机梯度下降 (stochastic gradient descent, SGD )。随机梯度下降是第4.3节介绍的梯度下降算</p>

<p>法的一个扩展。</p>

<p>机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的</p>

<p>计算代价也更大。</p>

<p>机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。例如</p>

<p>训练数据的负条件对数似然可以写成</p>

<p>m</p>

<p>J(0) = Ex,y.pdata L(a,y, 0) =    L(a ⑴，y⑷，0)，    (5.96)</p>

<p>m i=1</p>

<p>其中L是每个样本的损失L(a，y，0) = -logp(y | a;0)。</p>

<p>对于这些相加的代价函数，梯度下降需要计算</p>

<p>m</p>

<p>▽J⑹=-V VeL(a⑷，y⑴，0).    (5.97)</p>

<p>m i=1</p>

<p>这个运算的计算代价是O(m)。随着训练集规模增长为数十亿的样本，计算一步梯度 也会消耗相当长的时间。</p>

<p>随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。具 体而言，在算法的每一步，我们从训练集中均匀抽出一小批量(minibatch )样本 B = {a(1)，&hellip;，a(m，)}。小批量的数目mz通常是一个相对较小的数，从一到几百。重 要的是，当训练集大小m增长时，m&rsquo;通常是固定的。我们可能在拟合几十亿的样 本时，每次更新计算只用到几百个样本。</p>

<p>梯度的估计可以表示成</p>

<p>1    m’</p>

<p>g = m ▽^ L(a ⑴，y⑴，0).    (5.98)</p>

<p>m    i=1</p>

<p>使用来自小批量 B 的样本。然后，随机梯度下降算法使用如下的梯度下降估计：</p>

<p>0    0 - eg，</p>

<p>(5.99)</p>

<p>其中，e是学习率。</p>

<p>梯度下降往往被认为很慢或不可靠。以前，将梯度下降应用到非凸优化问题被</p>

<p>认为很鲁莽或没有原则。现在，我们知道梯度下降用于本书第二部分中的训练时效</p>

<p>果不错。优化算法不一定能保证在合理的时间内达到一个局部最小值，但它通常能</p>

<p>及时地找到代价函数一个很小的值，并且是有用的。</p>

<p>随机梯度下降在深度学习之外有很多重要的应用。它是在大规模数据上训练大</p>

<p>型线性模型的主要方法。对于固定大小的模型，每一步随机梯度下降更新的计算量</p>

<p>不取决于训练集的大小m。在实践中，当训练集大小增长时，我们通常会使用一个</p>

<p>更大的模型，但这并非是必须的。达到收敛所需的更新次数通常会随训练集规模增 大而增加。然而，当 m 趋向于无穷大时，该模型最终会在随机梯度下降抽样完训练</p>

<p>集上的所有样本之前收敛到可能的最优测试误差。继续增加m不会延长达到模型可 能的最优测试误差的时间。从这点来看，我们可以认为用SGD训练模型的渐近代价</p>

<p>是关于 m 的函数的 O(1) 级别。</p>

<p>在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性模型。 很多核学习算法需要构建一个m x m的矩阵Gij = k(x(i)，x(j))。构建这个矩阵的计 算量是O(m2)。当数据集是几十亿个样本时，这个计算量是不能接受的。在学术界， 深度学习从 2006年开始受到关注的原因是，在数以万计样本的中等规模数据集上 深度学习在新样本上比当时很多热门算法泛化得更好。不久后，深度学习在工业界 受到了更多的关注，因为其提供了一种训练大数据集上的非线性模型的可扩展方式。</p>

<p>我们将会在第八章继续探讨随机梯度下降及其很多改进方法。</p>

<p>5.10 构建机器学习算法
几乎所有的深度学习算法都可以被描述为一个相当简单的配方：特定的数据集、</p>

<p>代价函数、优化过程和模型。</p>

<p>例如，线性回归算法由以下部分组成：X和y构成的数据集，代价函数</p>

<p>J(w，b) = —Ex,y^pdata logpmodel(y 1 x)，    (5-100)</p>

<p>模型是Pmodel(y | X) = N(y;XTW+ b，1)，在大多数情况下，优化算法可以定义为求 解代价函数梯度为零的正规方程。</p>

<p>意识到我们可以替换独立于其他组件的大多数组件，因此我们能得到很多不同</p>

<p>的算法。</p>

<p>通常代价函数至少含有一项使学习过程进行统计估计的成分。最常见的代价函</p>

<p>数是负对数似然，最小化代价函数导致的最大似然估计。</p>

<p>代价函数也可能含有附加项，如正则化项。例如，我们可以将权重衰减加到线</p>

<p>性回归的代价函数中</p>

<p>J(W, b) = A || w&rdquo;2 - Ex,y^Pdata logPmodel(y 1 a).    (5-101)</p>

<p>该优化仍然有闭解。</p>

<p>如果我们将该模型变成非线性的，那么大多数代价函数不再能通过闭解优化。</p>

<p>这就要求我们选择一个迭代数值优化过程，如梯度下降等。</p>

<p>组合模型、代价和优化算法来构建学习算法的配方同时适用于监督学习和无监 督学习。线性回归示例说明了如何适用于监督学习的。无监督学习时，我们需要定 义一个只包含 X 的数据集、一个合适的无监督代价和一个模型。例如，通过指定如</p>

<p>下损失函数可以得到PCA的第一个主向量</p>

<p>J(w) = Ex~Pdata lla- r(a; W)ll2    (5.102)</p>

<p>模型定义为重构函数r(a) = wTaw，并且w有范数为1的限制。</p>

<p>在某些情况下，由于计算原因，我们不能实际计算代价函数。在这种情况下，只</p>

<p>要我们有近似其梯度的方法，那么我们仍然可以使用迭代数值优化近似最小化目标。</p>

<p>尽管有时候不显然，但大多数学习算法都用到了上述配方。如果一个机器学习算</p>

<p>法看上去特别独特或是手动设计的，那么通常需要使用特殊的优化方法进行求解。</p>

<p>有些模型，如决策树或k-均值，需要特殊的优化，因为它们的代价函数有平坦的区 域，使其不适合通过基于梯度的优化去最小化。在我们认识到大部分机器学习算法 可以使用上述配方描述之后，我们可以将不同算法视为出于相同原因解决相关问题 的一类方法，而不是一长串各个不同的算法。</p>

<p>5.11 促使深度学习发展的挑战
本章描述的简单机器学习算法在很多不同的重要问题上效果都良好。但是它们</p>

<p>不能成功解决人工智能中的核心问题，如语音识别或者对象识别。</p>

<p>深度学习发展动机的一部分原因是传统学习算法在这类人工智能问题上泛化能</p>

<p>力不足。</p>

<p>本节介绍为何处理高维数据时在新样本上泛化特别困难，以及为何在传统机器</p>

<p>学习中实现泛化的机制不适合学习高维空间中复杂的函数。这些空间经常涉及巨大</p>

<p>的计算代价。深度学习旨在克服这些以及其他一些难题。</p>

<p>5.11.1 维数灾难
当数据的维数很高时，很多机器学习问题变得相当困难。这种现象被称为维数 灾难(curse of dimensionality )。特别值得注意的是，一组变量不同的可能配置数量 会随着变量数目的增加而指数级增长。</p>

<p>维数灾难发生在计算机科学的许多地方，在机器学习中尤其如此。</p>

<p>由维数灾难带来的一个挑战是统计挑战。如图5.9所示，统计挑战产生于x的 可能配置数目远大于训练样本的数目。为了充分理解这个问题，我们假设输人空间 如图所示被分成单元格。空间是低维时，我们可以用由大部分数据占据的少量单元 格去描述这个空间。泛化到新数据点时，通过检测和新输人点在相同单元格中的训 练样本，我们可以判断如何处理新数据点。例如，如果要估计某点x处的概率密度， 我们可以返回x处单位体积单元格内训练样本的数目除以训练样本的总数。如果我 们希望对一个样本进行分类，我们可以返回相同单元格中训练样本最多的类别。如 果我们是做回归分析，我们可以平均该单元格中样本对应的目标值。但是，如果该单 元格中没有样本，该怎么办呢？因为在高维空间中参数配置数目远大于样本数目，大 部分单元格中没有样本。我们如何能在这些新配置中找到一些有意义的东西呢？许 多传统机器学习算法只是简单地假设在一个新点的输出应大致和最接近的训练点的 输出相同。</p>

<p>5.11.2 局部不变性和平滑正则化
为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型的函数。</p>

<p>此前，我们已经看到过由模型参数的概率分布形成的先验。通俗地讲，我们也可以说</p>

<p>先验信念直接影响函数本身，而仅仅通过它们对函数的影响来间接改变参数。此外</p>

<p>我们还能通俗地说，先验信念还间接地体现在选择一些偏好某类函数的算法，尽管</p>

<p>这些偏好并没有通过我们对不同函数置信程度的概率分布表现出来(也许根本没法</p>

<p>图 5.9: 当数据的相关维度增大时（从左向右），我们感兴趣的配置数目会随之指数级增长。（ 左 ） 在 这个一维的例子中，我们用一个变量来区分所感兴趣的 10 个区域。当每个区域都有足够的样本数 时（每个区域对应图中的一个单元格），学习算法能够轻易地泛化得很好。泛化的一个直接方法是 估计目标函数在每个区域的值（可能是在相邻区域之间插值）。（中） 在二维情况下，对每个变量区</p>

<p>分10个不同的值更加困难。我们需要追踪10 X 10 = 100个区域，至少需要很多样本来覆盖所有</p>

<p>的区域。（右） 三维情况下，区域数量增加到了 103 = 1000，至少需要那么多的样本。对于需要区 分的 d 维以及 v 个值来说，我们需要 O（vd） 个区域和样本。这就是维数灾难的一个示例。感谢由 Nicolas Chapados 提供的图片。</p>

<p>表现）。</p>

<p>其中最广泛使用的隐式“先验’’是平滑先验（smoothness prior），或局部不变 性先验（local constancy prior ）。这个先验表明我们学习的函数不应在小区域内发生</p>

<p>很大的变化。</p>

<p>许多简单算法完全依赖于此先验达到良好的泛化，其结果是不能推广去解决人</p>

<p>工智能级别任务中的统计挑战。本书中，我们将介绍深度学习如何引人额外的（显</p>

<p>式或隐式的）先验去降低复杂任务中的泛化误差。这里，我们解释为什么仅依靠平</p>

<p>滑先验不足以应对这类任务。</p>

<p>有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或局部不变的先</p>

<p>验。所有这些不同的方法都旨在鼓励学习过程能够学习出函数f*，对于大多数设置 a和小变动e，都满足条件</p>

<p>f * （a） f * （a + e）.    （5.103）</p>

<p>换言之，如果我们知道对应输人 a 的答案（例如， a 是个有标签的训练样本），那么</p>

<p>该答案对于 a 的邻域应该也适用。如果在有些邻域中我们有几个好答案，那么我们</p>

<p>可以组合它们（通过某种形式的平均或插值法）以产生一个尽可能和大多数输人一</p>

<p>致的答案。</p>

<p>局部不变方法的一个极端例子是k-最近邻系列的学习算法。当一个区域里的所 有点a在训练集中的k个最近邻是一样的，那么对这些点的预测也是一样的。当 k=1 时，不同区域的数目不会比训练样本还多。</p>

<p>虽然k-最近邻算法复制了附近训练样本的输出，大部分核机器也是在和附近训 练样本相关的训练集输出上插值。一类重要的核函数是局部核(local kernel)，其核 函数k(M，0在M r时很大，当M和r距离拉大时而减小。局部核可以看作是执 行模版匹配的相似函数，用于度量测试样本 a 和每个训练样本 a(i) 有多么相似。近 年来深度学习的很多推动力源自研究局部模版匹配的局限性，以及深度学习如何克 服这些局限性 (Bengio et al., 2006a)。</p>

<p>决策树也有平滑学习的局限性，因为它将输人空间分成和叶节点一样多的区间 并在每个区间使用单独的参数(或者有些决策树的拓展有多个参数)。如果目标函数 需要至少拥有 n 个叶节点的树才能精确表示，那么至少需要 n 个训练样本去拟合。 需要几倍于 n 的样本去达到预测输出上的某种统计置信度。</p>

<p>总的来说，区分输人空间中 O(k) 个区间，所有的这些方法需要 O(k) 个样本。 通常会有O(k)个参数，0(1)参数对应于O(k)区间之一。最近邻算法中，每个训 练样本至多用于定义一个区间，如图5.10所示。</p>

<p>图 5.10: 最近邻算法如何划分输人空间的示例。每个区域内的一个样本(这里用圆圈表示)定义了 区域边界(这里用线表示)。每个样本相关的 y 值定义了对应区域内所有数据点的输出。由最近 邻定义并且匹配几何模式的区域被称为 Voronoi 图。这些连续区域的数量不会比训练样本的数量 增加得更快。尽管此图具体说明了最近邻算法的效果，其他的单纯依赖局部光滑先验的机器学习 算法也表现出了类似的泛化能力：每个训练样本仅仅能告诉学习者如何在其周围的相邻区域泛化。</p>

<p>有没有什么方法能表示区间数目比训练样本数目还多的复杂函数？显然，只是</p>

<p>假设函数的平滑性不能做到这点。例如，想象目标函数作用在西洋跳棋盘上。棋盘包</p>

<p>含许多变化，但只有一个简单的结构。想象一下，如果训练样本数目远小于棋盘上</p>

<p>的黑白方块数目，那么会发生什么。基于局部泛化和平滑性或局部不变性先验，如</p>

<p>果新点和某个训练样本位于相同的棋盘方块中，那么我们能够保证正确地预测新点</p>

<p>的颜色。但如果新点所在的方块没有训练样本，学习器不一定能举一反三。如果仅</p>

<p>依靠这个先验，一个样本只能告诉我们它所在的方块的颜色。获得整个棋盘颜色的</p>

<p>唯一方法是其上的每个方块至少要有一个样本。</p>

<p>只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和</p>

<p>相关的无参数学习算法的效果都非常好。当要学习的函数足够平滑，并且只在少数</p>

<p>几维变化，这样做一般没问题。在高维空间中，即使是非常平滑的函数，也会在不</p>

<p>同维度上有不同的变化方式。如果函数在不同的区间中表现不一样，那么就非常难</p>

<p>用一组训练样本去刻画函数。如果函数是复杂的(我们想区分多于训练样本数目的</p>

<p>大量区间)，有希望很好地泛化么？</p>

<p>这些问题，即是否可以有效地表示复杂的函数以及所估计的函数是否可以很好 地泛化到新的输入，答案是有。关键观点是，只要我们通过额外假设生成数据的分 布来建立区域间的依赖关系，那么O(k)个样本足以描述多如O(2k)的大量区间。通 过这种方式，我们确实能做到非局部的泛化 (Bengio and Monperrus, 2005; Bengio et al., 2006b)。为了利用这些优势，许多不同的深度学习算法都提出了一些适用于多 种AI任务的隐式或显式的假设。</p>

<p>一些其他的机器学习方法往往会提出更强的，针对特定问题的假设。例如，假</p>

<p>设目标函数是周期性的，我们很容易解决棋盘问题。通常，神经网络不会包含这些很</p>

<p>强的(针对特定任务的)假设，因此神经网络可以泛化到更广泛的各种结构中。人</p>

<p>工智能任务的结构非常复杂，很难限制到简单的、人工手动指定的性质，如周期性</p>

<p>因此我们希望学习算法具有更通用的假设。深度学习的核心思想是假设数据由因素</p>

<p>或特征组合产生，这些因素或特征可能来自一个层次结构的多个层级。许多其他类</p>

<p>似的通用假设进一步提高了深度学习算法。这些很温和的假设允许了样本数目和可</p>

<p>区分区间数目之间的指数增益。这类指数增益将在第6.4.1节、第15.4节和第15.5节</p>

<p>中更详尽地介绍。深度的分布式表示带来的指数增益有效地解决了维数灾难带来的</p>

<p>挑战。
5.11.3 流形学习
流形是一个机器学习中很多想法内在的重要概念。</p>

<p>流形(manifold)指连接在一起的区域。数学上，它是指一组点，且每个点都 有其邻域。给定一个任意的点，其流形局部看起来像是欧几里得空间。日常生活中</p>

<p>我们将地球视为二维平面，但实际上它是三维空间中的球状流形。</p>

<p>每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位置。例</p>

<p>如在地球表面这个流形中，我们可以朝东南西北走。</p>

<p>尽管术语 “流形&rdquo; 有正式的数学定义，但是机器学习倾向于更松散地定义一组 点，只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地近似。每一维都</p>

<p>对应着局部的变化方向。如图5.11所示，训练数据位于二维空间中的一维流形中。</p>

<p>在机器学习中，我们允许流形的维数从一个点到另一个点有所变化。这经常发生于</p>

<p>流形和自身相交的情况中。例如，数字 “8&rdquo; 形状的流形在大多数位置只有一维，但 在中心的相交处有两维。</p>

<p>图 5.11: 从一个二维空间的分布中抽取的数据样本，这些样本实际上聚集在一维流形附近，像一个 缠绕的带子。实线代表学习器应该推断的隐式流形。</p>

<p>如果我们希望机器学习算法学习整个Rn上有趣变化的函数，那么很多机器学 习问题看上去都是无望的。流形学习(manifold learning )算法通过一个假设来克服 这个障碍，该假设认为 Rn 中大部分区域都是无效的输入，有意义的输入只分布在包 含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿 着流形的方向或仅发生在我们切换到另一流形时。流形学习最初用于连续数值和无</p>

<p>监督学习的环境，尽管这个概率集中的想法也能够泛化到离散数据和监督学习的设</p>

<p>定下：关键假设仍然是概率质量高度集中。</p>

<p>图 5.12: 随机地均匀抽取图像(根据均匀分布随机地选择每一个像素)会得到噪声图像。尽管在人 工智能应用中以这种方式生成一个脸或者其他物体的图像是非零概率的，但是实际上我们从来没 有观察到这种现象。这也意味着人工智能应用中遇到的图像在所有图像空间中的占比可以是忽略</p>

<p>不计的。</p>

<p>数据位于低维流形的假设并不总是对的或者有用的。我们认为在人工智能的一</p>

<p>些场景中，如涉及到处理图像、声音或者文本时，流形假设至少是近似对的。这个</p>

<p>假设的支持证据包含两类观察结果。</p>

<p>第一个支持流形假设(manifold hypothesis )的观察是现实生活中的图像、文</p>

<p>本、声音的概率分布都是高度集中的。均匀的噪声从来不会与这类领域的结构化输 入类似。图5.12显示均匀采样的点看上去像是没有信号时模拟电视上的静态模式。</p>

<p>同样，如果我们均匀地随机抽取字母来生成文件，能有多大的概率得到一个有意义</p>

<p>的英语文档？几乎是零。因为大部分字母长序列不对应着自然语言序列：自然语言</p>

<p>序列的分布只占了字母序列的总空间里非常小的一部分。</p>

<p>当然，集中的概率分布不足以说明数据位于一个相当小的流形中。我们还必须</p>

<p>确保，我们遇到的样本和其他样本相互连接，每个样本被其他高度相似的样本包围</p>

<p>而这些高度相似的样本可以通过变换来遍历该流形得到。支持流形假设的第二个论</p>

<p>点是，我们至少能够非正式地想象这些邻域和变换。在图像中，我们当然会认为有</p>

<p>很多可能的变换仍然允许我们描绘出图片空间的流形：我们可以逐渐变暗或变亮光</p>

<p>泽、逐步移动或旋转图中对象、逐渐改变对象表面的颜色等等。在大多数应用中很</p>

<p>有可能会涉及到多个流形。例如，人脸图像的流形不太可能连接到猫脸图像的流形。</p>

<p>这些支持流形假设的思维实验传递了一些支持它的直观理由。更严格的实</p>

<p>验(Cayton, 2005; Narayanan and Mitter, 2010; Scholkopf et al., 1998a; Roweis and Saul, 2000; Tenenbaum et al., 2000; Brand, 2003a; Belkin and Niyogi, 2003b; Donoho and Grimes, 2003; Weinberger and Saul, 2004a) 在人工智能中备受关注的一大类数 据集上支持了这个假设。</p>

<p>当数据位于低维流形中时，使用流形中的坐标而非收&rdquo;中的坐标表示机器学习数 据更为自然。日常生活中，我们可以认为道路是嵌入在三维空间的一维流形。我们 用一维道路中的地址号码确定地址，而非三维空间中的坐标。提取这些流形中的坐 标是非常具有挑战性的，但是很有希望改进许多机器学习算法。这个一般性原则能 够用在很多情况中。图5.13展示了包含人脸的数据集的流形结构。在本书的最后 我们会介绍一些学习这样的流形结构的必备方法。在图20.6中，我们将看到机器学 习算法如何成功完成这个目标。</p>

<p>第一部分介绍了数学和机器学习中的基本概念，这将用于本书其他章节中。至</p>

<p>此，我们已经做好了研究深度学习的准备。</p>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">03 概率与信息论</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">
            <span class="next-text nav-default">07 深度学习中的正则化</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
