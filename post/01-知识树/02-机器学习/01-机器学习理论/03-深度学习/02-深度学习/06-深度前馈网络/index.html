<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>06 深度前馈网络 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="ORIGINAL 《机器学习》周志华 TODO aaa INTRODUCTION aaa 第六章 深度前馈网络 深度前馈网络(deep feedforward network )，也叫作前馈神经网络(feedforward neural network )或者多层感知机(" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="06 深度前馈网络" />
<meta property="og:description" content="ORIGINAL 《机器学习》周志华 TODO aaa INTRODUCTION aaa 第六章 深度前馈网络 深度前馈网络(deep feedforward network )，也叫作前馈神经网络(feedforward neural network )或者多层感知机(" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/" /><meta property="article:published_time" content="2018-08-21T18:16:23&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-21T18:16:23&#43;00:00"/>
<meta itemprop="name" content="06 深度前馈网络">
<meta itemprop="description" content="ORIGINAL 《机器学习》周志华 TODO aaa INTRODUCTION aaa 第六章 深度前馈网络 深度前馈网络(deep feedforward network )，也叫作前馈神经网络(feedforward neural network )或者多层感知机(">


<meta itemprop="datePublished" content="2018-08-21T18:16:23&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-21T18:16:23&#43;00:00" />
<meta itemprop="wordCount" content="39212">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="06 深度前馈网络"/>
<meta name="twitter:description" content="ORIGINAL 《机器学习》周志华 TODO aaa INTRODUCTION aaa 第六章 深度前馈网络 深度前馈网络(deep feedforward network )，也叫作前馈神经网络(feedforward neural network )或者多层感知机("/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">06 深度前馈网络</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-21 </span>
        
        <span class="more-meta"> 39212 words </span>
        <span class="more-meta"> 79 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="original">ORIGINAL</h1>

<ol>
<li>《机器学习》周志华</li>
</ol>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>第六章 深度前馈网络
深度前馈网络(deep feedforward network )，也叫作前馈神经网络(feedforward neural network )或者多层感知机(multilayer perceptron, MLP )，是典型的深度学 习模型。前馈网络的目标是近似某个函数广。例如，对于分类器，y = f*⑷将输人 映射到一个类别y。前馈网络定义了一个映射y = f(a;;0)，并且学习参数0的值，</p>

<p>使它能够得到最佳的函数近似。</p>

<p>这种模型被称为前向(feedforward)的，是因为信息流过z的函数，流经用于 定义f的中间计算过程，最终到达输出y。在模型的输出和模型本身之间没有反馈 (feedback)连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经 网络 (recurrent neural network )，在第十章介绍。</p>

<p>前馈网络对于机器学习的从业者是极其重要的。它们是许多重要商业应用的基</p>

<p>础。例如，用于对照片中的对象进行识别的卷积神经网络就是一种专门的前馈网络。</p>

<p>前馈网络是通往循环网络之路的概念基石，后者在自然语言的许多应用中发挥着巨</p>

<p>大作用。</p>

<p>前馈神经网络被称作网络(network)是因为它们通常用许多不同函数复合 在一起来表示。该模型与一个有向无环图相关联， 而图描述了函数是如何复</p>

<p>合在一起的。例如， 我们有三个函数 f(1),f(2) 和 f(3) 连接在一个链上以形成 f(X)= f⑶f (2)(f⑴(勾))。这些链式结构是神经网络中最常用的结构。在这种情况 下，f(1)被称为网络的第一层(first layer)，f(2)被称为第二层(second layer)，以 此类推。链的全长称为模型的深度(depth)。正是因为这个术语才出现了 “深度学 习”这个名字。前馈网络的最后一层被称为输出层(output layer)。在神经网络训练 的过程中，我们让f(x)去匹配广(X)的值。训练数据为我们提供了在不同训练点上 取值的、含有噪声的f<em>(x)的近似实例。每个样本X都伴随着一个标签y » f</em>(x)。 训练样本直接指明了输出层在每一点X上必须做什么；它必须产生一个接近y的值。</p>

<p>但是训练数据并没有直接指明其他层应该怎么做。学习算法必须决定如何使用这些</p>

<p>层来产生想要的输出，但是训练数据并没有说每个单独的层应该做什么。相反，学</p>

<p>习算法必须决定如何使用这些层来最好地实现广的近似。因为训练数据并没有给出 这些层中的每一层所需的输出，所以这些层被称为隐藏层(hidden layer )。</p>

<p>最后， 这些网络被称为神经网络是因为它们或多或少地受到神经科学的启 发。网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度</p>

<p>(width)。向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想 象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元(unit) 组成，每个单元表示一个向量到标量的函数。每个单元在某种意义上类似一个神经 元，它接收的输入来源于许多其他的单元，并计算它自己的激活值。使用多层向量值 表示的想法来源于神经科学。用于计算这些表示的函数 f(i)(x) 的选择，也或多或少 地受到神经科学观测的指引，这些观测是关于生物神经元计算功能的。然而，现代 的神经网络研究受到更多的是来自许多数学和工程学科的指引，并且神经网络的目 标并不是完美地给大脑建模。我们最好将前馈神经网络想成是为了实现统计泛化而 设计出的函数近似机，它偶尔从我们了解的大脑中提取灵感，但并不是大脑功能的 模型。</p>

<p>一种理解前馈网络的方式是从线性模型开始，并考虑如何克服它的局限性。线</p>

<p>性模型，例如逻辑回归和线性回归，是非常吸引人的，因为无论是通过闭解形式还</p>

<p>是使用凸优化，它们都能高效且可靠地拟合。线性模型也有明显的缺陷，那就是该</p>

<p>模型的能力被局限在线性函数里，所以它无法理解任何两个输入变量间的相互作用。</p>

<p>为了扩展线性模型来表示的非线性函数，我们可以不把线性模型用于本身， 而是用在一个变换后的输人我⑹上，这里4是一个非线性变换。同样，我们可以 使用第5.7.2节中描述的核技巧，来得到一个基于隐含地使用映射的非线性学习算 法。我们可以认为4提供了一组描述的特征，或者认为它提供了 的一个新的表 示。</p>

<p>剩下的问题就是如何选择映射 4。</p>

<ol>
<li>其中一种选择是使用一个通用的 4， 例如无限维的 4， 它隐含地用在基 于RBF核的核机器上。如果4(勾具有足够高的维数，我们总是有足够的能力</li>
</ol>

<p>来拟合训练集，但是对于测试集的泛化往往不佳。非常通用的特征映射通常只</p>

<p>基于局部光滑的原则，并且没有将足够的先验信息进行编码来解决高级问题。</p>

<ol>
<li>另一种选择是手动地设计 4。在深度学习出现以前，这一直是主流的方法。这</li>
</ol>

<p>种方法对于每个单独的任务都需要人们数十年的努力，从业者各自擅长特定的 领域(如语音识别或计算机视觉)，并且不同领域之间很难迁移 (transfer)。</p>

<p>3.深度学习的策略是去学习么在这种方法中，我们有一个模型y = f (% 0, w)= ＜Xx; 0)Tw。我们现在有两种参数：用于从一大类函数中学习4的参数0，以及 用于将＜Xx)映射到所需的输出的参数w。这是深度前馈网络的一个例子，其</p>

<p>中 4 定义了一个隐藏层。这是三种方法中唯一一种放弃了训练问题的凸性的</p>

<p>但是利大于弊。在这种方法中，我们将表示参数化为＜Xx;0)，并且使用优化算</p>

<p>法来寻找 0，使它能够得到一个好的表示。如果我们想要的话，这种方法也可 以通过使它变得高度通用以获得第一种方法的优点——我们只需使用一个非常 广泛的函数族＜Xx;0)。这种方法也可以获得第二种方法的优点。人类专家可以</p>

<p>将他们的知识编码进网络来帮助泛化，他们只需要设计那些他们期望能够表现</p>

<p>优异的函数族 4(x;0) 即可。这种方法的优点是人类设计者只需要寻找正确的 函数族即可，而不需要去寻找精确的函数。</p>

<p>这种通过学习特征来改善模型的一般化原则不仅仅适用于本章描述的前馈神经</p>

<p>网络。它是深度学习中反复出现的主题，适用于全书描述的所有种类的模型。前馈 神经网络是这个原则的应用，它学习从x到y的确定性映射并且没有反馈连接。后 面出现的其他模型会把这些原则应用到学习随机映射、学习带有反馈的函数以及学</p>

<p>习单个向量的概率分布。</p>

<p>本章我们先从前馈网络的一个简单例子说起。接着，我们讨论部署一个前馈网 络所需的每个设计决策。首先，训练一个前馈网络至少需要做和线性模型同样多的设 计决策：选择一个优化模型、代价函数以及输出单元的形式。我们先回顾这些基于梯 度学习的基本知识，然后去面对那些只出现在前馈网络中的设计决策。前馈网络已经 引人了隐藏层的概念，这需要我们去选择用于计算隐藏层值的激活函数(activation function)。我们还必须设计网络的结构，包括网络应该包含多少层、这些层应该如 何连接，以及每一层包含多少单元。在深度神经网络的学习中需要计算复杂函数的 梯度。我们给出反向传播(back propagation )算法和它的现代推广，它们可以用来 高效地计算这些梯度。最后，我们以某些历史观点来结束这一章。</p>

<p>6.1 实例：学习 XOR
为了使前馈网络的想法更加具体，我们首先从一个可以完整工作的前馈网络说 起。这个例子解决一个非常简单的任务：学习 XOR 函数。</p>

<p>XOR函数(“异或”逻辑)是两个二进制值xi和X2的运算。当这些二进制值 中恰好有一个为1时，XOR函数返回值为1。其余情况下返回值为0。XOR函数提 供了我们想要学习的目标函数y = f&rsquo;⑷。我们的模型给出了一个函数y = f(%0) 并且我们的学习算法会不断调整参数0来使得f尽可能接近广。</p>

<p>在这个简单的例子中，我们不会关心统计泛化。我们希望网络在这四个点 X= {[0,0]T,[0,1]T,[1,0]T,[1,1]T}上表现正确。我们会用全部这四个点来训练我们</p>

<p>的网络，唯一的挑战是拟合训练集。</p>

<p>我们可以把这个问题当作是回归问题，并使用均方误差损失函数。我们选择这 个损失函数是为了尽可能简化本例中用到的数学。在应用领域，对于二进制数据建 模时，MSE通常并不是一个合适的损失函数。更加合适的方法将在第6.2.2.2节中讨</p>

<p>论。</p>

<p>评估整个训练集上表现的 MSE 损失函数为</p>

<p>J(0) = | [(f*(x) - f(x;0))2. (6-1)</p>

<p>xGX</p>

<p>我们现在必须要选择我们模型 f(x;0) 的形式。假设我们选择一个线性模型， 0 包含W和b，那么我们的模型被定义成</p>

<p>f(x; w, b) = xTw + b. (6.2)</p>

<p>我们可以使用正规方程关于W和b最小化J(0)，来得到一个闭式解。</p>

<p>解正规方程以后，我们得到w = 0以及b = 1。线性模型仅仅是在任意一点都输 出0.5。为什么会发生这种事？图6.1演示了线性模型为什么不能用来表示XOR函 数。解决这个问题的其中一种方法是使用一个模型来学习一个不同的特征空间，在 这个空间上线性模型能够表示这个解。</p>

<p>具体来说，我们这里引人一个非常简单的前馈神经网络，它有一层隐藏层并且隐 藏层中包含两个单元。见图6.2中对该模型的解释。这个前馈网络有一个通过函数 f(1)(x; W,c)计算得到的隐藏单元的向量心这些隐藏单元的值随后被用作第二层的 输人。第二层就是这个网络的输出层。输出层仍然只是一个线性回归模型，只不过</p>

<p>Original x space</p>

<p>Learned h space</p>

<p>1</p>

<p>1</p>

<p>1</p>

<p>1</p>

<p>0</p>

<p>1</p>

<p>1 1 1</p>

<ul>
<li>0 -</li>
</ul>

<p>0</p>

<p>0</p>

<p>1</p>

<p>1</p>

<p>1</p>

<p>0</p>

<ul>
<li>0 1</li>
</ul>

<p>1 1 1</p>

<p>0</p>

<p>1</p>

<p>0 1 2</p>

<p>x1 h1</p>

<p>图 6.1: 通过学习一个表示来解决 XOR 问题。图上的粗体数字标明了学得的函数必须在每个点输 出的值。(左)直接应用于原始输人的线性模型不能实现 XOR 函数。当 x1 =0时，模型的输出必</p>

<p>须随着X2的增大而增大。当X1 = 1时，模型的输出必须随着X2的增大而减小。线性模型必须对 X2使用固定的系数W2。因此，线性模型不能使用X1的值来改变X2的系数，从而不能解决这个</p>

<p>问题。 (右) 在由神经网络提取的特征表示的变换空间中，线性模型现在可以解决这个问题了。在 我们的示例解决方案中，输出必须为1的两个点折叠到了特征空间中的单个点。换句话说，非线</p>

<p>性特征将X= [1,0]T和X = [0, 1]T都映射到了特征空间中的单个点fo= [1, 0]T。线性模型现在可</p>

<p>以将函数描述为 h1 增大和 h2 减小。在该示例中，学习特征空间的动机仅仅是使得模型的能力更 大，使得它可以拟合训练集。在更现实的应用中，学习的表示也可以帮助模型泛化。</p>

<p>现在它作用于而不是X。网络现在包含链接在一起的两个函数：h = f⑴(x; W, c) 和 y = f ⑵(h; w,b)，完整的模型是 f(x; W, c, w, b) = f ⑵(f ⑴(x))。</p>

<p>f(1) 应该是哪种函数？线性模型到目前为止都表现不错，让 f(1) 也是线性的似 乎很有诱惑力。可惜的是，如果f(1)是线性的，那么前馈网络作为一个整体对于输 人仍然是线性的。暂时忽略截距项，假设f(1)(x) = Wtx并且f(2)(h) = hTw，那么 f (x) = wT WTX。我们可以将这个函数重新表示成f(x) = xTW其中wz = Ww。</p>

<p>显然，我们必须用非线性函数来描述这些特征。大多数神经网络通过仿射变换之 后紧跟着一个被称为激活函数的固定非线性函数来实现这个目标，其中仿射变换由 学得的参数控制。我们这里使用这种策略，定义h= g(WTx+c)，其中W是线性 变换的权重矩阵， c 是偏置。此前，为了描述线性回归模型，我们使用权重向量和一 个标量的偏置参数来描述从输人向量到输出标量的仿射变换。现在，因为我们描述 的是向量X到向量h的仿射变换，所以我们需要一整个向量的偏置参数。激活函数 g通常选择对每个元素分别起作用的函数，有hi = g(xTW：,i + ci)。在现代神经网络</p>

<p>图6.2:使用两种不同样式绘制的前馈网络的示例。具体来说，这是我们用来解决XOR问题的前</p>

<p>馈网络。它有单个隐藏层，包含两个单元。（左） 在这种样式中，我们将每个单元绘制为图中的一个 节点。这种风格是清楚而明确的，但对于比这个例子更大的网络，它可能会消耗太多的空间。 （右） 在这种样式中，我们将表示每一层激活的整个向量绘制为图中的一个节点。这种样式更加紧凑。有 时，我们对图中的边使用参数名进行注释，这些参数是用来描述两层之间的关系的。这里，我们用</p>

<p>矩阵研描述从x到的映射，用向量w描述从到y的映射。当标记这种图时，我们通常省</p>

<p>略与每个层相关联的截距参数。</p>

<p>中，默认的推荐是使用由激活函数 g(z) = max{0, z} 定义的 整流线性单元( rectified</p>

<p>linear unit)或者称为 ReLU (Jarrett et al., 2009b; Nair and Hinton, 2010a; Glorot et al., 2011a)，如图 6.3所示。</p>

<p>我们现在可以指明我们的整个网络是</p>

<p>f(x; W, c, w, b) = wT max{0, WTx + c} + b. (6.3)</p>

<p>我们现在可以给出XOR问题的一个解。令</p>

<p>W=</p>

<p>11</p>

<p>11</p>

<p>(6.4)</p>

<p>(6.5)</p>

<p>(6.6)</p>

<p>0</p>

<p>c=</p>

<p>1 1</p>

<p>w=</p>

<p>2</p>

<p>以及 b = 0。</p>

<p>我们现在可以了解这个模型如何处理一批输人。令X表示设计矩阵，它包含二</p>

<p>图 6.3: 整流线性激活函数。该激活函数是被推荐用于大多数前馈神经网络的默认激活函数。将此 函数用于线性变换的输出将产生非线性变换。然而，函数仍然非常接近线性，在这种意义上它是 具有两个线性部分的分段线性函数。由于整流线性单元几乎是线性的，因此它们保留了许多使得 线性模型易于使用基于梯度的方法进行优化的属性。它们还保留了许多使得线性模型能够泛化良 好的属性。计算机科学的一个通用原则是，我们可以从最小的组件构建复杂的系统。就像图灵机 的内存只需要能够存储 0 或 1 的状态，我们可以从整流线性函数构建一个万能函数近似器。</p>

<p>进制输人空间中全部的四个点，每个样本占一行，那么矩阵表示为：</p>

<p>X=</p>

<p>00</p>

<p>01</p>

<p>10</p>

<p>11</p>

<p>神经网络的第一步是将输人矩阵乘以第一层的权重矩阵：</p>

<p>XW =</p>

<p>然后，我们加上偏置向量C，得到</p>

<p>(6.7)</p>

<p>00</p>

<p>11</p>

<p>11</p>

<p>22</p>

<p>0 -1</p>

<p>10</p>

<p>10</p>

<p>21</p>

<p>(6.8)</p>

<p>(6.9)</p>

<p>在这个空间中，所有的样本都处在一条斜率为 1 的直线上。当我们沿着这条直线移 动时，输出需要从0升到 1，然后再降回0。线性模型不能实现这样一种函数。为了</p>

<p>用 fo 对每个样本求值，我们使用整流线性变换：</p>

<p>0</p>

<p>1</p>

<p>(6.10)</p>

<p>1</p>

<p>2</p>

<p>这个变换改变了样本间的关系。它们不再处于同一条直线上了。如图6.1所示</p>

<p>它们现在处在一个可以用线性模型解决的空间上。</p>

<p>我们最后乘以一个权重向量 w:</p>

<p>0</p>

<p>1</p>

<p>(6.11)</p>

<p>1</p>

<p>0</p>

<p>神经网络对这一批次中的每个样本都给出了正确的结果。</p>

<p>在这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为零。在 实际情况中，可能会有数十亿的模型参数以及数十亿的训练样本，所以不能像我们 这里做的那样进行简单地猜解。与之相对的，基于梯度的优化算法可以找到一些参 数使得产生的误差非常小。我们这里给出的XOR问题的解处在损失函数的全局最 小点，所以梯度下降算法可以收敛到这一点。梯度下降算法还可以找到XOR问题一 些其他的等价解。梯度下降算法的收敛点取决于参数的初始值。在实践中，梯度下 降通常不会找到像我们这里给出的那种干净的、容易理解的、整数值的解。</p>

<p>6.2 基于梯度的学习
设计和训练神经网络与使用梯度下降训练其他任何机器学习模型并没有太大不</p>

<p>同。在第5.10节中，我们描述了如何通过指定一个优化过程、代价函数和一个模型</p>

<p>族来构建一个机器学习算法。</p>

<p>我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线</p>

<p>性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常使</p>

<p>用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于 训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或 SVM 的凸优化算 法那样保证全局收敛。凸优化从任何一种初始参数出发都会收敛（理论上如此——</p>

<p>在实践中也很鲁棒但可能会遇到数值问题）。用于非凸损失函数的随机梯度下降没有</p>

<p>这种收敛性保证，并且对参数的初始值很敏感。对于前馈神经网络，将所有的权重</p>

<p>值初始化为小随机数是很重要的。偏置可以初始化为零或者小的正值。这种用于训</p>

<p>练前馈神经网络以及几乎所有深度模型的迭代的基于梯度的优化算法会在第八章详</p>

<p>细介绍，参数初始化会在第8.4节中具体说明。就目前而言，只需要懂得，训练算法</p>

<p>几乎总是基于使用梯度来使得代价函数下降的各种方法即可。一些特别的算法是对</p>

<p>梯度下降思想的改进和提纯（在第4.3节中介绍）还有一些更特别的，大多数是对随</p>

<p>机梯度下降算法的改进（在第5.9节中介绍）。</p>

<p>我们当然也可以用梯度下降来训练诸如线性回归和支持向量机之类的模型，并</p>

<p>且事实上当训练集相当大时这是很常用的。从这点来看，训练神经网络和训练其他</p>

<p>任何模型并没有太大区别。计算梯度对于神经网络会略微复杂一些，但仍然可以很</p>

<p>高效而精确地实现。第6.5节将会介绍如何用反向传播算法以及它的现代扩展算法来</p>

<p>求得梯度。</p>

<p>和其他的机器学习模型一样，为了使用基于梯度的学习方法我们必须选择一个</p>

<p>代价函数，并且我们必须选择如何表示模型的输出。现在，我们重温这些设计上的</p>

<p>考虑，并且特别强调神经网络的情景。</p>

<p>6.2.1 代价函数
深度神经网络设计中的一个重要方面是代价函数的选择。幸运的是，神经网络</p>

<p>的代价函数或多或少是和其他的参数模型例如线性模型的代价函数相同的。</p>

<p>在大多数情况下，我们的参数模型定义了一个分布p（y I x;0）并且我们简单地 使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函 数。</p>

<p>有时，我们使用一个更简单的方法，不是预测y的完整概率分布，而是仅仅预 测在给定 X 的条件下 y 的某种统计量。某些专门的损失函数允许我们来训练这些估 计量的预测器。</p>

<p>用于训练神经网络的完整的代价函数，通常在我们这里描述的基本代价函数的</p>

<p>基础上结合一个正则项。我们已经在第5.2.2节中看到正则化应用到线性模型中的一</p>

<p>些简单的例子。用于线性模型的权重衰减方法也直接适用于深度神经网络，而且是</p>

<p>最流行的正则化策略之一。用于神经网络的更高级的正则化策略将在第七章中讨论。</p>

<p>6.2.1.1 使用最大似然学习条件分布</p>

<p>大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数</p>

<p>似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为</p>

<p>J(0) = —Ex,y^pdata log pmodel(y 1 x). (6-12)</p>

<p>代价函数的具体形式随着模型而改变，取决于 logpmodel 的具体形式。上述方程 的展开形式通常会有一些项不依赖于模型的参数，我们可以舍去。例如，正如我们 在第5.1.1节中看到的，如果Pmodel(y | X) = N(y； f(X； 0), I)，那么我们就重新得到</p>

<p>了均方误差代价，</p>

<p>J⑹=2 Ex ,y^pdata ||y-f(X；0)||2+const, (6.13)</p>

<p>至少系数2和常数项不依赖于0。舍弃的常数是基于高斯分布的方差，在这种情况 下我们选择不把它参数化。之前，我们看到了对输出分布的最大似然估计和对线性 模型均方误差的最小化之间的等价性，但事实上，这种等价性并不要求f(x;0)用于 预测高斯分布的均值。</p>

<p>使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计 代价函数的负担。明确一个模型p(y | x)则自动地确定了一个代价函数logp(y | x)。</p>

<p>贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具</p>

<p>有足够的预测性，来为学习算法提供一个好的指引。饱和(变得非常平)的函数破</p>

<p>坏了这一目标，因为它们把梯度变得非常小。这在很多情况下都会发生，因为用于</p>

<p>产生隐藏单元或者输出单元的输出的激活函数会饱和。负的对数似然帮助我们在很</p>

<p>多模型中避免这个问题。很多输出单元都会包含一个指数函数，这在它的变量取绝</p>

<p>对值非常大的负值时会造成饱和。负对数似然代价函数中的对数函数消除了某些输</p>

<p>出单元中的指数效果。我们将会在第6.2.2节中讨论代价函数和输出单元的选择间的</p>

<p>相互作用。</p>

<p>用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就是当它</p>

<p>被应用于实践中经常遇到的模型时，它通常没有最小值。对于离散型输出变量，大 多数模型以一种特殊的形式来参数化，即它们不能表示概率零和一，但是可以无限</p>

<p>接近。逻辑回归是其中一个例子。对于实值的输出变量，如果模型可以控制输出分</p>

<p>布的密度(例如，通过学习高斯输出分布的方差参数)，那么它可能对正确的训练集</p>

<p>输出赋予极其高的密度，这将导致交叉熵趋向负无穷。第七章中描述的正则化技术</p>

<p>提供了一些不同的方法来修正学习问题，使得模型不会通过这种方式来获得无限制</p>

<p>的收益。</p>

<p>6.2.1.2 学习条件统计量</p>

<p>有时我们并不是想学习一个完整的概率分布p(y I x; 0)，而仅仅是想学习在给定</p>

<p>x 时 y 的某个条件统计量。</p>

<p>例如，我们可能有一个预测器f(x;0)，我们想用它来预测y的均值。如果我 们使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一大类函 数中的任何一个函数f，这个类仅仅被一些特征所限制，例如连续性和有界，而不 是具有特殊的参数形式。从这个角度来看，我们可以把代价函数看作是一个泛函 (functional)而不仅仅是一个函数。泛函是函数到实数的映射。我们因此可以将学习 看作是选择一个函数而不仅仅是选择一组参数。我们可以设计代价泛函在我们想要 的某些特殊函数处取得最小值。例如，我们可以设计一个代价泛函，使它的最小值处 于一个特殊的函数上，这个函数将x映射到给定x时y的期望值。对函数求解优化 问题需要用到变分法(calculus of variations )这个数学工具，我们将在第19.4.2节 中讨论。理解变分法对于理解本章的内容不是必要的。目前，只需要知道变分法可 以被用来导出下面的两个结果。</p>

<p>我们使用变分法导出的第一个结果是解优化问题</p>

<p>f* = argmin Ex,yidaJy-f (x)H2 (6.14)</p>

<p>f</p>

<p>得到</p>

<p>f (x) = Ey~Pdata(y|x)[y]， (6.15)</p>

<p>要求这个函数处在我们要优化的类里。换句话说，如果我们能够用无穷多的、来源 于真实的数据生成分布的样本进行训练，最小化均方误差代价函数将得到一个函数 它可以用来对每个x的值预测出y的均值。</p>

<p>不同的代价函数给出不同的统计量。第二个使用变分法得到的结果是</p>

<p>f * = arg min EX,y^Pdata ||y - f (X)||1 (6.16)</p>

<p>f</p>

<p>将得到一个函数可以对每个x预测y取值的中位数，只要这个函数在我们要优化的 函数族里。这个代价函数通常被称为平均绝对误差(mean absolute error )。</p>

<p>可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不</p>

<p>佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是为什</p>

<p>么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在</p>

<p>没必要估计整个 p(y | X) 分布时。</p>

<p>6.2.2 输出单元
代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数</p>

<p>据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。</p>

<p>任何可用作输出的神经网络单元，也可以被用作隐藏单元。这里，我们着重讨</p>

<p>论将这些单元用作模型输出时的情况，不过原则上它们也可以在内部使用。我们将</p>

<p>在第6.3节中重温这些单元，并且给出当它们被用作隐藏单元时一些额外的细节。</p>

<p>在本节中，我们假设前馈网络提供了一组定义为h=f(x;0)的隐藏特征。输出 层的作用是随后对这些特征进行一些额外的变换来完成整个网络必须完成的任务。</p>

<p>6.2.2.1 用于高斯输出分布的线性单元</p>

<p>一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这</p>

<p>些单元往往被直接称为线性单元。</p>

<p>给定特征h，线性输出单元层产生一个向量y= WTh+ b。</p>

<p>线性输出层经常被用来产生条件高斯分布的均值：</p>

<p>p(y| x) = N(y; y, I). (6.17)</p>

<p>最大化其对数似然此时等价于最小化均方误差。</p>

<p>最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或更容易地使高斯</p>

<p>分布的协方差矩阵作为输人的函数。然而，对于所有输人，协方差矩阵都必须被限</p>

<p>定成一个正定矩阵。线性输出层很难满足这种限定，所以通常使用其他的输出单元</p>

<p>来对协方差参数化。对协方差建模的方法将在第6.2.2.4节中简要介绍。</p>

<p>因为线性模型不会饱和，所以它们易于采用基于梯度的优化算法，甚至可以使</p>

<p>用其他多种优化算法。</p>

<p>6.2.2.2 用于 Bernoulli 输出分布的 sigmoid 单元</p>

<p>许多任务需要预测二值型变量 y 的值。具有两个类的分类问题可以归结为这种 形式。</p>

<p>此时最大似然的方法是定义 y 在 x 条件下的 Bernoulli 分布。</p>

<p>Bernoulli 分布仅需单个参数来定义。神经网络只需要预测 P(y = 1 | x) 即可。 为了使这个数是有效的概率，它必须处在区间 [0,1] 中。</p>

<p>为满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单元，并</p>

<p>且通过阈值来限制它成为一个有效的概率：</p>

<p>P(y = 1 | x) = max {0, min{1, wTh+ b} } . (6.18)</p>

<p>这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。</p>

<p>当wTh + b处于单位区间外时，模型的输出对其参数的梯度都将为0。梯度为0通</p>

<p>常是有问题的，因为学习算法对于如何改善相应的参数不再具有指导意义。</p>

<p>相反，最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总</p>

<p>能有一个较大的梯度。这种方法是基于使用 sigmoid 输出单元结合最大似然来实现</p>

<p>的。</p>

<p>sigmoid 输出单元定义为</p>

<p>y = a (wTh+b)， (6.19)</p>

<p>这里c是第3.10节中介绍的logistic sigmoid函数。</p>

<p>我们可以认为 sigmoid 输出单元具有两个部分。首先，它使用一个线性层来计 算z = wTh + b。接着，它使用sigmoid激活函数将z转化成概率。</p>

<p>我们暂时忽略对于 x 的依赖性，只讨论如何用 z 的值来定义 y 的概率分布。 sigmoid可以通过构造一个非归一化(和不为1)的概率分布P(y)来得到。我们可 以随后除以一个合适的常数来得到有效的概率分布。如果我们假定非归一化的对数 概率对 y 和 z 是线性的，可以对它取指数来得到非归一化的概率。我们然后对它归 一化，可以发现这服从 Bernoulli 分布，该分布受 z 的 sigmoid 变换控制：</p>

<p>log P(y) = yz,</p>

<p>(6.20)</p>

<p>P(y) = exp(yz),</p>

<p>(6.21)</p>

<p>P (y)= exp(yz)</p>

<p>Ey^=o exp(y/z) ’</p>

<p>(6.22)</p>

<p>P (y) = 4(2y- 1)z).</p>

<p>(6.23)</p>

<p>基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这种二值型变 量分布的变量 z 被称为 分对数( logit)。</p>

<p>这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于 最大似然的代价函数是-logP(y | x)，代价函数中的log抵消了 sigmoid中的exp。 如果没有这个效果， sigmoid 的饱和性会阻止基于梯度的学习做出好的改进。我们使 用最大似然来学习一个由sigmoid参数化的Bernoulli分布，它的损失函数为</p>

<p>J(0) = -logP(y| x) (6.24)</p>

<p>=-log a((2y - 1)z) (6.25)</p>

<p>= Z((1- 2y)z). (6.26)</p>

<p>这个推导使用了第3.10节中的一些性质。通过将损失函数写成softplus函数的 形式，我们可以看到它仅仅在(1 - 2y)z取绝对值非常大的负值时才会饱和。因此饱 和只会出现在模型已经得到正确答案时一当y =1且z取非常大的正值时，或者 y = 0且z取非常小的负值时。当z的符号错误时，softplus函数的变量(1 - 2y)z 可以简化为|z|。当|z|变得很大并且z的符号错误时，softplus函数渐近地趋向于它 的变量|z|。对z求导则渐近地趋向于sign(z)，所以，对于极限情况下极度不正确的 z， softplus 函数完全不会收缩梯度。这个性质很有用，因为它意味着基于梯度的学 习可以很快地改正错误的 z。</p>

<p>当我们使用其他的损失函数，例如均方误差之类的，损失函数会在c(z)饱和时 饱和。 sigmoid 激活函数在 z 取非常小的负值时会饱和到 0，当 z 取非常大的正值时 会饱和到 1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时 模型给出的是正确还是错误的答案。因此，最大似然几乎总是训练sigmoid输出单 元的优选方法。</p>

<p>理论上， sigmoid 的对数总是确定和有限的，因为 sigmoid 的返回值总是被限制</p>

<p>在开区间(0,1) 上，而不是使用整个闭区间［0,1］的有效概率。在软件实现时，为了 避免数值问题，最好将负的对数似然写作z的函数，而不是y = ^(z)的函数。如 果 sigmoid 函数下溢到零，那么之后对 y 取对数会得到负无穷。</p>

<p>6.2.2.3 用于 Multinoulli 输出分布的 softmax 单元</p>

<p>任何时候当我们想要表示一个具有 n 个可能取值的离散型随机变量的分布时 我们都可以使用 softmax 函数。它可以看作是 sigmoid 函数的扩展，其中 sigmoid 函 数用来表示二值型变量的分布。</p>

<p>softmax 函数最常用作分类器的输出，来表示 n 个不同类上的概率分布。比较 少见的是， softmax 函数可以在模型内部使用，例如如果我们想要在某个内部变量的</p>

<p>n 个不同选项中进行选择。</p>

<p>在二值型变量的情况下，我们希望计算一个单独的数</p>

<p>y=P(y=1|X). (6.27)</p>

<p>因为这个数需要处在 0 和 1 之间，并且我们想要让这个数的对数可以很好地用于对 数似然的基于梯度的优化，我们选择去预测另外一个数z = logP(y = 1 | X)。对其 指数化和归一化，我们就得到了一个由sigmoid函数控制的Bernoulli分布。</p>

<p>为了推广到具有n个值的离散型变量的情况，我们现在需要创造一个向量y， 它的每个元素是么=P(y = i | X)。我们不仅要求每个么元素介于0和1之间，还 要使得整个向量的和为1,使得它表示一个有效的概率分布。用于Bernoulli分布的 方法同样可以推广到Multinoulli分布。首先，线性层预测了未归一化的对数概率：</p>

<p>z=WTh+b, (6.28)</p>

<p>其中zi = logP(y = i | X)。softmax函数然后可以对z指数化和归一化来获得需要 的y。最终，softmax函数的形式为</p>

<p>softmax(zVj =eXP^^—. (6.29)</p>

<p>j exp(zj)</p>

<p>和logistic sigmoid—样，当使用最大化对数似然训练softmax来输出目标值y 时，使用指数函数工作地非常好。这种情况下，我们想要最大化 logP(y=i;z) =</p>

<p>logsoftmax(z)i。将softmax定义成指数的形式是很自然的因为对数似然中的log可 以抵消 softmax 中的 exp：</p>

<p>logsoftmax(z)i = zi - log exp(zj). (6.30)</p>

<p>j</p>

<p>式(6.30)中的第一项表示输人zi总是对代价函数有直接的贡献。因为这一项不 会饱和，所以即使zi对式(6.30)的第二项的贡献很小，学习依然可以进行。当最大化 对数似然时，第一项鼓励zi被推高，而第二项则鼓励所有的z被压低。为了对第二项 logZj exp(zj)有一个直观的理解，注意到这一项可以大致近似为maxj zj。这种近似 是基于对任何明显小于maxjzj的zk，exp(zk)都是不重要的。我们能从这种近似中 得到的直觉是，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。如果正确 答案已经具有了 softmax的最大输人，那么-zi项和logEj exp(zj) « maxj zj = zi 项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正 确分类的样本产生。</p>

<p>到目前为止我们只讨论了一个例子。总体来说，未正则化的最大似然会驱动模 型去学习一些参数，而这些参数会驱动softmax函数来预测在训练集中观察到的每 个结果的比率：</p>

<p>softmax(z(x; 0)) c =i,x(,)=. (6.31)</p>

<p>j=1 1x(j) =x</p>

<p>因为最大似然是一致的估计量，所以只要模型族能够表示训练的分布，这就能保证</p>

<p>发生。在实践中，有限的模型能力和不完美的优化将意味着模型只能近似这些比率。</p>

<p>除了对数似然之外的许多目标函数对softmax函数不起作用。具体来说，那些 不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负 值时会造成梯度消失，从而无法学习。特别是，平方误差对于softmax单元来说是一 个很差的损失函数，即使模型做出高度可信的不正确预测，也不能训练模型改变其 输出 (Bridle, 1990)。要理解为什么这些损失函数可能失败，我们需要检查 softmax 函数本身。</p>

<p>像 sigmoid 一样， softmax 激活函数可能会饱和。 sigmoid 函数具有单个输出 当它的输人极端负或者极端正时会饱和。对于 softmax 的情况，它有多个输出值。 当输人值之间的差异变得极端时，这些输出值可能饱和。当softmax饱和时，基于 softmax 的许多代价函数也饱和，除非它们能够转化饱和的激活函数。</p>

<p>为了说明softmax函数对于输人之间差异的响应，观察到当对所有的输人都加</p>

<p>上一个相同常数时 softmax 的输出不变：</p>

<p>softmax(z) = softmax(z + c).</p>

<p>(6.32)</p>

<p>使用这个性质，我们可以导出一个数值方法稳定的softmax函数的变体：</p>

<p>softmax(z) = softmax(z - maxzi). (6.33)</p>

<p>i</p>

<p>变换后的形式允许我们在对softmax函数求值时只有很小的数值误差，即使是当z 包含极正或者极负的数时。观察 softmax 数值稳定的变体，可以看到 softmax 函数 由它的变量偏离 maxi zi 的量来驱动。</p>

<p>当其中一个输人是最大(zi = maxi zi)并且zi远大于其他的输人时，相应的 输出 softmax(z)i 会饱和到 1。当 zi 不是最大值并且最大值非常大时，相应的输出 softmax(z)i 也会饱和到 0。这是 sigmoid 单元饱和方式的一般化，并且如果损失函</p>

<p>数不被设计成对其进行补偿，那么也会造成类似的学习困难。</p>

<p>softmax函数的变量z可以通过两种方式产生。最常见的是简单地使神经网络 较早的层输出的每个元素，就像先前描述的使用线性层z= WTh+ 6。虽然很直 观，但这种方法是对分布的过度参数化。n个输出总和必须为1的约束意味着只有 n - 1个参数是必要的；第n个概率值可以通过1减去前面n - 1个概率来获得。因 此，我们可以强制要求z的一个元素是固定的。例如，我们可以要求zn = 0。事实 上，这正是sigmoid单元所做的。定义P(y = 1 | x) = a(z)等价于用二维的z以及 zi =0来定义P(y = 1 | x) = softmax(z)i。无论是n - 1个变量还是n个变量的方 法，都描述了相同的概率分布，但会产生不同的学习机制。在实践中，无论是过度 参数化的版本还是限制的版本都很少有差别，并且实现过度参数化的版本更为简单。</p>

<p>从神经科学的角度看，有趣的是认为 softmax 是一种在参与其中的单元之间形 成竞争的方式： softmax 输出总是和为 1，所以一个单元的值增加必然对应着其他单 元值的减少。这与被认为存在于皮质中相邻神经元间的侧抑制类似。在极端情况下 (当最大的 ai 和其他的在幅度上差异很大时)，它变成了 赢者通吃( winner-take-all) 的形式(其中一个输出接近 1，其他的接近 0)。</p>

<p>“softmax&rdquo; 的名称可能会让人产生困惑。这个函数更接近于 argmax 函数而不是 max 函数。 “soft&rdquo; 这个术语来源于 softmax 函数是连续可微的。 “argmax&rdquo; 函数的结 果表示为一个one-hot向量(只有一个元素为1，其余元素都为0的向量)，不是连续 和可微的。softmax函数因此提供了 argmax的“软化”版本。max函数相应的软化 版本是softmax(z)Tz。可能最好是把softmax函数称为“softargmax&rdquo;，但当前名称 已经是一个根深蒂固的习惯了。</p>

<p>6.2.2.4 其他的输出类型</p>

<p>之前描述的线性、sigmoid和softmax输出单元是最常见的。神经网络可以推广</p>

<p>到我们希望的几乎任何种类的输出层。最大似然原则给如何为几乎任何种类的输出</p>

<p>层设计一个好的代价函数提供了指导。</p>

<p>一般的，如果我们定义了一个条件分布p(y | X; 0)，最大似然原则建议我们使用 -logp(y | x; 0)作为代价函数。</p>

<p>一般来说，我们可以认为神经网络表示函数f(x; 0)。这个函数的输出不是对y 值的直接预测。相反，f(x;0) = ^提供了 y分布的参数。我们的损失函数就可以表 示成- logp(y; ^(x))。</p>

<p>例如，我们想要学习在给定 x 时， y 的条件高斯分布的方差。简单情况下，方 差V是一个常数，此时有一个解析表达式，这是因为方差的最大似然估计量仅仅是 观测值 y 与它们的期望值的差值的平方平均。一种计算上代价更加高但是不需要写 特殊情况代码的方法是简单地将方差作为分布p(y | X)的其中一个属性，这个分布 由^ = f(x;0)控制。负对数似然-logp(y;^(x))将为代价函数提供一个必要的合 适项来使我们的优化过程可以逐渐地学到方差。在标准差不依赖于输入的简单情况 下，我们可以在网络中创建一个直接复制到中的新参数。这个新参数可以是本 身，或者可以是表示f的参数V，或者可以是表示；;的参数儿取决于我们怎样 对分布参数化。我们可能希望模型对不同的x值预测出y不同的方差。这被称为异 方差(heteroscedastic )模型。在异方差情况下，我们简单地把方差指定为f (x; 0) 其中一个输出值。实现它的典型方法是使用精度而不是方差来表示高斯分布，就像 式(3.22)所描述的。在多维变量的情况下，最常见的是使用一个对角精度矩阵</p>

<p>diag(^). (6.34)</p>

<p>这个公式适用于梯度下降，因为由参数化的高斯分布的对数似然的公式仅涉及啟 的乘法和logft的加法。乘法、加法和对数运算的梯度表现良好。相比之下，如果 我们用方差来参数化输出，我们需要用到除法。除法函数在零附近会变得任意陡峭。 虽然大梯度可以帮助学习，但任意大的梯度通常导致不稳定。如果我们用标准差来 参数化输出，对数似然仍然会涉及除法，并且还将涉及平方。通过平方运算的梯度 可能在零附近消失，这使得学习被平方的参数变得困难。无论我们使用的是标准差， 方差还是精度，我们必须确保高斯分布的协方差矩阵是正定的。因为精度矩阵的特 征值是协方差矩阵特征值的倒数，所以这等价于确保精度矩阵是正定的。如果我们 使用对角矩阵，或者是一个常数乘以单位矩阵1，那么我们需要对模型输出强加的唯 一条件是它的元素都为正。如果我们假设a是用于确定对角精度的模型的原始激活， 那么可以用softplus函数来获得正的精度向量：卢=Z(a)。这种相同的策略对于方 差或标准差同样适用，也适用于常数乘以单位阵的情况。</p>

<p>学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见的。如果 协方差矩阵是满的和有条件的，那么参数化的选择就必须要保证预测的协方差矩阵 是正定的。这可以通过写成S(x) = B(x)BT(x)来实现，这里B是一个无约束的方 阵。如果矩阵是满秩的，那么一个实际问题是计算似然的代价是很高的，计算一个 d x d的矩阵的行列式或者S(x)的逆(或者等价地并且更常用地，对它特征值分解 或者 B(x) 的特征值分解)需要 O(d2) 的计算量。</p>

<p>我们经常想要执行多峰回归(multimodal regression)，即预测条件分布p(y | x) 的实值，该条件分布对于相同的 x 值在 y 空间中有多个不同的峰值。在这种情况下 高斯混合是输出的自然表示 (Jacobs et al., 1991; Bishop, 1994)。将高斯混合作为其 输出的神经网络通常被称为混合密度网络(mixture density network )。具有n个分 量的高斯混合输出由下面的条件分布定义：</p>

<p>n</p>

<p>p(y | x) = Lp(c = i | x)N(y; M(i)(x), S(i)(x)). (6.35)</p>

<p>i=1</p>

<p>神经网络必须有三个输出：定义p(c = i | x)的向量，对所有的i给出M(i)(x)的矩 阵，以及对所有的i给出S(i)(x)的张量。这些输出必须满足不同的约束：</p>

<ol>
<li><p>混合组件p(c = i | x):它们由潜变量3 c关联着，在n个不同组件上形 成 Multinoulli 分布。这个分布通常可以由 n 维向量的 softmax 来获得，以确 保这些输出是正的并且和为 1。</p></li>

<li><p>均值M(i)(x):它们指明了与第i个高斯组件相关联的中心或者均值，并且是无</p></li>
</ol>

<p>约束的(通常对于这些输出单元完全没有非线性)。如果y是个d维向量，那</p>

<p>么网络必须输出一个由n个这种d维向量组成的n X d的矩阵。用最大似然来</p>

<p>学习这些均值要比学习只有一个输出模式的分布的均值稍稍复杂一些。我们只</p>

<p>想更新那个真正产生观测数据的组件的均值。在实践中，我们并不知道是哪个</p>

<p>组件产生了观测数据。负对数似然表达式将每个样本对每个组件的贡献进行赋</p>

<p>权，权重的大小由相应的组件产生这个样本的概率来决定。</p>

<p>3.协方差S(i)(x)：它们指明了每个组件i的协方差矩阵。和学习单个高斯组件时 一样，我们通常使用对角矩阵来避免计算行列式。和学习混合均值时一样，最 大似然是很复杂的，它需要将每个点的部分责任分配给每个混合组件。如果给</p>

<p>定了混合模型的正确的负对数似然，梯度下降将自动地遵循正确的过程。</p>

<p>有报告说基于梯度的优化方法对于混合条件高斯(作为神经网络的输出)可能是不 可靠的，部分是因为涉及到除法(除以方差)可能是数值不稳定的(当某个方差对于 特定的实例变得非常小时，会导致非常大的梯度)。一种解决方法是梯度截断(clip gradient)(见第10.11.1节)，另外一种是启发式缩放梯度(Murray and Larochelle,</p>

<p>2014)。</p>

<p>高斯混合输出在语音生成模型(Schuster, 1999)和物理运动(Graves, 2013)中特 别有效。混合密度策略为网络提供了一种方法来表示多种输出模式，并且控制输出 的方差，这对于在这些实数域中获得高质量的结果是至关重要的。混合密度网络的 一个实例如图6.4所示。</p>

<p>一般的，我们可能希望继续对包含更多变量的、更大的向量 y 来建模，并在 这些输出变量上施加更多更丰富的结构。例如，我们可能希望神经网络输出字符序 列形成一个句子。在这些情况下，我们可以继续使用最大似然原理应用到我们的模 型p(y; w(x))上，但我们用来描述y的模型会变得非常复杂，超出了本章的范畴。 第十章描述了如何使用循环神经网络来定义这种序列上的模型，第三部分描述了对 任意概率分布进行建模的高级技术。</p>

<p>图6.4:从具有混合密度输出层的神经网络中抽取的样本。输人x从均匀分布中采样，输出y从</p>

<p>pmodel(y |x) 中采样。神经网络能够学习从输入到输出分布的参数的非线性映射。这些参数包括控 制三个组件中的哪一个将产生输出的概率，以及每个组件各自的参数。每个混合组件都是高斯分</p>

<p>布，具有预测的均值和方差。输出分布的这些方面都能够相对输人x变化，并且以非线性的方式</p>

<p>改变。</p>

<p>1</p>

<p>译者注：这里原文是 “If we use a diagonal matrix, or a scalar times the diagonal matrix&hellip;&rdquo; 即 ‘‘如果我们使 用对角矩阵，或者是一个标量乘以对角矩阵&hellip;&ldquo;，但一个标量乘以对角矩阵和对角矩阵没区别，结合上下文可以看出，</p>

<p>这里原作者误把“identity&rdquo;写成了 “diagonal matrix”，因此这里采用“常数乘以单位矩阵”的译法。</p>

<p>2</p>

<p>量。</p>

<p>3</p>

<p>我们之所以认为c是潜在的，是因为我们不能直接在数据中观测到它：给定输人x和目标y，不可能确切地知道 是哪个高斯组件产生y，但我们可以想象y是通过选择其中一个来产生的，并且将那个未被观测到的选择作为随机变</p>

<p>6.3 隐藏单元
到目前为止，我们集中讨论了神经网络的设计选择，这对于使用基于梯度的优化</p>

<p>方法来训练的大多数参数化机器学习模型都是通用的。现在我们转向一个前馈神经</p>

<p>网络独有的问题：该如何选择隐藏单元的类型，这些隐藏单元用在模型的隐藏层中。</p>

<p>隐藏单元的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理</p>

<p>论原则。</p>

<p>整流线性单元是隐藏单元极好的默认选择。许多其他类型的隐藏单元也是可用</p>

<p>的。决定何时使用哪种类型的隐藏单元是困难的事(尽管整流线性单元通常是一个</p>

<p>可接受的选择)。我们这里描述对于每种隐藏单元的一些基本直觉。这些直觉可以用</p>

<p>来建议我们何时来尝试一些单元。通常不可能预先预测出哪种隐藏单元工作得最好。</p>

<p>设计过程充满了试验和错误，先直觉认为某种隐藏单元可能表现良好，然后用它组</p>

<p>成神经网络进行训练，最后用验证集来评估它的性能。</p>

<p>这里列出的一些隐藏单元可能并不是在所有的输人点上都是可微的。例如，整 流线性单元 g(z) = max{0, z} 在 z = 0 处不可微。这似乎使得 g 对于基于梯度的学</p>

<p>习算法无效。在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因</p>

<p>是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它 的值，如图4.3所示。这些想法会在第八章中进一步描述。因为我们不再期望训练能 够实际到达梯度为 0 的点，所以代价函数的最小值对应于梯度未定义的点是可以接 受的。不可微的隐藏单元通常只在少数点上不可微。一般来说，函数 g(z) 具有左导 数和右导数，左导数定义为紧邻在 z 左边的函数的斜率，右导数定义为紧邻在 z 右 边的函数的斜率。只有当函数在 z 处的左导数和右导数都有定义并且相等时，函数 在 z 点处才是可微的。神经网络中用到的函数通常对左导数和右导数都有定义。在 g(z) = max{0,z}的情况下，在z = 0处的左导数是0，右导数是1。神经网络训练 的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一 个错误。这可以通过观察到在数字计算机上基于梯度的优化总是会受到数值误差的 影响来启发式地给出理由。当一个函数被要求计算g(0)时，底层值真正为0是不太 可能的。相对的，它可能是被舍人为0的一个小量e。在某些情况下，理论上有更好 的理由，但这些通常对神经网络训练并不适用。重要的是，在实践中，我们可以放 心地忽略下面描述的隐藏单元激活函数的不可微性。</p>

<p>除非另有说明，大多数的隐藏单元都可以描述为接受输人向量x，计算仿射变 换2= WTx+b，然后使用一个逐元素的非线性函数g(z)。大多数隐藏单元的区别 仅仅在于激活函数 g(z) 的形式。</p>

<p>6.3.1 整流线性单元及其扩展
整流线性单元使用激活函数g(z) = max{0,z}。</p>

<p>整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性 单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流 线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流 操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数 处处为 1。这意味着相比于引人二阶效应的激活函数来说，它的梯度方向对于学习来 说更加有用。</p>

<p>整流线性单元通常作用于仿射变换之上：</p>

<p>h=g(WTx+b). (6.36)</p>

<p>当初始化仿射变换的参数时，可以将 b 的所有元素设置成一个小的正值，例如 0.1。 这使得整流线性单元很可能初始时就对训练集中的大多数输人呈现激活状态，并且</p>

<p>允许导数通过。</p>

<p>有很多整流线性单元的扩展存在。大多数这些扩展的表现比得上整流线性单元</p>

<p>并且偶尔表现得更好。</p>

<p>整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活</p>

<p>为零的样本。整流线性单元的各种扩展保证了它们能在各个位置都接收到梯度。</p>

<p>整流线性单元的三个扩展基于当zi &lt; 0时使用一个非零的斜率ai: hi = g(z, a)i = max(0, zi) + ai min(0, zi)。绝对值整流(absolute value rectification )固 定ai = -1来得到g(z) = |z|。它用于图像中的对象识别(Jarrett et al., 2009a)，其中 寻找在输人照明极性反转下不变的特征是有意义的。整流线性单元的其他扩展比这 应用地更广泛。渗漏整流线性单元(Leaky ReLU) (Maas et al., 2013)将aj固定成 一个类似0.01的小值，参数化整流线性单元(parametric ReLU)或者PReLU将 ai 作为学习的参数 (He et al., 2015)。</p>

<p>maxout 单元( maxout unit)(Goodfellow et al., 2013a) 进一步扩展了整流线 性单元。 maxout 单元将 z 划分为每组具有 k 个值的组，而不是使用作用于每个元 素的函数g(z)。每个maxout单元则输出每组中的最大元素：</p>

<p>g(z)i = maxzj (6.37)</p>

<p>jeG⑷</p>

<p>这里G⑴是组i的输人索引集｛(i - 1)k + 1,&hellip;, ik｝。这提供了一种方法来学习对输 人 x 空间中多个方向响应的分段线性函数。</p>

<p>maxout单元可以学习具有多达k段的分段线性的凸函数。maxout单元因此可 以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的 k， maxout 单 元可以以任意的精确度来近似任何凸函数。特别地，具有两块的 maxout 层可以学 习实现和传统层相同的输人x的函数，这些传统层可以使用整流线性激活函数、绝 对值整流、渗漏整流线性单元 或参数化整流线性单元，或者可以学习实现与这些都 不同的函数。maxout层的参数化当然也将与这些层不同，所以即使是maxout学习 去实现和其他种类的层相同的x的函数这种情况下，学习的机理也是不一样的。</p>

<p>每个 maxout 单元现在由 k 个权重向量来参数化，而不仅仅是一个，所以 maxout 单元通常比整流线性单元需要更多的正则化。如果训练集很大并且每个单元的块数 保持很低的话，它们可以在没有正则化的情况下工作得不错(Cai et al., 2013)。</p>

<p>maxout 单元还有一些其他的优点。在某些情况下，要求更少的参数可以获得一 些统计和计算上的优点。具体来说，如果由 n 个不同的线性过滤器描述的特征可以</p>

<p>在不损失信息的情况下，用每一组k个特征的最大值来概括的话，那么下一层可以</p>

<p>获得 k 倍更少的权重数。</p>

<p>因为每个单元由多个过滤器驱动，maxout单元具有一些冗余来帮助它们抵抗一 种被称为灾难遗忘(catastrophic forgetting)的现象，这个现象是说神经网络忘记 了如何执行它们过去训练的任务(Goodfellow ef &lt; 2014a)。</p>

<p>整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更 接近线性，那么模型更容易优化。使用线性行为更容易优化的一般性原则同样也适 用于除深度线性网络以外的情景。循环网络可以从序列中学习并产生状态和输出的 序列。当训练它们时，需要通过一些时间步来传播信息，当其中包含一些线性计算 (具有大小接近 1 的某些方向导数)时，这会更容易。作为性能最好的循环网络结构 之一， LSTM 通过求和在时间上传播信息，这是一种特别直观的线性激活。它将在 第10.10节中进一步讨论。</p>

<p>6.3.2 logistic sigmoid与双曲正切函数
在引人整流线性单元之前，大多数神经网络使用 logistic sigmoid 激活函数</p>

<p>g(z) = ct(z) (6.38)</p>

<p>或者是双曲正切激活函数</p>

<p>g(z) = tanh(z). (6.39)</p>

<p>这些激活函数紧密相关，因为tanh(z) = 2a(2z) - 1。</p>

<p>我们已经看过sigmoid单元作为输出单元用来预测二值型变量取值为1的概率。 与分段线性单元不同，sigmoid单元在其大部分定义域内都饱和一当z取绝对值 很大的正值时，它们饱和到一个高值，当 z 取绝对值很大的负值时，它们饱和到一 个低值，并且仅仅当 z 接近 0 时它们才对输人强烈敏感。 sigmoid 单元的广泛饱和 性会使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将它们用作前 馈网络中的隐藏单元。当使用一个合适的代价函数来抵消sigmoid的饱和性时，它 们作为输出单元可以与基于梯度的学习相兼容。</p>

<p>当必须要使用 sigmoid 激活函数时，双曲正切激活函数通常要比 logistic sig-moid函数表现更好。在tanh(0) = 0而ct(0) = |的意义上，它更像是单位函数。因 为tanh在0附近与单位函数类似，训练深层神经网络y = wTtanh( J7Ttanh( VTx))</p>

<p>类似于训练一个线性模型y = wT^TrTx，只要网络的激活能够被保持地很小。这</p>

<p>使得训练 tanh 网络更加容易。</p>

<p>sigmoid 激活函数在除了前馈网络以外的情景中更为常见。循环网络、许多概率</p>

<p>模型以及一些自编码器有一些额外的要求使得它们不能使用分段线性激活函数，并</p>

<p>且使得 sigmoid 单元更具有吸引力，尽管它存在饱和性的问题。</p>

<p>6.3.3 其他隐藏单元
也存在许多其他种类的隐藏单元，但它们并不常用。</p>

<p>一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数与流行</p>

<p>的激活函数表现得一样好。为了提供一个具体的例子，作者在 MNIST 数据集上使 用 h=cos(Wx+b) 测试了一个前馈网络，并获得了小于 1%的误差率，这可以与</p>

<p>更为传统的激活函数获得的结果相媲美。在新技术的研究和开发期间，通常会测试</p>

<p>许多不同的激活函数，并且会发现许多标准方法的变体表现非常好。这意味着，通</p>

<p>常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。新的隐藏</p>

<p>单元类型如果与已有的隐藏单元表现大致相当的话，那么它们是非常常见的，不会</p>

<p>引起别人的兴趣。</p>

<p>列出文献中出现的所有隐藏单元类型是不切实际的。我们只对一些特别有用和</p>

<p>独特的类型进行强调。</p>

<p>其中一种是完全没有激活函数g(z)。也可以认为这是使用单位函数作为激活函 数的情况。我们已经看过线性单元可以用作神经网络的输出。它也可以用作隐藏单 元。如果神经网络的每一层都仅由线性变换组成，那么网络作为一个整体也将是线 性的。然而，神经网络的一些层是纯线性也是可以接受的。考虑具有n个输人和p 个输出的神经网络层h = g(WTx+b)。我们可以用两层来代替它，一层使用权重矩 阵仄另一层使用权重矩阵V。如果第一层没有激活函数，那么我们对基于W的 原始层的权重矩阵进行因式分解。分解方法是计算h = g(VT UTx + b)。如果U产 生了 q个输出，那么U和V —起仅包含(n+p)q个参数，而W包含np个参数。 如果 q 很小，这可以在很大程度上节省参数。这是以将线性变换约束为低秩的代价 来实现的，但这些低秩关系往往是足够的。线性隐藏单元因此提供了一种减少网络 中参数数量的有效方法。</p>

<p>softmax 单元是另外一种经常用作输出的单元(如第6.2.2.3节中所描述的)，但</p>

<p>有时也可以用作隐藏单元。softmax单元很自然地表示具有k个可能值的离散型随 机变量的概率分布，所以它们可以用作一种开关。这些类型的隐藏单元通常仅用于 明确地学习操作内存的高级结构中，将在第10.12节中描述。</p>

<p>其他一些常见的隐藏单元类型包括：</p>

<p>•径向基函数(radial basis function, RBF ):〜=exp (―去||W:ii — x||2)。这个 函数在x接近模板W:,i时更加活跃。因为它对大部分$都饱和到0，因此很</p>

<p>难优化。</p>

<p>• softplus函数：g(a) = Z(a) = log(1 + ea)。这是整流线性单元的平滑版本， 由 Dugas et al. (2001) 引人用于函数近似，由 Nair and Hinton (2010a) 引人用 于无向概率模型的条件分布。 Glorot et al. (2011a) 比较了 softplus 和整流线性 单元，发现后者的结果更好。通常不鼓励使用softplus函数。softplus表明隐藏 单元类型的性能可能是非常反直觉的——因为它处处可导或者因为它不完全饱 和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</p>

<p>•硬双曲正切函数(hardtanh):它的形状和tanh以及整流线性单元类似，但是 不同于后者，它是有界的，g(a) = max(-1,min(1,a))。它由 Collobert (2004) 引人。</p>

<p>隐藏单元的设计仍然是一个活跃的研究领域，许多有用的隐藏单元类型仍有待</p>

<p>发现。</p>

<p>6.4 架构设计
神经网络设计的另一个关键点是确定它的架构。架构(architecture) —词是指 网络的整体结构:它应该具有多少单元，以及这些单元应该如何连接。</p>

<p>大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置 成链式结构，其中每一层都是前一层的函数。在这种结构中，第一层由下式给出:</p>

<p>h⑴=g(”(W(1)T x + b⑴)； (6.40)</p>

<p>第二层由</p>

<p>h(2) =g⑺(W(2)Th(1) + b(2));</p>

<p>(6.41)</p>

<p>给出，以此类推。</p>

<p>在这些链式架构中，主要的架构考虑是选择网络的深度和每一层的宽度。我们</p>

<p>将会看到，即使只有一个隐藏层的网络也足够适应训练集。更深层的网络通常能够</p>

<p>对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但是通常</p>

<p>也更难以优化。对于一个具体的任务，理想的网络架构必须通过实验，观测在验证</p>

<p>集上的误差来找到。</p>

<p>6.4.1 万能近似性质和深度
线性模型，通过矩阵乘法将特征映射到输出，顾名思义，仅能表示线性函数。它</p>

<p>具有易于训练的优点，因为当使用线性模型时，许多损失函数会导出凸优化问题。可</p>

<p>惜的是，我们经常希望我们的系统学习非线性函数。</p>

<p>乍一看，我们可能认为学习非线性函数需要为我们想要学习的那种非线性专 门设计一类模型族。幸运的是，具有隐藏层的前馈网络提供了一种万能近似框架。 具体来说，万能近似定理(universal approximation theorem) (Hornik et al., 1989; Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何 一种“挤压”性质的激活函数(例如logistic sigmoid激活函数)的隐藏层，只要给予 网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另 一个有限维空间的 Borel 可测函数。前馈网络的导数也可以任意好地来近似函数的 导数 (Hornik et al., 1990)。 Borel 可测的概念超出了本书的范畴；对于我们想要实 现的目标，只需要知道定义在 Rn 的有界闭集上的任意连续函数是 Borel 可测的 因此可以用神经网络来近似。神经网络也可以近似从任何有限维离散空间映射到另 一个的任意函数。虽然原始定理最初以具有特殊激活函数的单元的形式来描述，这 个激活函数当变量取绝对值非常大的正值和负值时都会饱和，万能近似定理也已经 被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的整流线性单 元 (Leshno et al., 1993)。</p>

<p>万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP 一 定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。即使 MLP 能够表示该函数，学习也可能因两个不同的原因而失败。首先，用于训练的优化算 法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错 误的函数。回忆第5.2.1节中的‘‘没有免费的午餐&rdquo; 定理，说明了没有普遍优越的机 器学习算法。前馈网络提供了表示函数的万能系统，在这种意义上，给定一个函数，</p>

<p>存在一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上的特殊</p>

<p>样本，又能够选择一个函数来扩展到训练集上没有的点。</p>

<p>万能近似定理说明了，存在一个足够大的网络能够达到我们所希望的任意精度 但是定理并没有说这个网络有多大。 Barron (1993) 提供了单层网络近似一大类函数 所需大小的一些界。不幸的是，在最坏情况下，可能需要指数数量的隐藏单元(可 能一个隐藏单元对应着一个需要区分的输人配置)。这在二进制值的情况下很容易看</p>

<p>到：向量r e {0,l}n上的可能的二值型函数的数量是22n，并且选择一个这样的函 数需要 2n 位，这通常需要 O(2n) 的自由度。</p>

<p>总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现</p>

<p>并且可能无法正确地学习和泛化。在很多情况下，使用更深的模型能够减少表示期</p>

<p>望函数所需的单元的数量，并且可以减少泛化误差。</p>

<p>存在一些函数族能够在网络的深度大于某个值d时被高效地近似，而当深度被 限制到小于或等于 d 时需要一个远远大于之前的模型。在很多情况下，浅层模型所 需的隐藏单元的数量是 n 的指数级。这个结果最初被证明是在那些不与连续可微的 神经网络类似的机器学习模型中出现，但现在已经扩展到了这些模型。第一个结果 是关于逻辑门电路的(Hastad, 1986)。后来的工作将这些结果扩展到了具有非负权 重的线性阈值单元(Hastad and Goldmann, 1991; Hajnal et al., 1993)，然后扩展到 了具有连续值激活的网络 (Maass, 1992; Maass et al., 1994)。许多现代神经网络使 用整流线性单元。 Leshno et al. (1993) 证明带有一大类非多项式激活函数族的浅层 网络，包括整流线性单元，具有万能的近似性质，但是这些结果并没有强调深度或 效率的问题——它们仅指出足够宽的整流网络能够表示任意函数。 Montufar etal. (2014) 指出一些用深度整流网络表示的函数可能需要浅层网络(一个隐藏层)指数 级的隐藏单元才能表示。更确切的说，他们说明分段线性网络(可以通过整流非线 性或 maxout 单元获得)可以表示区域的数量是网络深度的指数级的函数。图6.5解 释了带有绝对值整流的网络是如何创建函数的镜像图像的，这些函数在某些隐藏单 元的顶部计算，作用于隐藏单元的输人。每个隐藏单元指定在哪里折叠输人空间，来 创造镜像响应(在绝对值非线性的两侧)。通过组合这些折叠操作，我们获得指数级 的分段线性区域，他们可以概括所有种类的规则模式(例如，重复)。</p>

<p>Montufar et al. (2014)的主要定理指出，具有d个输人、深度为1、每个隐藏</p>

<p>图 6.5: 关于更深的整流网络具有指数优势的一个直观的几何解释，来自 Montufar et al. （2014）。 （左）绝对值整流单元对其输入中的每对镜像点有相同的输出。镜像的对称轴由单元的权重和偏置 定义的超平面给出。在该单元顶部计算的函数（绿色决策面）将是横跨该对称轴的更简单模式的 一个镜像。（中） 该函数可以通过折叠对称轴周围的空间来得到。（右） 另一个重复模式可以在第一 个的顶部折叠（由另一个下游单元）以获得另外的对称性（现在重复四次，使用了两个隐藏层） 经 Montufar et al. （2014） 许可改编此图。</p>

<p>层具有 n 个单元的深度整流网络可以描述的线性区域的数量是 d（l-1）</p>

<p>O</p>

<p>(6.42)</p>

<p>意味着，这是深度 l 的指数级。在每个单元具有 k 个过滤器的 maxout 网络中，线 性区域的数量是 （ ）</p>

<p>O （k （l-1）+d ） . （6.43）</p>

<p>当然，我们不能保证在机器学习（特别是AI）的应用中我们想要学得的函数类</p>

<p>型享有这样的属性。</p>

<p>我们还可能出于统计原因来选择深度模型。任何时候，当我们选择一个特定的机 器学习算法时，我们隐含地陈述了一些先验，这些先验是关于算法应该学得什么样的 函数的。选择深度模型默许了一个非常普遍的信念，那就是我们想要学得的函数应该 涉及几个更加简单的函数的组合。这可以从表示学习的观点来解释，我们相信学习的 问题包含发现一组潜在的变差因素，它们可以根据其他更简单的潜在的变差因素来 描述。或者，我们可以将深度结构的使用解释为另一种信念，那就是我们想要学得的 函数是包含多个步骤的计算机程序，其中每个步骤使用前一步骤的输出。这些中间 输出不一定是变差因素，而是可以类似于网络用来组织其内部处理的计数器或指针 根据经验，更深的模型似乎确实在广泛的任务中泛化得更好（Bengio et al., 2007b;</p>

<p>Erhan et al., 2009; Bengio, 2009; Mesnil et al., 2011; Ciresan et al., 2012; Krizhevsky et al., 2012a; Sermanet et al., 2013; Farabet et al., 2013; Couprie et al., 2013; Kahou et al., 2013; Goodfellow et al., 2014d; Szegedy et al., 2014a）。图 6.6 和图 6.7展示了 一些实验结果的例子。这表明使用深层架构确实在模型学习的函数空间上表示了一 个有用的先验。</p>

<p>{^USOJSd)^016.!30016:^0^</p>

<p>96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 92.5 92.0</p>

<p>3 4 5 6 7 8 9 10 11</p>

<p>图 6.6: 深度的影响。实验结果表明，当从地址照片转录多位数字时，更深层的网络能够更好地泛</p>

<p>化。数据来自Goodfellow et al. (2014d)。测试集上的准确率随着深度的增加而不断增加。图6.7给</p>

<p>出了一个对照实验，它说明了对模型尺寸其他方面的增加并不能产生相同的效果。</p>

<p>6.4.2 其他架构上的考虑
目前为止，我们都将神经网络描述成层的简单链式结构，主要的考虑因素是网</p>

<p>络的深度和每层的宽度。在实践中，神经网络显示出相当的多样性。</p>

<p>许多神经网络架构已经被开发用于特定的任务。用于计算机视觉的卷积神经网</p>

<p>络的特殊架构将在第九章中介绍。前馈网络也可以推广到用于序列处理的循环神经</p>

<p>网络，但有它们自己的架构考虑，将在第十章中介绍。</p>

<p>一般的，层不需要连接在链中，尽管这是最常见的做法。许多架构构建了一个 主链，但随后又添加了额外的架构特性，例如从层 i 到层 i+2 或者更高层的跳跃连 接。这些跳跃连接使得梯度更容易从输出层流向更接近输入的层。</p>

<p>架构设计考虑的另外一个关键点是如何将层与层之间连接起来。默认的神经网</p>

<p>络层采用矩阵W描述的线性变换，每个输人单元连接到每个输出单元。在之后章节</p>

<p>-USOJSd)^016.!30016:^0^</p>

<p>97</p>

<p>96 95 94 93 92</p>

<p>91</p>

<p>0.2 0.4 0.6 0.8</p>

<p>Number of parameters</p>

<p>1 . 0</p>

<p>0.0</p>

<p>Xl08</p>

<p>图6.7:参数数量的影响。更深的模型往往表现更好。这不仅仅是因为模型更大。Goodfellow et al.</p>

<p>(2014d) 的这项实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提升测试集 性能方面几乎没有效果，如此图所示。图例标明了用于画出每条曲线的网络深度，以及曲线表示 的是卷积层还是全连接层的大小变化。我们可以观察到，在这种情况下，浅层模型在参数数量达 到 2000 万时就过拟合，而深层模型在参数数量超过 6000 万时仍然表现良好。这表明，使用深层 模型表达出了对模型可以学习的函数空间的有用偏好。具体来说，它表达了一种信念，即该函数 应该由许多更简单的函数复合在一起而得到。这可能导致学习由更简单的表示所组成的表示(例 如，由边所定义的角)或者学习具有顺序依赖步骤的程序(例如，首先定位一组对象，然后分割它 们，之后识别它们)。</p>

<p>中的许多专用网络具有较少的连接，使得输人层中的每个单元仅连接到输出层单元</p>

<p>的一个小子集。这些用于减少连接数量的策略减少了参数的数量以及用于评估网络</p>

<p>的计算量，但通常高度依赖于问题。例如，第九章描述的卷积神经网络使用对于计</p>

<p>算机视觉问题非常有效的稀疏连接的专用模式。在这一章中，很难对通用神经网络</p>

<p>的架构给出更多具体的建议。我们在随后的章节中介绍一些特殊的架构策略，可以</p>

<p>在不同的领域工作良好。</p>

<p>6.5 反向传播和其他的微分算法
当我们使用前馈神经网络接收输人并产生输出&amp;时，信息通过网络向前流 动。输人提供初始信息，然后传播到每一层的隐藏单元，最终产生输出b这称 之为前向传播(forward propagation)。在训练过程中，前向传播可以持续向前直 到它产生一个标量代价函数J(0)。反向传播(back propagation)算法(Rumelhart</p>

<p>et al., 1986c)，经常简称为backprop，允许来自代价函数的信息通过网络向后流动，</p>

<p>以便计算梯度。</p>

<p>计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在计算上</p>

<p>的代价可能很大。反向传播算法使用简单和廉价的程序来实现这个目标。</p>

<p>反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上</p>

<p>反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度</p>

<p>来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但是原则上它</p>

<p>可以计算任何函数的导数(对于一些函数，正确的响应是报告函数的导数是未定义</p>

<p>的)。特别地，我们会描述如何计算一个任意函数f的梯度其中z是一 组变量，我们需要它们的导数，而 y 是函数的另外一组输入变量，但我们并不需要 它们的导数。在学习算法中，我们最常需要的梯度是代价函数关于参数的梯度，即 VeJ(0)。许多机器学习任务需要计算其他导数，来作为学习过程的一部分，或者用 来分析学得的模型。反向传播算法也适用于这些任务，不局限于计算代价函数关于 参数的梯度。通过在网络中传播信息来计算导数的想法非常普遍，它还可以用于计 算诸如多输出函数 f 的 Jacobian 的值。我们这里描述的是最常用的情况，其中 f 只有单个输出。</p>

<p>6.5.1 计算图
目前为止，我们已经用相对非正式的图形语言讨论了神经网络。为了更精确地</p>

<p>描述反向传播算法，使用更精确的计算图(computational graph )语言是很有帮助 的。</p>

<p>将计算形式化为图形的方法有很多。</p>

<p>这里，我们使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩</p>

<p>阵、张量、或者甚至是另一类型的变量。</p>

<p>为了形式化我们的图形，我们还需引人操作(operation)这一概念。操作是指 一个或多个变量的简单函数。我们的图形语言伴随着一组被允许的操作。我们可以 通过将多个操作复合在一起来描述更为复杂的函数。</p>

<p>不失一般性，我们定义一个操作仅返回单个输出变量。这并没有失去一般性，是</p>

<p>因为输出变量可以有多个条目，例如向量。反向传播的软件实现通常支持具有多个</p>

<p>输出的操作，但是我们在描述中避免这种情况，因为它引人了对概念理解不重要的</p>

<p>许多额外细节。</p>

<p>如果变量y是变量x通过一个操作计算得到的，那么我们画一条从x到y的有</p>

<p>向边。我们有时用操作的名称来注释输出的节点，当上下文很明确时，有时也会省</p>

<p>略这个标注。</p>

<p>计算图的实例可以参考图6.8。</p>

<p>图6.8: —些计算图的示例。faj使用x操作计算z = xy的图。用于逻辑回归预测y = a(xTw+b)的图。一些中间表达式在代数表达式中没有名称，但在图形中却需要。我们简单地将 第i个这样的变量命名为u()。C表达式丑=max{0, XW + b}的计算图，在给定包含小批量 输人数据的设计矩阵X时，它计算整流线性单元激活的设计矩阵丑。fdj示例a-c对每个变量最 多只实施一个操作，但是对变量实施多个操作也是可能的。这里我们展示一个计算图，它对线性 回归模型的权重w实施多个操作。这个权重不仅用于预测y，也用于权重衰减罚项A^^w?。</p>

<p>6.5.2 微积分中的链式法则
微积分中的链式法则(为了不与概率中的链式法则相混淆)用于计算复合函数</p>

<p>的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。</p>

<p>设x是实数，f和g是从实数映射到实数的函数。假设y = g(x)并且z = f (g(x)) = f (y)。那么链式法则是说</p>

<p>dz dz dy dx dy dx</p>

<p>(6.44)</p>

<p>我们可以将这种标量情况进行扩展。假设z e Rm, y e Rn，g是从Rm到Rn的 映射，f是从Rn到R的映射。如果y = g(z)并且z = f㈦，那么</p>

<p>dz dz dyj</p>

<p>(6.45)</p>

<p>dxi dyj dxi</p>

<p>使用向量记法，可以等价地写成</p>

<p>^ = (dZ)(6.46)</p>

<p>这里寇是g的n x m的Jacobian矩阵。</p>

<p>从这里我们看到，变量z的梯度可以通过Jacobian矩阵劈和梯度Vaz相乘来 得到。反向传播算法由图中每一个这样的 Jacobian 梯度的乘积操作所组成。</p>

<p>通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。从概念 上讲，这与使用向量的反向传播完全相同。唯一的区别是如何将数字排列成网格以 形成张量。我们可以想象，在我们运行反向传播之前，将每个张量变平为一个向量 计算一个向量值梯度，然后将该梯度重新构造成一个张量。从这种重新排列的观点 上看，反向传播仍然只是将 Jacobian 乘以梯度。</p>

<p>为了表示值z关于张量X的梯度，我们记为Vxz，就像X是向量一样。X的 索引现在有多个坐标——例如，一个 3 维的张量由三个坐标索引。我们可以通过</p>

<p>使用单个变量 i 来表示完整的索引元组，从而完全抽象出来。对所有可能的元组 i (▽xz)i给出 ＞。这与向量中索引的方式完全一致，(Vzz)i给出盖。使用这种记 法，我们可以写出适用于张量的链式法则。如果Y = g(X)并且z = f (Y)，那么</p>

<p>dz</p>

<p>(6.47)</p>

<p>▽xz = E(VxY-)W - -</p>

<p>6.5.3 递归地使用链式法则来实现反向传播
使用链式规则，我们可以直接写出某个标量关于计算图中任何产生该标量的节</p>

<p>点的梯度的代数表达式。然而，实际在计算机中计算该表达式时会引人一些额外的</p>

<p>考虑。</p>

<p>具体来说，许多子表达式可能在梯度的整个表达式中重复若干次。任何计算梯</p>

<p>度的程序都需要选择是存储这些子表达式还是重新计算它们几次。图6.9给出了一个</p>

<p>例子来说明这些重复的子表达式是如何出现的。在某些情况下，计算两次相同的子</p>

<p>表达式纯粹是浪费。在复杂图中，可能存在指数多的这种计算上的浪费，使得简单</p>

<p>的链式法则不可实现。在其他情况下，计算两次相同的子表达式可能是以较高的运</p>

<p>行时间为代价来减少内存开销的有效手段。</p>

<p>我们首先给出一个版本的反向传播算法，它指明了梯度的直接计算方式(算 法6.2以及相关的正向计算的算法6.1 )，按照它实际完成的顺序并且递归地使用链 式法则。我们可以直接执行这些计算或者将算法的描述视为用于计算反向传播的计</p>

<p>算图的符号表示。然而，这些公式并没有明确地操作和构造用于计算梯度的符号图。</p>

<p>这些公式将在后面的第6.5.6节和算法6.5中给出，其中我们还推广到了包含任意张</p>

<p>量的节点。</p>

<p>首先考虑描述如何计算单个标量 u(n) (例如训练样本上的损失函数)的计算图。 我们想要计算这个标量对 ni 个输人节点 u(1) 到 u(ni) 的梯度。换句话说，我们希望</p>

<p>对所有的i e {1,2,&hellip; ,ni}计算dUn)。在使用反向传播计算梯度来实现参数的梯度 下降时，u(n)将对应单个或者小批量实例的代价函数，而u⑴到u(ni)则对应于模型 的参数。</p>

<p>我们假设图的节点已经以一种特殊的方式被排序，使得我们可以一个接一个地 计算他们的输出，从u(ni+1)开始，一直上升到u(n)。如算法6.1中所定义的，每个 节点 u(i) 与操作 f(i) 相关联，并且通过对以下函数求值来得到</p>

<p>u(i) = f(A(i)), (6.48)</p>

<p>其中 A(i) 是 u(i) 所有父节点的集合。</p>

<p>该算法详细说明了前向传播的计算，我们可以将其放人图G中。为了执行反向 传播，我们可以构造一个依赖于 G 并添加额外一组节点的计算图。这形成了一个子 图B，它的每个节点都是G的节点。B中的计算和G中的计算顺序完全相反，而且 B中的每个节点计算导数d£与前向图中的节点u(i)相关联。这通过对标量输出</p>

<p>算法 6.1 计算将 ni 个输人 u(1) 到 u(ni) 映射到一个输出 u(n) 的程序。这定义了一</p>

<p>个计算图，其中每个节点通过将函数f(i)应用到变量集合A(i)上来计算u(i)的值， A⑷包含先前节点u⑴的值满足j &lt; i且j e Pa(u⑴)。计算图的输人是向量x，并 且被分配给前ni个节点u⑴到u(ni)。计算图的输出可以从最后一个(输出)节点 u⑷读出。_</p>

<p>for i = 1, &hellip; , ni do</p>

<p>u(i) Xi</p>

<p>end for</p>

<p>for i = ni + 1, &hellip; , n do</p>

<p>A⑴ I {uj) | j e Pa(u(i))} u⑷i f⑷(A⑴)</p>

<p>end for return u(n)</p>

<p>u(n) 使用链式法则来完成：</p>

<p>du(n) du(n) du(i) (6 49)</p>

<p>du^= h du^ (6.49)</p>

<p>i:jEPa(u(i))</p>

<p>这在算法6.2中详细说明。子图B恰好包含每一条对应着G中从节点uj)到节点 u⑴的边。从uj)到u(i)的边对应着计算dj)。另外，对于每个节点都要执行一个 内积，内积的一个因子是对于uj子节点u(i)的已经计算的梯度，另一个因子是对于 相同子节点u(i)的偏导数dg组成的向量。总而言之，执行反向传播所需的计算量 与G中的边的数量成比例，其中每条边的计算包括计算偏导数(节点关于它的一个 父节点的偏导数)以及执行一次乘法和一次加法。下面，我们将此分析推广到张量 值节点，这只是在同一节点中对多个标量值进行分组并能够更高效地实现。</p>

<p>反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。具体来</p>

<p>说，它大约对图中的每个节点执行一个Jacobian乘积。这可以从算法6.2中看出，反 向传播算法访问了图中的节点 u(j) 到节点 u(i) 的每条边一次，以获得相关的偏导数 dj)。反向传播因此避免了重复子表达式的指数爆炸。然而，其他算法可能通过对 计算图进行简化来避免更多的子表达式，或者也可能通过重新计算而不是存储这些 子表达式来节省内存。我们将在描述完反向传播算法本身后再重新审视这些想法。</p>

<p>图6.9:计算梯度时导致重复子表达式的计算图。令w e R为图的输人。我们对链中的每一步使 用相同的操作函数f : R 4 R，这样x = f(w),y = f(x),z = f(y)。为了计算dw，我们应用</p>

<p>式 (6 . 44)得到：</p>

<p>dz dw</p>

<p>(6.50)</p>

<p>(6.51)</p>

<p>(6.52)</p>

<p>=竺迮么</p>

<p>dy dx dw =f ’(y)f ’(x)f&rsquo;(w)</p>

<p>=f(f(f (w)))f &lsquo;(f (w))f (w). (6.53)</p>

<p>式(6.52)建议我们采用的实现方式是，仅计算 f(w) 的值一次并将它存储在变量 x 中。这是反 向传播算法所采用的方法。式(6.53)提出了一种替代方法，其中子表达式 f(w) 出现了不止一 次。在替代方法中，每次只在需要时重新计算f(w)。当存储这些表达式的值所需的存储较少时， 式(6.52)的反向传播方法显然是较优的，因为它减少了运行时间。然而，式(6.53)也是链式法则的 有效实现，并且当存储受限时它是有用的。</p>

<p>6.5.4 全连接 MLP 中的反向传播计算
为了阐明反向传播的上述定义，让我们考虑一个与全连接的多层MLP相关联 的特定图。</p>

<p>算法6.3首先给出了前向传播，它将参数映射到与单个训练样本(输人，目标) (x, y)相关联的监督损失函数L(y, y)，其中&amp;是当x提供输人时神经网络的输出。</p>

<p>算法6.4随后说明了将反向传播应用于该图所需的相关计算。</p>

<p>算法 6.2 反向传播算法的简化版本，用于计算 u(n) 关于图中变量的导数。这个示 例旨在通过演示所有变量都是标量的简化情况来进一步理解反向传播算法，这里我 们希望计算关于 u(1) ’ &hellip; ’ u(ni) 的导数。这个简化版本计算了关于图中所有节点的导 数。假定与每条边相关联的偏导数计算需要恒定的时间的话，该算法的计算成本与 图中边的数量成比例。这与前向传播的计算次数具有相同的阶。每个是u(i)的 父节点u(j)的函数，从而将前向图的节点链接到反向传播图中添加的节点。</p>

<p>运行前向传播(对于此例是算法6.1)获得网络的激活。</p>

<p>初始化grad_table，用于存储计算好的导数的数据结构。grad_table[u⑴]将存</p>

<p>储dun计算好的值。</p>

<p>grad_table[u⑷]卜 1</p>

<p>for j=n-1 down to 1 do</p>

<p>下一行使用存储的值计算duj) = ^i：-epa(u(i))iui)duj):</p>

<p>grad_table[u(j)]仏 Zi:-ePa(u(i)) grad_table[u⑴]dUj)</p>

<p>end for</p>

<p>return {grad_table[u(i)] | i = 1’ &hellip; ’ ni}</p>

<p>算法6.3和算法6.4是简单而直观的演示。然而，它们专门针对特定的问题。</p>

<p>现在的软件实现基于之后第6.5.6节中描述的一般形式的反向传播，它可以通过</p>

<p>显式地操作表示符号计算的数据结构，来适应任何计算图。</p>

<p>6.5.5 符号到符号的导数
代数表达式和计算图都对符号(symbol)或不具有特定值的变量进行操作。这 些代数或者基于图的表达式被称为符号表示(symbolic representation )。当我们实 际使用或者训练神经网络时，我们必须给这些符号赋特定的值。我们用一个特定 的数值(numeric value )来替代网络的符号输人z，例如[1.2’3’765’ -1.8]T。</p>

<p>一些反向传播的方法采用计算图和一组用于图的输人的数值，然后返回在这些 输人值处梯度的一组数值。我们将这种方法称为符号到数值的微分。这种方法用在 诸如 Torch (Collobert et al., 2011b) 和 Caffe (Jia, 2013) 之类的库中。</p>

<p>另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节 点提供了我们所需导数的符号描述。这是 Theano (Bergstra et al., 2010b; Bastien</p>

<p>算法6.3典型深度神经网络中的前向传播和代价函数的计算。损失函数L(紅y)取 决于输出&amp;和目标y (参考第6.2.1.1节中损失函数的示例)。为了获得总代价J，损 失函数可以加上正则项叫0)，其中0包含所有参数(权重和偏置)。算法6.4说明了 如何计算J关于参数W和b的梯度。为简单起见，该演示仅使用单个输人样本x。 实际应用应该使用小批量。请参考第6.5.7节以获得更加真实的演示。</p>

<p>Require: 网络深度， l</p>

<p>Require: W(i) ,i e {1,, 1}，模型的权重矩阵 Require: b(i),i e {1,…，1}，模型的偏置参数 Require: x，程序的输人</p>

<p>Require: y，目标输出</p>

<p>h(0) = x for k = 1 , . .</p>

<p>. , 1 do</p>

<p>a(k) = b(k) + W(k)h(k-1) h(k) = f(a(k))</p>

<p>end for</p>

<p>y = h(l)</p>

<p>J = L(y, y) + AQ(0)</p>

<p>etal.,2012b) 和 TensorFlow (Abadi et al., 2015) 所采用的方法。图6.10给出了该方 法如何工作的一个例子。这种方法的主要优点是导数可以使用与原始表达式相同的 语言来描述。因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数 再进行求导就能得到更高阶的导数。高阶导数的计算在第6.5.10节中描述。</p>

<p>我们将使用后一种方法，并且使用构造导数的计算图的方法来描述反向传播算</p>

<p>法。图的任意子集之后都可以使用特定的数值来求值。这允许我们避免精确地指明</p>

<p>每个操作应该在何时计算。相反，通用的图计算引擎只要当一个节点的父节点的值</p>

<p>都可用时就可以进行求值。</p>

<p>基于符号到符号的方法的描述包含了符号到数值的方法。符号到数值的方法可</p>

<p>以理解为执行了与符号到符号的方法中构建图的过程中完全相同的计算。关键的区</p>

<p>别是符号到数值的方法不会显示出计算图。</p>

<p>算法6.4深度神经网络中算法6.3的反向计算，它不止使用了输人x和目标y。该 计算对于每一层k都产生了对激活a(k)的梯度，从输出层开始向后计算一直到第一 个隐藏层。这些梯度可以看作是对每层的输出应如何调整以减小误差的指导，根据 这些梯度可以获得对每层参数的梯度。权重和偏置上的梯度可以立即用作随机梯度 更新的一部分(梯度算出后即可执行更新)，或者与其他基于梯度的优化方法一起使</p>

<p>用_</p>

<p>在前向计算完成后，计算顶层的梯度： g ▽ J = VyL(y, y) for k = l, l - 1 , &hellip; , 1 do</p>

<p>将关于层输出的梯度转换为非线性激活输人前的梯度(如果f是逐元素的，则</p>

<p>逐元素地相乘)：</p>

<p>g Va(k)J = g © f7(a⑷)</p>

<p>计算关于权重和偏置的梯度(如果需要的话，还要包括正则项)：</p>

<p>▽bw J = g+ AVb(k)Q(0)</p>

<p>V^(k)J = g h(k-1)T + AV^(k)Q(0)</p>

<p>关于下一更低层的隐藏层传播梯度： g Vh(k-i)J = W(k)T g</p>

<p>end for</p>

<p>图 6.10: 使用符号到符号的方法计算导数的示例。在这种方法中，反向传播算法不需要访问任何实 际的特定数值。相反，它将节点添加到计算图中来描述如何计算这些导数。通用图形求值引擎可</p>

<p>以在随后计算任何特定数值的导数。f左J在这个例子中，我们从表示z = f(f(f(w)))的图开始。 (右)我们运行反向传播算法，指导它构造表达式对应的图。在这个例子中，我们不解释反向</p>

<p>传播算法如何工作。我们的目的只是说明想要的结果是什么：符号描述的导数的计算图。</p>

<p>6.5.6 一般化的反向传播
反向传播算法非常简单。为了计算某个标量z关于图中它的一个祖先Z的梯 度，我们首先观察到它关于z的梯度由dz = i给出。然后，我们可以计算对图中z 的每个父节点的梯度，通过现有的梯度乘以产生z的操作的Jacobian。我们继续乘 以Jacobian，以这种方式向后穿过图，直到我们到达z。对于从z出发可以经过两 个或更多路径向后行进而到达的任意节点，我们简单地对该节点来自不同路径上的 梯度进行求和。</p>

<p>更正式地，图 G 中的每个节点对应着一个变量。为了实现最大的一般化，我们 将这个变量描述为一个张量V。张量通常可以具有任意维度，并且包含标量、向量 和矩阵。</p>

<p>我们假设每个变量 V 与下列子程序相关联：</p>

<p>• get_operation(V)： 它返回用于计算 V 的操作，代表了在计算图中流人 V 的边。例如， 可能有一个 Python 或者 C++ 的类表示矩阵乘法操作，以 及get_operation函数。假设我们的一个变量是由矩阵乘法产生的，C= AB。</p>

<p>那么， get_operation(V) 返回一个指向相应 C++ 类的实例的指针。</p>

<p>• get_consumers(V, G):它返回一组变量，是计算图G中V的子节点。</p>

<p>• get_inputs(V, G):它返回一组变量，是计算图G中V的父节点。</p>

<p>每个操作 op 也与 bprop 操作相关联。该 bprop 操作可以计算如式(6.47)所描 述的 Jacobian 向量积。这是反向传播算法能够实现很大通用性的原因。每个操作负 责了解如何通过它参与的图中的边来反向传播。例如，我们可以使用矩阵乘法操作 来产生变量C= 4B。假设标量z关于C的梯度是G。矩阵乘法操作负责定义两 个反向传播规则，每个规则对应于一个输人变量。如果我们调用 bprop 方法来请求 关于4的梯度，那么在给定输出的梯度为G的情况下，矩阵乘法操作的bprop方 法必须说明关于4的梯度是GBT。类似的，如果我们调用bprop方法来请求关 于B的梯度，那么矩阵操作负责实现bprop方法并指定希望的梯度是4TG。反向 传播算法本身并不需要知道任何微分法则。它只需要使用正确的参数调用每个操作 的 bprop 方法即可。正式地， op.bprop(inputs, X, G) 必须返回</p>

<p>y^(Vxop.f(inputs)i)Gi, (6.54)</p>

<p>i</p>

<p>这只是如式(6.47)所表达的链式法则的实现。这里， inputs 是提供给操作的一组输 人，op.f是操作实现的数学函数，X是输人，我们想要计算关于它的梯度，G是操 作对于输出的梯度。</p>

<p>op.bprop 方法应该总是假装它的所有输人彼此不同，即使它们不是。例如，如 果 mul 操作传递两个 x 来计算 x2， op.bprop 方法应该仍然返回 x 作为对于两个输 人的导数。反向传播算法后面会将这些变量加起来获得2x，这是x上总的正确的导 数。</p>

<p>反向传播算法的软件实现通常提供操作和其 bprop 方法，所以深度学习软件库 的用户能够对使用诸如矩阵乘法、指数运算、对数运算等等常用操作构建的图进行 反向传播。构建反向传播新实现的软件工程师或者需要向现有库添加自己的操作的 高级用户通常必须手动为新操作推导 op.bprop 方法。</p>

<p>反向传播算法的正式描述参考算法6.5 。</p>

<p>在第6.5.2节中，我们使用反向传播作为一种策略来避免多次计算链式法则中的</p>

<p>相同子表达式。由于这些重复子表达式的存在，简单的算法可能具有指数运行时间</p>

<p>现在我们已经详细说明了反向传播算法，我们可以去理解它的计算成本。如果我们</p>

<p>算法 6.5 反向传播算法最外围的骨架。这部分做简单的设置和清理工作。大多数重</p>

<p>要的工作发生在算法6.6的子程序build_grad中。</p>

<p>Require: T，需要计算梯度的目标变量集</p>

<p>Require: G，计算图</p>

<p>Require: z，要微分的变量</p>

<p>令G7为G剪枝后的计算图，其中仅包括z的祖先以及T中节点的后代。</p>

<p>初始化grad_table，它是关联张量和对应导数的数据结构。 grad_table [z]卜 1</p>

<p>for V in T do</p>

<p>build_grad(V, G, G7, grad_table)</p>

<p>end for</p>

<p>Return grad_table restricted to T</p>

<p>假设每个操作的执行都有大致相同的开销，那么我们可以依据执行操作的数量来分 析计算成本。注意这里我们将一个操作记为计算图的基本单位，它实际可能包含许 多算术运算(例如，我们可能将矩阵乘法视为单个操作)。在具有 n 个节点的图中计 算梯度，将永远不会执行超过 O(n2) 个操作，或者存储超过 O(n2) 个操作的输出 这里我们是对计算图中的操作进行计数，而不是由底层硬件执行的单独操作，所以 重要的是要记住每个操作的运行时间可能是高度可变的。例如，两个矩阵相乘可能 对应着图中的一个单独的操作，但这两个矩阵可能每个都包含数百万个元素。我们 可以看到，计算梯度至多需要 O(n2) 的操作，因为在最坏的情况下，前向传播的步 骤将在原始图的全部n个节点上运行(取决于我们想要计算的值，我们可能不需要 执行整个图)。反向传播算法在原始图的每条边添加一个Jacobian向量积，可以用 0(1)个节点来表达。因为计算图是有向无环图，它至多有O(n2)条边。对于实践中 常用图的类型，情况会更好。大多数神经网络的代价函数大致是链式结构的，使得 反向传播只有 O(n) 的成本。这远远胜过简单的方法，简单方法可能需要在指数级 的节点上运算。这种潜在的指数级代价可以通过非递归地扩展和重写递归链式法则 (式(6.49))来看出：</p>

<p>du(n)</p>

<p>du(j)</p>

<p>t</p>

<p>e n</p>

<p>path(u(ni),u(n2),…,u(nt)), k=2 from ni =j to nt=n</p>

<p>du(nk)</p>

<p>du(nk-1)</p>

<p>(6.55)</p>

<p>由于节点 j 到节点 n 的路径数目可以关于这些路径的长度上指数地增长，所以上述</p>

<p>算法6.6反向传播算法的内循环子程序build_grad(V’ G’ Gz’ grad<em>table)，由算 法6.5中定义的反向传播算法调用。</em></p>

<p>Require: V，应该被加到G和grad_table的变量。</p>

<p>Require: G，要修改的图。</p>

<p>Require: Gz，根据参与梯度的节点G的受限图。</p>

<p>Require: grad_table，将节点映射到对应梯度的数据结构。 if V is in grad_table then</p>

<p>Return grad_table[V] end if i 1</p>

<p>for C in get_consumers(V’ Gz) do op 卜 get_operation&copy;</p>

<p>D 卜 build<em>grad(C’ G’ Gz’ grad</em> table)</p>

<p>G(i)卜 op.bprop(get_inputs(C’G，)’ V’ D) i i + 1</p>

<p>end for</p>

<p>G Ei G(i)</p>

<p>grad_table[V] = G</p>

<p>插人 G 和将其生成到 G 中的操作</p>

<p>Return G</p>

<p>求和符号中的项数（这些路径的数目），可能以前向传播图的深度的指数级增长。会 产生如此大的成本是因为对于dUi），相同的计算会重复进行很多次。为了避免这种 重新计算，我们可以将反向传播看作一种表填充算法，利用存储的中间结果^来 对表进行填充。图中的每个节点对应着表中的一个位置，这个位置存储对该节点的 梯度。通过顺序填充这些表的条目，反向传播算法避免了重复计算许多公共子表达 式。这种表填充策略有时被称为动态规划（dynamic programming ）。</p>

<p>6.5.7 实例：用于 MLP 训练的反向传播
作为一个例子，我们利用反向传播算法来训练多层感知机。</p>

<p>这里，我们考虑一个具有单个隐藏层的非常简单的多层感知机。为了训练这个</p>

<p>模型，我们将使用小批量随机梯度下降算法。反向传播算法用于计算单个小批量上 的代价的梯度。具体来说，我们使用训练集上的一小批量实例，将其规范化为一个设 计矩阵X以及相关联的类标签向量y。网络计算隐藏特征层= max｛0, XW(1)｝。 为了简化表示，我们在这个模型中不使用偏置。假设我们的图语言包含relu操作， 该操作可以对max｛0,Z｝表达式的每个元素分别进行计算。类的非归一化对数概率 的预测将随后由HW(2｝给出。假设我们的图语言包含cross_entropy操作，用以 计算目标 y 和由这些未归一化对数概率定义的概率分布间的交叉熵。所得到的交叉 熵定义了代价函数Jmle。最小化这个交叉熵将执行对分类器的最大似然估计。然而， 为了使得这个例子更加真实，我们也包含一个正则项。总的代价函数为</p>

<p>J = Jmle + JE(Wij))2 + £ (Wj))2) (6.56)</p>

<p>i,j i,j</p>

<p>包含了交叉熵和系数为A的权重衰减项。它的计算图在图6.11中给出。</p>

<p>图 6.11: 用于计算代价函数的计算图，这个代价函数是使用交叉熵损失以及权重衰减训练我们的 单层 MLP 示例所产生的。</p>

<p>这个示例的梯度计算图实在太大，以致绘制或者阅读都将是乏味的。这显示出</p>

<p>了反向传播算法的优点之一，即它可以自动生成梯度，而这种计算对于软件工程师 来说需要进行直观但冗长的手动推导。</p>

<p>我们可以通过观察图6.11中的正向传播图来粗略地描述反向传播算法的行为。 为了训练，我们希望计算Vgi) J和V^(2) J。有两种不同的路径从J后退到权重:</p>

<p>一条通过交叉熵代价，另一条通过权重衰减代价。权重衰减代价相对简单，它总是</p>

<p>对妒4)上的梯度贡献2AW(i)。</p>

<p>另一条通过交叉熵代价的路径稍微复杂一些。令 G 是由 cross_entropy 操作</p>

<p>提供的对未归一化对数概率以2)的梯度。反向传播算法现在需要探索两个不同的分 支。在较短的分支上，它使用对矩阵乘法的第二个变量的反向传播规则，将ffTG加 到 W(2) 的梯度上。另一条更长些的路径沿着网络逐步下降。首先，反向传播算法使 用对矩阵乘法的第一个变量的反向传播规则，计算VhJ= GW^T。接下来，relu 操作使用其反向传播规则对先前梯度的部分位置清零，这些位置对应着中所有 小于0的元素。记上述结果为G&rsquo;。反向传播算法的最后一步是使用对matmul操作 的第二个变量的反向传播规则，将X G加到W(1)的梯度上。</p>

<p>在计算了这些梯度以后，梯度下降算法或者其他优化算法所要做的就是使用这</p>

<p>些梯度来更新参数。</p>

<p>对于MLP，计算成本主要来源于矩阵乘法。在前向传播阶段，我们乘以每个权 重矩阵，得到了 O(w) 数量的乘-加，其中 w 是权重的数量。在反向传播阶段，我们 乘以每个权重矩阵的转置，这具有相同的计算成本。算法主要的存储成本是我们需</p>

<p>要将输入存储到隐藏层的非线性中去。这些值从被计算时开始存储，直到反向过程</p>

<p>回到了同一点。因此存储成本是O(mnh)，其中m是小批量中样本的数目，w是隐 藏单元的数量。</p>

<p>6.5.8 复杂化
我们这里描述的反向传播算法要比实践中实际使用的实现要简单。</p>

<p>正如前面提到的，我们将操作的定义限制为返回单个张量的函数。大多数软件</p>

<p>实现需要支持可以返回多个张量的操作。例如，如果我们希望计算张量中的最大值</p>

<p>和该值的索引，则最好在单次运算中计算两者，因此将该过程实现为具有两个输出</p>

<p>的操作效率更高。</p>

<p>我们还没有描述如何控制反向传播的内存消耗。反向传播经常涉及将许多张量</p>

<p>加在一起。在朴素方法中，将分别计算这些张量中的每一个，然后在第二步中对所</p>

<p>有这些张量求和。朴素方法具有过高的存储瓶颈，可以通过保持一个缓冲器，并且</p>

<p>在计算时将每个值加到该缓冲器中来避免该瓶颈。</p>

<p>反向传播的现实实现还需要处理各种数据类型，例如 32 位浮点数、 64 位浮点 数和整型。处理这些类型的策略需要特别的设计考虑。</p>

<p>一些操作具有未定义的梯度，并且重要的是跟踪这些情况并且确定用户请求的</p>

<p>梯度是否是未定义的。</p>

<p>各种其他技术的特性使现实世界的微分更加复杂。这些技术性并不是不可逾越</p>

<p>的，本章已经描述了计算微分所需的关键知识工具，但重要的是要知道还有许多的</p>

<p>精妙之处存在。</p>

<p>6.5.9 深度学习界以外的微分
深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来，并且在很大</p>

<p>程度上发展了自己关于如何进行微分的文化态度。更一般地， 自动微分( automatic differentiation)领域关心如何以算法方式计算导数。这里描述的反向传播算法只是 自动微分的一种方法。它是一种称为反向模式累加(reverse mode accumulation )的 更广泛类型的技术的特殊情况。其他方法以不同的顺序来计算链式法则的子表达式 一般来说，确定一种计算的顺序使得计算开销最小，是困难的问题。找到计算梯度 的最优操作序列是 NP 完全问题 (Naumann, 2008)，在这种意义上，它可能需要将 代数表达式简化为它们最廉价的形式。</p>

<p>例如，假设我们有变量Pl’P2 &hellip;’Pn表示概率，以及变量zi’z2’&hellip;’zn表示未 归一化的对数概率。假设我们定义</p>

<p>exp(zi) Ei exP⑷’</p>

<p>(6.57)</p>

<p>其中我们通过指数化、求和与除法运算构建softmax函数，并构造交叉熵损失函</p>

<p>数J = i Pi log %。人类数学家可以观察到J对々的导数有一个非常简单的形 式：qi-Pi反向传播算法不能够以这种方式来简化梯度，而是会通过原始图中的所 有对数和指数操作显式地传播梯度。一些软件库如 Theano (Bergstra et al., 2010b; Bastien et al., 2012b) 能够执行某些种类的代数替换来改进由纯反向传播算法提出的</p>

<p>图。</p>

<p>当前向图g具有单个输出节点，并且每个偏导数duj)都可以用恒定的计算量 来计算时，反向传播保证梯度计算的计算数目和前向计算的计算数目是同一个量级：</p>

<p>这可以在算法6.2中看出，因为每个局部偏导数dg以及递归链式公式(式(6.49)) 中相关的乘和加都只需计算一次。因此，总的计算量是O(#edges)。然而，可能通过 对反向传播算法构建的计算图进行简化来减少这些计算量，并且这是NP完全问题。 诸如 Theano 和 TensorFlow 的实现使用基于匹配已知简化模式的试探法，以便重复</p>

<p>地尝试去简化图。我们定义反向传播仅用于计算标量输出的梯度，但是反向传播可 以扩展到计算Jacobian矩阵(该Jacobian矩阵或者来源于图中的k个不同标量节 点，或者来源于包含 k 个值的张量值节点)。朴素的实现可能需要 k 倍的计算：对于 原始前向图中的每个内部标量节点，朴素的实现计算 k 个梯度而不是单个梯度。当 图的输出数目大于输人的数目时，有时更偏向于使用另外一种形式的自动微分，称 为前向模式累加(forward mode accumulation)。前向模式计算已经被提出用于循 环神经网络梯度的实时计算，例如 (Williams and Zipser, 1989)。这也避免了存储整 个图的值和梯度的需要，是计算效率和内存使用的折中。前向模式和后向模式的关 系类似于左乘和右乘一系列矩阵之间的关系，例如</p>

<p>4BCD,</p>

<p>(6.58)</p>

<p>其中的矩阵可以认为是Jacobian矩阵。例如，如果D是列向量，而4有很多行， 那么这对应于一幅具有单个输出和多个输人的图，并且从最后开始乘，反向进行，只 需要矩阵-向量的乘积。这对应着反向模式。相反，从左边开始乘将涉及一系列的矩 阵-矩阵乘积，这使得总的计算变得更加昂贵。然而，如果 4 的行数小于 D 的列数 则从左到右乘更为便宜，这对应着前向模式。</p>

<p>在机器学习以外的许多社区中，更常见的是使用传统的编程语言来直接实现微 分软件，例如用 Python 或者 C 来编程，并且自动生成使用这些语言编写的不同函 数的程序。在深度学习界中，计算图通常使用由专用库创建的明确的数据结构表示。 专用方法的缺点是需要库开发人员为每个操作定义 bprop 方法，并且限制了库的用 户仅使用定义好的那些操作。然而，专用方法也允许定制每个操作的反向传播规则 允许开发者以非显而易见的方式提高速度或稳定性，对于这种方式自动的过程可能 不能复制。</p>

<p>因此，反向传播不是计算梯度的唯一方式或最佳方式，但它是一个非常实用的</p>

<p>方法，继续为深度学习社区服务。在未来，深度网络的微分技术可能会提高，因为</p>

<p>深度学习的从业者更加懂得了更广泛的自动微分领域的进步。</p>

<p>6.5.10 高阶微分
一些软件框架支持使用高阶导数。在深度学习软件框架中，这至少包括 Theano</p>

<p>和TensorFlow。这些库使用一种数据结构来描述要被微分的原始函数，它们使用相 同类型的数据结构来描述这个函数的导数表达式。这意味着符号微分机制可以应用</p>

<p>于导数(从而产生高阶导数)。</p>

<p>在深度学习的相关领域，很少会计算标量函数的单个二阶导数。相反，我们通 常对Hessian矩阵的性质比较感兴趣。如果我们有函数f : Rn 4 R，那么Hessian 矩阵的大小是nxn。在典型的深度学习应用中，n将是模型的参数数量，可能很容 易达到数十亿。因此，完整的Hessian矩阵甚至不能表示。</p>

<p>典型的深度学习方法是使用Krylov方法(Krylov method )，而不是显式地计 算 Hessian 矩阵。 Krylov 方法是用于执行各种操作的一组迭代技术，这些操作包括</p>

<p>像近似求解矩阵的逆、或者近似矩阵的特征值或特征向量等，而不使用矩阵-向量乘</p>

<p>法以外的任何操作。</p>

<p>为了在 Hesssian 矩阵上使用 Krylov 方法，我们只需要能够计算 Hessian 矩阵 H和一个任意向量T间的乘积即可。实现这一目标的一种直观方法(Christianson,</p>

<p>1992) 是</p>

<p>Hv = Vx [(Vxf (x))Tv] . (6.59)</p>

<p>该表达式中两个梯度的计算都可以由适当的软件库自动完成。注意，外部梯度表达</p>

<p>式是内部梯度表达式的函数的梯度。</p>

<p>如果 v 本身是由计算图产生的一个向量，那么重要的是指定自动微分软件不要</p>

<p>对产生 v 的图进行微分。</p>

<p>虽然计算 Hessian 通常是不可取的，但是可以使用 Hessian 向量积。可以对 所有的i = 1,&hellip;,n简单地计算He⑴，其中e⑴是e^ = 1并且其他元素都为0 的one-hot向量。</p>

<p>6.6 历史小记
前馈网络可以被视为一种高效的非线性函数近似器，它以使用梯度下降来最小</p>

<p>化函数近似误差为基础。从这个角度来看，现代前馈网络是一般函数近似任务的几</p>

<p>个世纪进步的结晶。</p>

<p>处于反向传播算法底层的链式法则是17世纪发明的(Leibniz, 1676; L&rsquo;Hopital, 1696)。微积分和代数长期以来被用于求解优化问题的封闭形式，但梯度下降直到 19 世纪才作为优化问题的一种迭代近似的求解方法被引入 (Cauchy, 1847)。</p>

<p>从 20 世纪 40 年代开始，这些函数近似技术被用于导出诸如感知机的机器学习 模型。然而，最早的模型都是基于线性模型。来自包括 Marvin Minsky 的批评指出 了线性模型族的几个缺陷，例如它无法学习 XOR 函数，这导致了对整个神经网络方 法的抵制。</p>

<p>学习非线性函数需要多层感知机的发展和计算该模型梯度的方法。基于动态规 划的链式法则的高效应用开始出现在20世纪60年代和 70年代，主要用于控制领 域 (Kelley, 1960; Bryson and Denham, 1961; Dreyfus, 1962; Bryson and Ho, 1969; Dreyfus, 1973)，也用于灵敏度分析 (Linnainmaa, 1976)。 Werbos (1981) 提出应用这 些技术来训练人工神经网络。这个想法以不同的方式被独立地重新发现后 (LeCun, 1985; Parker, 1985; Rumelhart et al., 1986a)，最终在实践中得以发展。并行分布式 处理(Parallel Distributed Processing ) —书在其中一章提供了第一次成功使用反向 传播的一些实验的结果(Rumelhart et al, 1986b)，这对反向传播的普及做出了巨大 的贡献，并且开启了一个研究多层神经网络非常活跃的时期。然而，该书作者提出 的想法，特别是 Rumelhart 和 Hinton 提出的想法远远超过了反向传播。它们包括一 些关键思想，关于可能通过计算实现认知和学习的几个核心方面，后来被冠以 “ 联 结主义&rdquo; 的名称，因为它强调了神经元之间的连接作为学习和记忆的轨迹的重要性。 特别地，这些想法包括分布式表示的概念 (Hinton et al., 1986)。</p>

<p>在反向传播的成功之后，神经网络研究获得了普及，并在20世纪90年代初达</p>

<p>到高峰。随后，其他机器学习技术变得更受欢迎，直到 2006 年开始的现代深度学习 复兴。</p>

<p>现代前馈网络的核心思想自 20世纪80年代以来没有发生重大变化。仍然使用 相同的反向传播算法和相同的梯度下降方法。 1986年至2015年神经网络性能的大 部分改进可归因于两个因素。首先，较大的数据集减少了统计泛化对神经网络的挑</p>

<p>战的程度。第二，神经网络由于更强大的计算机和更好的软件基础设施已经变得更</p>

<p>大。然而，少量算法上的变化也显著改善了神经网络的性能。</p>

<p>其中一个算法上的变化是用交叉熵族损失函数替代均方误差损失函数。均方误 差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原 理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具</p>

<p>有 sigmoid 和 softmax 输出的模型的性能，而当使用均方误差损失时会存在饱和和 学习缓慢的问题。</p>

<p>另一个显著改善前馈网络性能的算法上的主要变化是使用分段线性隐藏单元来 替代 sigmoid 隐藏单元，例如用整流线性单元。使用 max{0,z} 函数的整流在早期 神经网络中已经被引人，并且至少可以追溯到认知机(Cognitron)和神经认知机 (Neocognitron)(Fukushima, 1975, 1980)。这些早期的模型没有使用整流线性单元</p>

<p>而是将整流用于非线性函数。尽管整流在早期很普及，在 20 世纪 80 年代，整流很</p>

<p>大程度上被 sigmoid 所取代，也许是因为当神经网络非常小时， sigmoid 表现更好。 到 21 世纪初，由于有些迷信的观念，认为必须避免具有不可导点的激活函数，所 以避免了整流线性单元。这在 2009 年开始发生改变。 Jarrett et al. (2009b) 观察到 在神经网络结构设计的几个不同因素中 ‘‘使用整流非线性是提高识别系统性能的最 重要的唯一因素&rdquo;。</p>

<p>对于小的数据集， Jarrett etal.(2009b) 观察到，使用整流非线性甚至比学习隐 藏层的权重值更加重要。随机的权重足以通过整流网络传播有用的信息，允许在顶 部的分类器层学习如何将不同的特征向量映射到类标识。</p>

<p>当有更多数据可用时，学习开始提取足够的有用知识来超越随机选择参数的性</p>

<p>能。 Glorot et al. (2011a) 说明，在深度整流网络中的学习比在激活函数具有曲率或</p>

<p>两侧饱和的深度网络中的学习更容易。</p>

<p>整流线性单元还具有历史意义，因为它们表明神经科学继续对深度学习算法的 发展产生影响。 Glorot et al. (2011a) 从生物学考虑整流线性单元的导出。半整流非 线性旨在描述生物神经元的这些性质：(1) 对于某些输人，生物神经元是完全不活 跃的。 (2) 对于某些输人，生物神经元的输出和它的输人成比例。 (3) 大多数时间 生物神经元是在它们不活跃的状态下进行操作(即它们应该具有稀疏激活(sparse activation))。</p>

<p>当2006 年深度学习开始现代复兴时，前馈网络仍然有不良的声誉。从2006 年</p>

<p>至 2012 年，人们普遍认为，前馈网络不会表现良好，除非它们得到其他模型的辅助</p>

<p>例如概率模型。现在已经知道，只要具备适当的资源和工程实践，前馈网络表现得</p>

<p>非常好。今天，前馈网络中基于梯度的学习被用作发展概率模型的工具，例如第二</p>

<p>十章中描述的变分自编码器和生成式对抗网络。前馈网络中基于梯度的学习自 2012</p>

<p>年以来一直被视为一种强大的技术，并应用于许多其他机器学习任务，而不是被视</p>

<p>为必须由其他技术支持的不可靠技术。在2006年，业内使用无监督学习来支持监督 学习，现在更讽刺的是，更常见的是使用监督学习来支持无监督学习。</p>

<p>前馈网络还有许多未实现的潜力。未来，我们期望它们用于更多的任务，优化</p>

<p>算法和模型设计的进步将进一步提高它们的性能。本章主要描述了神经网络模型族。</p>

<p>在接下来的章节中，我们将讨论如何使用这些模型——如何对它们进行正则化和训</p>

<p>练。</p>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/03-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9B%B8%E5%85%B3/06-%E6%A0%B8%E6%96%B9%E6%B3%95/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">06 核方法</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/06-%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">
            <span class="next-text nav-default">06 逻辑斯谛回归与最大熵模型</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
