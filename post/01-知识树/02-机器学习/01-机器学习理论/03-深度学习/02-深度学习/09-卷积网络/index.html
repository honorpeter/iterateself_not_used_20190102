<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>09 卷积网络 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="相关资料 《深度学习》Ian Goodfellow 需要补充的 aaa INTRODUCTION aaa 第九章 卷积网络 卷积网络( convolutional network)(LeCun, 1989)，也叫做 卷积神经网络( convolutional neural network, CNN )，是一种专门用来处理具有类" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="09 卷积网络" />
<meta property="og:description" content="相关资料 《深度学习》Ian Goodfellow 需要补充的 aaa INTRODUCTION aaa 第九章 卷积网络 卷积网络( convolutional network)(LeCun, 1989)，也叫做 卷积神经网络( convolutional neural network, CNN )，是一种专门用来处理具有类" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" /><meta property="article:published_time" content="2018-06-12T22:19:29&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-12T22:19:29&#43;00:00"/>
<meta itemprop="name" content="09 卷积网络">
<meta itemprop="description" content="相关资料 《深度学习》Ian Goodfellow 需要补充的 aaa INTRODUCTION aaa 第九章 卷积网络 卷积网络( convolutional network)(LeCun, 1989)，也叫做 卷积神经网络( convolutional neural network, CNN )，是一种专门用来处理具有类">


<meta itemprop="datePublished" content="2018-06-12T22:19:29&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-12T22:19:29&#43;00:00" />
<meta itemprop="wordCount" content="25247">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="09 卷积网络"/>
<meta name="twitter:description" content="相关资料 《深度学习》Ian Goodfellow 需要补充的 aaa INTRODUCTION aaa 第九章 卷积网络 卷积网络( convolutional network)(LeCun, 1989)，也叫做 卷积神经网络( convolutional neural network, CNN )，是一种专门用来处理具有类"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">最近</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">最近</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">09 卷积网络</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-12 </span>
        
        <span class="more-meta"> 25247 words </span>
        <span class="more-meta"> 51 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#相关资料">相关资料</a></li>
<li><a href="#需要补充的">需要补充的</a></li>
</ul></li>
<li><a href="#introduction">INTRODUCTION</a></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h2 id="相关资料">相关资料</h2>

<ol>
<li>《深度学习》Ian Goodfellow</li>
</ol>

<h2 id="需要补充的">需要补充的</h2>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>第九章 卷积网络
卷积网络( convolutional network)(LeCun, 1989)，也叫做 卷积神经网络( convolutional neural network, CNN )，是一种专门用来处理具有类似网格结构的数据的</p>

<p>神经网络。例如时间序列数据(可以认为是在时间轴上有规律地采样形成的一维网 格)和图像数据(可以看作是二维的像素网格)。卷积网络在诸多应用领域都表现优 异。“卷积神经网络” 一词表明该网络使用了卷积(convolution)这种数学运算。卷</p>

<p>积是一种特殊的线性运算。 卷积网络是指那些至少在网络的一层中使用卷积运算来 替代一般的矩阵乘法运算的神经网络。</p>

<p>本章，我们首先说明什么是卷积运算。接着，我们会解释在神经网络中使用卷 积运算的动机。然后我们会介绍池化(pooling)，这是一种几乎所有的卷积网络都会 用到的操作。通常来说，卷积神经网络中用到的卷积运算和其他领域(例如工程领 域以及纯数学领域)中的定义并不完全一致。我们会对神经网络实践中广泛应用的 几种卷积函数的变体进行说明。我们也会说明如何在多种不同维数的数据上使用卷 积运算。之后我们讨论使得卷积运算更加高效的一些方法。卷积网络是神经科学原 理影响深度学习的典型代表。我们之后也会讨论这些神经科学的原理，并对卷积网 络在深度学习发展史中的作用作出评价。本章没有涉及如何为你的卷积网络选择合 适的结构，因为本章的目标是说明卷积网络提供的各种工具。第十一章将会对如何 在具体环境中选择使用相应的工具给出通用的准则。对于卷积网络结构的研究进展 得如此迅速，以至于针对特定基准(benchmark)，数月甚至几周就会公开一个新的 最优的网络结构，甚至在写这本书时也不好描述究竟哪种结构是最好的。然而，最 好的结构也是由本章所描述的基本部件逐步搭建起来的。</p>

<p>9.1 卷积运算
在通常形式中，卷积是对两个实变函数的一种数学运算1。为了给出卷积的定义</p>

<p>我们从两个可能会用到的函数的例子出发。</p>

<p>假设我们正在用激光传感器追踪一艘宇宙飞船的位置。我们的激光传感器给出</p>

<p>一个单独的输出x(t)，表示宇宙飞船在时刻t的位置。x和t都是实值的，这意味</p>

<p>着我们可以在任意时刻从传感器中读出飞船的位置。</p>

<p>现在假设我们的传感器受到一定程度的噪声干扰。为了得到飞船位置的低噪声 估计，我们对得到的测量结果进行平均。显然，时间上越近的测量结果越相关，所 以我们采用一种加权平均的方法，对于最近的测量结果赋予更高的权重。我们可以 采用一个加权函数 w(a) 来实现，其中 a 表示测量结果距当前时刻的时间间隔。如果 我们对任意时刻都采用这种加权平均的操作，就得到了一个新的对于飞船位置的平 滑估计函数s:</p>

<p>s(t)= x(a)w(t-a)da. (9.1)</p>

<p>这种运算就叫做卷积(convolution)。卷积运算通常用星号表示：</p>

<p>s(t) = (x * w)(t). (9.2)</p>

<p>在我们的例子中， w 必须是一个有效的概率密度函数，否则输出就不再是一个</p>

<p>加权平均。另外，在参数为负值时， w 的取值必须为 0，否则它会预测到未来，这不 是我们能够推测得了的。但这些限制仅仅是对我们这个例子来说。通常，卷积被定 义在满足上述积分式的任意函数上，并且也可能被用于加权平均以外的目的。</p>

<p>在卷积网络的术语中，卷积的第一个参数(在这个例子中，函数x)通常叫做输 入(input)，第二个参数(函数w)叫做核函数(kernel function )。输出有时被称</p>

<p>作 特征映射( feature map)。</p>

<p>在本例中，激光传感器在每个瞬间反馈测量结果的想法是不切实际的。一般地</p>

<p>当我们用计算机处理数据时，时间会被离散化，传感器会定期地反馈数据。所以在我</p>

<p>们的例子中，假设传感器每秒反馈一次测量结果是比较现实的。这样，时刻t只能取 整数值。如果我们假设 x 和 w 都定义在整数时刻 t 上，就可以定义离散形式的卷积</p>

<p>OO</p>

<p>s(t) = (x * w)(t) = x(a)w(t- a). (9.3)</p>

<p>a=-m</p>

<p>译者注：本书中 operation 视语境有时翻译成 ‘‘运算&rdquo;，有时翻译成 ‘‘操作&rsquo;</p>

<p>在机器学习的应用中，输人通常是多维数组的数据，而核通常是由学习算法优</p>

<p>化得到的多维数组的参数。我们把这些多维数组叫做张量。因为在输人与核中的每</p>

<p>一个元素都必须明确地分开存储，我们通常假设在存储了数值的有限点集以外，这</p>

<p>些函数的值都为零。这意味着在实际操作中，我们可以通过对有限个数组元素的求</p>

<p>和来实现无限求和。</p>

<p>最后，我们经常一次在多个维度上进行卷积运算。例如，如果把一张二维的图</p>

<p>像I作为输人，我们也许也想要使用一个二维的核K:</p>

<p>S(i,j) = (I * K)(i, j) = I(m,n)K(i - m,j - n). (9.4)</p>

<p>mn</p>

<p>卷积是可交换的(commutative)，我们可以等价地写作：</p>

<p>S(i,j)=(K*I)(i,j)= I(i-m,j-n)K(m,n). (9.5)</p>

<p>mn</p>

<p>通常，下面的公式在机器学习库中实现更为简单，因为m和n的有效取值范围</p>

<p>相对较小。</p>

<p>卷积运算可交换性的出现是因为我们将核相对输人进行了翻转(flip )，从m增 大的角度来看，输人的索引在增大，但是核的索引在减小。我们将核翻转的唯一目 的是实现可交换性。尽管可交换性在证明时很有用，但在神经网络的应用中却不是 一个重要的性质。与之不同的是，许多神经网络库会实现一个相关的函数，称为互 相关函数(cross-correlation )，和卷积运算几乎一样但是并没有对核进行翻转：</p>

<p>S(i,j)=(I*K)(i,j)= I(i+m,j+n)K(m,n). (9.6)</p>

<p>mn</p>

<p>许多机器学习的库实现的是互相关函数但是称之为卷积。在这本书中我们遵循把两</p>

<p>种运算都叫做卷积的这个传统，在与核翻转有关的上下文中，我们会特别指明是否</p>

<p>对核进行了翻转。在机器学习中，学习算法会在核合适的位置学得恰当的值，所以一</p>

<p>个基于核翻转的卷积运算的学习算法所学得的核，是对未进行翻转的算法学得的核</p>

<p>的翻转。单独使用卷积运算在机器学习中是很少见的，卷积经常与其他的函数一起</p>

<p>使用，无论卷积运算是否对它的核进行了翻转，这些函数的组合通常是不可交换的。</p>

<p>图9.1演示了一个在2维张量上的卷积运算(没有对核进行翻转)的例子。</p>

<p>离散卷积可以看作矩阵的乘法，然而，这个矩阵的一些元素被限制为必须和另外</p>

<p>一些元素相等。例如对于单变量的离散卷积，矩阵每一行中的元素都与上一行对应</p>

<p>Output</p>

<p>―►</p>

<p>aw + bx + ey + fz</p>

<p>bw + cx + fy + gz</p>

<p>cw + dx + gy + hz</p>

<p>ew + fx + iy + jz</p>

<p>fw + gx + jy + kz</p>

<p>gw + hx + ky + lz</p>

<p>图 9.1: 一个 2 维卷积的例子(没有对核进行翻转)。我们限制只对核完全处在图像中的位置进行 输出，在一些上下文中称为 “有效&rdquo; 卷积。我们用画有箭头的盒子来说明输出张量的左上角元素是 如何通过对输入张量相应的左上角区域应用核进行卷积得到的。</p>

<p>Input</p>

<p>a</p>

<p>b</p>

<p>c</p>

<p>d</p>

<p>e</p>

<p>f</p>

<p>g</p>

<p>h</p>

<p>i</p>

<p>j</p>

<p>k</p>

<p>l</p>

<p>Kernel</p>

<p>w</p>

<p>x</p>

<p>y</p>

<p>z</p>

<p>位置平移一个单位的元素相同。这种矩阵叫做Toeplitz矩阵(Toeplitz matrix)。对 于二维情况，卷积对应着一个 双重分块循环矩阵( doubly block circulant matrix)。 除了这些元素相等的限制以外，卷积通常对应着一个非常稀疏的矩阵(一个几乎所 有元素都为零的矩阵)。这是因为核的大小通常要远小于输入图像的大小。任何一个 使用矩阵乘法但是并不依赖矩阵结构的特殊性质的神经网络算法，都适用于卷积运 算，并且不需要对神经网络做出大的修改。典型的卷积神经网络为了更有效地处理 大规模输入，确实使用了一些专门化的技巧，但这些在理论分析方面并不是严格必 要的。</p>

<p>9.2 动机
卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互( sparse</p>

<p>interactions)、 参数共享( parameter sharing)、 等变表示( equivariant representations )。另外，卷积提供了一种处理大小可变的输人的方法。我们下面依次介绍这些</p>

<p>思想。</p>

<p>传统的神经网络使用矩阵乘法来建立输人与输出的连接关系。其中，参数矩</p>

<p>阵中每一个单独的参数都描述了一个输人单元与一个输出单元间的交互。这意 味着每一个输出单元与每一个输人单元都产生交互。然而，卷积网络具有稀疏交 互(sparse interactions)(也叫做稀疏连接(sparse connectivity)或者稀疏权重 (sparse weights))的特征。这是使核的大小远小于输人的大小来达到的。举个例子， 当处理一张图像时，输人的图像可能包含成千上万个像素点，但是我们可以通过只 占用几十到上百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。这 意味着我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统 计效率。这也意味着为了得到输出我们只需要更少的计算量。这些效率上的提高往 往是很显著的。如果有m个输人和n个输出，那么矩阵乘法需要m X n个参数并 且相应算法的时间复杂度为O(m X n)(对于每一个例子)。如果我们限制每一个输 出拥有的连接数为k，那么稀疏的连接方法只需要k X n个参数以及O(k X n)的运 行时间。在很多实际应用中，只需保持k比m小几个数量级，就能在机器学习的 任务中取得好的表现。稀疏连接的图形化解释如图9.2和图9.3所示。在深度卷积网 络中，处在网络深层的单元可能与绝大部分输人是间接交互的，如图9.4所示。这允 许网络可以通过只描述稀疏交互的基石来高效地描述多个变量的复杂交互。</p>

<p>参数共享(parameter sharing)是指在一个模型的多个函数中使用相同的参数。 在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当 它乘以输人的一个元素后就再也不会用到了。作为参数共享的同义词，我们可以说 一个网络含有绑定的权重(tied weights )，因为用于一个输人的权重也会被绑定在 其他的权重上。在卷积神经网络中，核的每一个元素都作用在输人的每一位置上(是 否考虑边界像素取决于对边界决策的设计)。卷积运算中的参数共享保证了我们只需 要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。这虽 然没有改变前向传播的运行时间(仍然是O(kxn))，但它显著地把模型的存储需求 降低至k个参数，并且k通常要比m小很多个数量级。因为m和n通常有着大致 相同的大小， k 在实际中相对于 mX n 是很小的。因此，卷积在存储需求和统计效</p>

<p>图 9.2: 稀疏连接，对每幅图从下往上看。我们强调了一个输人单元 x3 以及在 s 中受该单元影响</p>

<p>的输出单元。f上J当s是由核宽度为3的卷积产生时，只有三个输出受到》的影响2。f下J当s</p>

<p>是由矩阵乘法产生时，连接不再是稀疏的，所以所有的输出都会受到 x3 的影响。</p>

<p>率方面极大地优于稠密矩阵的乘法运算。图9.5演示了参数共享是如何实现的。</p>

<p>作为前两条原则的一个实际例子，图9.6说明了稀疏连接和参数共享是如何显著</p>

<p>提高线性函数在一张图像上进行边缘检测的效率的。</p>

<p>对于卷积，参数共享的特殊形式使得神经网络层具有对平移 等变( equivariance) 的性质。如果一个函数满足输人改变，输出也以同样的方式改变这一性质，我们就说 它是等变 (equivariant) 的。特别地，如果函数 f(x) 与 g(x) 满足 f(g(x)) = g(f(x)) 我们就说 f(x) 对于变换 g 具有等变性。对于卷积来说，如果令 g 是输人的任意平 移函数，那么卷积函数对于g具有等变性。举个例子，令I表示图像在整数坐标上 的亮度函数， g 表示图像函数的变换函数(把一个图像函数映射到另一个图像函数 的函数)使得r = g(I)，其中图像函数r满足Iz(x,y) = I(x - 1, y)。这个函数把I 中的每个像素向右移动一个单位。如果我们先对 I 进行这种变换然后进行卷积操作 所得到的结果，与先对I进行卷积然后再对输出使用平移函数g得到的结果是一样 的4 。当处理时间序列数据时，这意味着通过卷积可以得到一个由输人中出现不同特</p>

<p>&lsquo;译者注：原文将此处误写成了 r</p>

<p>图 9.3: 稀疏连接，对每幅图从上往下看。我们强调了一个输出单元 s3 以及 x 中影响该单元的输</p>

<p>人单元。这些单元被称为S3的接受域（receptive field） 3。f上J当s是由核宽度为3的卷积产生</p>

<p>时，只有三个输入影响 s3。 （下） 当 s 是由矩阵乘法产生时，连接不再是稀疏的，所以所有的输入 都会影响 s3。</p>

<p>图 9.4: 处于卷积网络更深的层中的单元，它们的接受域要比处在浅层的单元的接受域更大。如果 网络还包含类似步幅卷积（图9.12）或者池化（第9.3节）之类的结构特征，这种效应会加强。这 意味着在卷积网络中尽管直接连接都是很稀疏的，但处在更深的层中的单元可以间接地连接到全 部或者大部分输人图像。</p>

<p>图 9.5: 参数共享。黑色箭头表示在两个不同的模型中使用了特殊参数的连接。 （上） 黑色箭头表示 在卷积模型中对 3 元素核的中间元素的使用。因为参数共享，这个单独的参数被用于所有的输人 位置。 （下） 这个单独的黑色箭头表示在全连接模型中对权重矩阵的中间元素的使用。这个模型没 有使用参数共享，所以参数只使用了一次。</p>

<p>征的时刻所组成的时间轴。如果我们把输人中的一个事件向后延时，在输出中仍然 会有完全相同的表示，只是时间延后了。图像与之类似，卷积产生了一个 2 维映射 来表明某些特征在输人中出现的位置。如果我们移动输人中的对象，它的表示也会 在输出中移动同样的量。当处理多个输人位置时，一些作用在邻居像素的函数是很 有用的。例如在处理图像时，在卷积网络的第一层进行图像的边缘检测是很有用的。 相同的边缘或多或少地散落在图像的各处，所以应当对整个图像进行参数共享。但 在某些情况下，我们并不希望对整幅图进行参数共享。例如，在处理已经通过剪裁 而使其居中的人脸图像时，我们可能想要提取不同位置上的不同特征（处理人脸上 部的部分网络需要去搜寻眉毛，处理人脸下部的部分网络就需要去搜寻下巴了）。</p>

<p>卷积对其他的一些变换并不是天然等变的，例如对于图像的放缩或者旋转变换</p>

<p>需要其他的一些机制来处理这些变换。</p>

<p>最后，一些不能被传统的由（固定大小的）矩阵乘法定义的神经网络处理的特</p>

<p>殊数据，可能通过卷积神经网络来处理，我们将在第9.7节中进行讨论。</p>

<p>图 9.6: 边缘检测的效率。右边的图像是通过先获得原始图像中的每个像素，然后减去左边相邻像 素的值而形成的。这个操作给出了输人图像中所有垂直方向上的边缘的强度，对目标检测来说是有</p>

<p>用的。两个图像的高度均为280个像素。输人图像的宽度为320个像素，而输出图像的宽度为319 个像素。这个变换可以通过包含两个元素的卷积核来描述，使用卷积需要319 X 280 X 3 = 267, 960</p>

<p>次浮点运算（每个输出像素需要两次乘法和一次加法）。为了用矩阵乘法描述相同的变换，需要一</p>

<p>个包含320 X 280 X 319 X 280个或者说超过80亿个元素的矩阵，这使得卷积对于表示这种变换</p>

<p>更有效 40 亿倍。直接运行矩阵乘法的算法将执行超过 160 亿次浮点运算，这使得卷积在计算上大</p>

<p>约有 60,000 倍的效率。当然，矩阵的大多数元素将为零。如果我们只存储矩阵的非零元，则矩阵 乘法和卷积都需要相同数量的浮点运算来计算。矩阵仍然需要包含2 X 319 X 280 = 178, 640个元 素。将小的局部区域上的相同线性变换应用到整个输人上，卷积是描述这种变换的极其有效的方 法。照片来源：Paula Goodfellow。</p>

<p>9.3 池化
卷积网络中一个典型层包含三级(如图 9.7 所示)。在第一级中，这一层并行地计 算多个卷积产生一组线性激活响应。在第二级中，每一个线性激活响应将会通过一个 非线性的激活函数，例如整流线性激活函数。这一级有时也被称为 探测级( detector</p>

<p>stage)。在第三级中，我们使用池化函数(pooling function )来进一步调整这一层</p>

<p>的输出。</p>

<p>图9.7: —个典型卷积神经网络层的组件。有两组常用的术语用于描述这些层。f左」在这组术语中，</p>

<p>卷积网络被视为少量相对复杂的层，每层具有许多 ‘‘级&rdquo;。在这组术语中，核张量与网络层之间存</p>

<p>在一一对应关系。在本书中，我们通常使用这组术语。f右」在这组术语中，卷积网络被视为更多</p>

<p>数量的简单层；每一个处理步骤都被认为是一个独立的层。这意味着不是每一‘‘层&rdquo; 都有参数。</p>

<p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。</p>

<p>例如，最大池化(max pooling)函数(Zhou and Chellappa, 1988)给出相邻矩形区 域内的最大值。其他常用的池化函数包括相邻矩形区域内的平均值、L2范数以及基</p>

<p>于据中心像素距离的加权平均函数。</p>

<p>不管采用什么样的池化函数，当输人作出少量平移时，池化能够帮助输人的表</p>

<p>示近似不变（invariant）。对于平移的不变性是指当我们对输人进行少量平移时，经 过池化函数后的大多数输出并不会发生改变。图9.8用了一个例子来说明这是如何实</p>

<p>现的。 局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现 而不关心它出现的具体位置时。例如，当判定一张图像中是否包含人脸时，我们并 不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只</p>

<p>在右边就行了。但在一些其他领域，保存特征的具体位置却很重要。例如当我们想</p>

<p>要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它</p>

<p>们是否相交。</p>

<p>图 9.8: 最大池化引人了不变性。 （上） 卷积层中间输出的视图。下面一行显示非线性的输出。上面 一行显示最大池化的输出，每个池的宽度为三个像素并且池化区域的步幅为一个像素。 （下） 相同 网络的视图，不过对输人右移了一个像素。下面一行的所有值都发生了改变，但上面一行只有一 半的值发生了改变，这是因为最大池化单元只对周围的最大值比较敏感，而不是对精确的位置。</p>

<p>使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对</p>

<p>少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。</p>

<p>对空间区域进行池化产生了平移不变性，但当我们对分离参数的卷积的输出进 行池化时，特征能够学得应该对于哪种变换具有不变性（如图 9.9所示）。</p>

<p>图 9.9: 学习不变性的示例。使用分离的参数学得多个特征，再使用池化单元进行池化，可以学得</p>

<p>对输入的某些变换的不变性。这里我们展示了用三个学得的过滤器和一个最大池化单元可以学得</p>

<p>对旋转变换的不变性。这三个过滤器都旨在检测手写的数字 5。每个过滤器尝试匹配稍微不同方向 的 5。当输入中出现5 时，相应的过滤器会匹配它并且在探测单元中引起大的激活。然后，无论哪 个探测单元被激活，最大池化单元都具有大的激活。我们在这里演示了网络如何处理两个不同的输</p>

<p>人，这导致两个不同的探测单元被激活，然而对池化单元的影响大致相同。这个原则在maxout网 络 (Goodfellow et al., 2013b) 和其他卷积网络中更有影响。空间位置上的最大池化对于平移是天 然不变的；这种多通道方法只在学习其他变换时是必要的。</p>

<p>因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可能，我 们可以通过综合池化区域的 k 个像素的统计特征而不是单个像素来实现。图9.10给</p>

<p>出了一个例子。这种方法提高了网络的计算效率，因为下一层少了约k倍的输人。 当下一层的参数数目是关于那一层输人大小的函数时(例如当下一层是全连接的基 于矩阵乘法的网络层时)，这种对于输人规模的减小也可以提高统计效率并且减少对</p>

<p>于参数的存储需求。</p>

<p>在很多任务中，池化对于处理不同大小的输人具有重要作用。例如我们想对不</p>

<p>同大小的图像进行分类时，分类层的输人必须是固定的大小，而这通常通过调整池</p>

<p>化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最</p>

<p>初的输人大小了。例如，最终的池化层可能会输出四组综合统计特征，每组对应着</p>

<p>图像的一个象限，而与图像的大小无关。</p>

<p>一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导 (Boureau et al., 2010)。将特征一起动态地池化也是可行的， 例如， 对于感兴趣</p>

<p>图 9.10: 带有降采样的池化。这里我们使用最大池化，池的宽度为三并且池之间的步幅为二。这使 得表示的大小减少了一半，减轻了下一层的计算和统计负担。注意到最右边的池化区域尺寸较小 但如果我们不想忽略一些探测单元的话就必须包含这个区域。</p>

<p>特征的位置运行聚类算法 (Boureau et al., 2011)。这种方法对于每幅图像产生一个 不同的池化区域集合。另一种方法是先学习一个单独的池化结构，再应用到全部的 图像中 (Jia et al., 2012)。</p>

<p>池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，例如玻尔兹</p>

<p>曼机和自编码器。这些问题将在第三章中当我们遇到这些类型的网络时进一步讨论。</p>

<p>卷积玻尔兹曼机中的池化出现在第20.6节。一些可微网络中需要的在池化单元上进</p>

<p>行的类逆运算将在第20.10.6节中讨论。</p>

<p>图9.11给出了一些使用卷积和池化操作的用于分类的完整卷积网络结构的例子。</p>

<p>图 9.11: 卷积网络用于分类的结构示例。本图中使用的具体步幅和深度并不建议实际使用；它们 被设计得非常浅以适合页面。实际的卷积网络还常常涉及大量的分支，不同于这里为简单起见所 使用的链式结构。 （左） 处理固定大小的图像的卷积网络。在卷积层和池化层几层交替之后，卷积 特征映射的张量被重新变形以展平空间维度。网络的其余部分是一个普通的前馈网络分类器，如 第六章所述。 （中） 处理大小可变的图像的卷积网络，但仍保持全连接的部分。该网络使用具有可 变大小但是数量固定的池的池化操作，以便向网络的全连接部分提供固定 576 个单位大小的向量 （右） 没有任何全连接权重层的卷积网络。相对的，最后的卷积层为每个类输出一个特征映射。该 模型可能会用来学习每个类出现在每个空间位置的可能性的映射。将特征映射进行平均得到的单 个值，提供了顶部 softmax 分类器的变量。</p>

<p>9.4 卷积与池化作为一种无限强的先验
回忆一下第5.2节中先验概率分布(prior probab胞y distribution )的概念。这</p>

<p>是一个模型参数的概率分布，它刻画了在我们看到数据之前我们认为什么样的模型</p>

<p>是合理的信念。</p>

<p>先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有较高的</p>

<p>熵值，例如方差很大的高斯分布。这样的先验允许数据对于参数的改变具有或多或</p>

<p>少的自由性。强先验具有较低的熵值，例如方差很小的高斯分布。这样的先验在决</p>

<p>定参数最终取值时起着更加积极的作用。</p>

<p>一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值</p>

<p>无论数据对于这些参数的值给出了多大的支持。</p>

<p>我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个</p>

<p>无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相</p>

<p>同，但可以在空间上移动。这个先验也要求除了那些处在隐藏单元的小的空间连续</p>

<p>的接受域内的权重以外，其余的权重都为零。总之，我们可以把卷积的使用当作是</p>

<p>对网络中一层的参数引人了一个无限强的先验概率分布。这个先验说明了该层应该</p>

<p>学得的函数只包含局部连接关系并且对平移具有等变性。类似的，使用池化也是一</p>

<p>个无限强的先验：每一个单元都具有对少量平移的不变性。</p>

<p>当然，把卷积神经网络当作一个具有无限强先验的全连接网络来实现会导致极</p>

<p>大的计算浪费。但把卷积神经网络想成具有无限强先验的全连接网络可以帮助我们</p>

<p>更好地洞察卷积神经网络是如何工作的。</p>

<p>其中一个关键的洞察是卷积和池化可能导致欠拟合。与任何其他先验类似，卷 积和池化只有当先验的假设合理且正确时才有用。如果一项任务依赖于保存精确 的空间信息，那么在所有的特征上使用池化将会增大训练误差。一些卷积网络结 构 (Szegedy et al., 2014a) 为了既获得具有较高不变性的特征又获得当平移不变性不 合理时不会导致欠拟合的特征，被设计成在一些通道上使用池化而在另一些通道上 不使用。当一项任务涉及到要对输人中相隔较远的信息进行合并时，那么卷积所利 用的先验可能就不正确了。</p>

<p>另一个关键洞察是当我们比较卷积模型的统计学习表现时，只能以基准中的其</p>

<p>他卷积模型作为比较的对象。其他不使用卷积的模型即使我们把图像中的所有像素</p>

<p>点都置换后依然有可能进行学习。对于许多图像数据集，还有一些分别的基准，有 些是针对那些具有置换不变性(permutation invariant)并且必须通过学习发现拓</p>

<p>扑结构的模型，还有一些是针对模型设计者将空间关系的知识植入了它们的模型。</p>

<p>9.5 基本卷积函数的变体
当在神经网络的上下文中讨论卷积时，我们通常不是特指数学文献中使用的那</p>

<p>种标准的离散卷积运算。实际应用中的函数略有不同。这里我们详细讨论一下这些</p>

<p>差异，并且对神经网络中用到的函数的一些重要性质进行重点说明。</p>

<p>首先，当我们提到神经网络中的卷积时，我们通常是指由多个并行卷积组成的</p>

<p>运算。这是因为具有单个核的卷积只能提取一种类型的特征，尽管它作用在多个空</p>

<p>间位置上。我们通常希望网络的每一层能够在多个位置提取多种类型的特征。</p>

<p>另外，输入通常也不仅仅是实值的网格，而是由一系列观测数据的向量构成的</p>

<p>网格。例如，一幅彩色图像在每一个像素点都会有红绿蓝三种颜色的亮度。在多层</p>

<p>的卷积网络中，第二层的输入是第一层的输出，通常在每个位置包含多个不同卷积 的输出。当处理图像时，我们通常把卷积的输入输出都看作是 3维的张量，其中一</p>

<p>个索引用于标明不同的通道(例如红绿蓝)，另外两个索引标明在每个通道上的空间 坐标。软件实现通常使用批处理模式，所以实际上会使用 4 维的张量，第四维索引 用于标明批处理中不同的实例，但我们为简明起见这里忽略批处理索引。</p>

<p>因为卷积网络通常使用多通道的卷积，所以即使使用了核翻转，也不一定保证</p>

<p>网络的线性运算是可交换的。只有当其中的每个运算的输出和输入具有相同的通道</p>

<p>数时，这些多通道的运算才是可交换的。。</p>

<p>假定我们有一个4维的核张量K，它的每一个元素是KijAi，表示输出中处于 通道i的一个单元和输人中处于通道j中的一个单元的连接强度，并且在输出单元 和输入单元之间有 k 行 l 列的偏置。假定我们的输入由观测数据 V 组成，它的每一 个元素是Vi,#，表示处在通道i中第j行第k列的值。假定我们的输出Z和输人 V具有相同的形式。如果输出Z是通过对K和V进行卷积而不涉及翻转K得到 的，那么</p>

<p>Zi,j,k = Vl,j+m-1,k+n-1Ki,l,m,n, (9.7)</p>

<p>l,m,n</p>

<p>这里对所有的 l， m 和 n 进行求和是对所有(在求和式中)有效的张量索引的值进</p>

<p>行求和。在线性代数中，向量的索引通常从1 开始，这就是上述公式中 -1的由来。</p>

<p>但是像 C 或 Python 这类编程语言索引通常从 0 开始，这使得上述公式可以更加简</p>

<p>洁。</p>

<p>我们有时会希望跳过核中的一些位置来降低计算的开销(相应的代价是提取</p>

<p>特征没有先前那么好了)。我们可以把这一过程看作是对全卷积函数输出的下采样</p>

<p>(downsampling)。如果我们只想在输出的每个方向上每间隔s个像素进行采样，那</p>

<p>么我们可以定义一个下采样卷积函数 c 使得</p>

<p>Zi,j,k = c(K, V, s )i,j,k =〉:[^1, j—1) xs+m,(k—1) xs+n, 〜,l,m,n] • (9.8)</p>

<p>l,m,n</p>

<p>我们把s称为下采样卷积的步幅(stride)。当然也可以对每个移动方向定义不同的</p>

<p>步幅。图 9.12 演示了一个实例。</p>

<p>在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地对输人 V 用零 进行填充 (pad) 使得它加宽。如果没有这个性质，表示的宽度在每一层就会缩减，缩 减的幅度是比核少一个像素这么多。对输人进行零填充允许我们对核的宽度和输出</p>

<p>的大小进行独立的控制。如果没有零填充，我们就被迫面临二选一的局面，要么选</p>

<p>择网络空间宽度的快速缩减，要么选择一个小型的核——这两种情境都会极大得限</p>

<p>制网络的表示能力。图9.13给出了一个例子。</p>

<p>有三种零填充设定的情况值得注意。第一种是无论怎样都不使用零填充的极端 情况，并且卷积核只允许访问那些图像中能够完全包含整个核的位置。在 MATLAB</p>

<p>的术语中，这称为有效(valid)卷积。在这种情况下，输出的所有像素都是输人中 相同数量像素的函数，这使得输出像素的表示更加规范。然而，输出的大小在每一 层都会缩减。如果输人的图像宽度是m，核的宽度是k，那么输出的宽度就会变成 m - k +1。如果卷积核非常大的话缩减率会非常显著。因为缩减数大于0，这限制 了网络中能够包含的卷积层的层数。当层数增加时，网络的空间维度最终会缩减到 1x1,这种情况下增加的层就不可能进行有意义的卷积了。第二种特殊的情况是只 进行足够的零填充来保持输出和输人具有相同的大小。在MATLAB的术语中，这 称为 相同( same )卷积。在这种情况下，只要硬件支持，网络就能包含任意多的卷 积层，这是因为卷积运算不改变下一层的结构。。然而，输人像素中靠近边界的部分 相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的 欠表示。这使得第三种极端情况产生了，在MATLAB中称为全(full)卷积。它进 行了足够多的零填充使得每个像素在每个方向上恰好被访问了 k 次，最终输出图像 的宽度为m + k-1。在这种情况下，输出像素中靠近边界的部分相比于中间部分是 更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核</p>

<p>图 9.12: 带有步幅的卷积。在这个例子中，我们的步幅为二。 （上）在单个操作中实现的步幅为二的 卷积。 （下）步幅大于一个像素的卷积在数学上等价于单位步幅的卷积随后降采样。显然，涉及降采 样的两步法在计算上是浪费的，因为它计算了许多将被丢弃的值。</p>

<p>更为困难。通常零填充的最优数量（对于测试集的分类正确率）处于 “有效卷积&rdquo; 和 “相同卷积&rdquo; 之间的某个位置。</p>

<p>在一些情况下，我们并不是真的想使用卷积，而是想用一些局部连接的网络层 （LeCun, 1986, 1989）。在这种情况下，我们的多层感知机对应的邻接矩阵是相同的 但每一个连接都有它自己的权重，用一个 6 维的张量 W 来表示。 W 的索引分别是</p>

<p>输出的通道i，输出的行j和列k，输人的通道1，输人的行偏置m和列偏置n。局</p>

<p>部连接层的线性部分可以表示为</p>

<p>Zi,j,k = [Vl,j+m-1,k+n-1 wi,j,k,l,m,n]. （9.9）</p>

<p>l,m,n</p>

<p>參參眷£^000000000000000、參眷</p>

<p>)000000000.</p>

<p>)000000000(</p>

<p>)000000000.</p>

<p>图 9.13: 零填充对网络大小的影响。考虑一个卷积网络，每层有一个宽度为六的核。在这个例子 中，我们不使用任何池化，所以只有卷积操作本身缩小网络的大小。 （上） 在这个卷积网络中，我 们不使用任何隐含的零填充。这使得表示在每层缩小五个像素。从十六个像素的输人开始，我们 只能有三个卷积层，并且最后一层不能移动核，所以可以说只有两层是真正的卷积层。可以通过 使用较小的核来减缓收缩速率，但是较小的核表示能力不足，并且在这种结构中一些收缩是不可 避免的。 （下） 通过向每层添加五个隐含的零，我们防止了表示随深度收缩。这允许我们设计一个 任意深的卷积网络。</p>

<p>这有时也被称为非共享卷积（unshared convolution ），因为它和具有一个小核的离</p>

<p>散卷积运算很像，但并不横跨位置来共享参数。图9.14比较了局部连接、卷积和全连</p>

<p>接的区别。</p>

<p>当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有</p>

<p>的空间上时，局部连接层是很有用的。例如，如果我们想要辨别一张图片是否是人</p>

<p>脸图像时，我们只需要去寻找嘴是否在图像下半部分即可。</p>

<p>使用那些连接被更进一步限制的卷积或者局部连接层也是有用的，例如，限制</p>

<p>图 9.14: 局部连接，卷积和全连接的比较。 （上） 每一小片（接受域）有两个像素的局部连接层。每 条边用唯一的字母标记，来显示每条边都有自身的权重参数。 （中） 核宽度为两个像素的卷积层 该模型与局部连接层具有完全相同的连接。区别不在于哪些单元相互交互，而在于如何共享参数 局部连接层没有参数共享。正如用于标记每条边的字母重复出现所指示的，卷积层在整个输人上 重复使用相同的两个权重。 （下） 全连接层类似于局部连接层，它的每条边都有其自身的参数（在 该图中用字母明确标记的话就太多了）。然而，它不具有局部连接层的连接受限的特征。</p>

<p>每一个输出的通道i仅仅是输人通道l的一部分的函数时。实现这种情况的一种通 用方法是使输出的前m个通道仅仅连接到输人的前n个通道，输出的接下来的m 个通道仅仅连接到输人的接下来的 n 个通道，以此类推。图9.15给出了一个例子。 对少量通道间的连接进行建模允许网络使用更少的参数，这降低了存储的消耗以及</p>

<p>提高了统计效率，并且减少了前向和反向传播所需要的计算量。这些目标的实现并</p>

<p>没有减少隐藏单元的数目。</p>

<p>平铺卷积（ tiled convolution）（Gregor and LeCun, 2010a; Le et al., 2010） 对卷 积层和局部连接层进行了折衷。这里并不是对每一个空间位置的权重集合进行学习 我们学习一组核使得当我们在空间移动时它们可以循环利用。这意味着在近邻的位</p>

<p>置上拥有不同的过滤器，就像局部连接层一样，但是对于这些参数的存储需求仅仅</p>

<p>会增长常数倍，这个常数就是核的集合的大小，而不是整个输出的特征映射的大小</p>

<p>图9.16对局部连接层、平铺卷积和标准卷积进行了比较。</p>

<p>为了用代数的方法定义平铺卷积，令K是一个6维的张量1，其中的两维对应</p>

<p>着输出映射中的不同位置。 K 在这里并没有对输出映射中的每一个位置使用单独的</p>

<p>索引，输出的位置在每个方向上在t个不同的核组成的集合中进行循环。如果t等</p>

<p>于输出的宽度，这就是局部连接层了。</p>

<p>(9.10)</p>

<p>Vl,j+m-1,k+n-1Ki,l,m,n,j%t+1,k%t+1,</p>

<p>l,m,n</p>

<p>这里百分号是取模运算，它的性质包括 t%t=0,(t+1)%t=1 等等。在每一维上使</p>

<p>用不同的t可以很容易对这个方程进行扩展。</p>

<p>局部连接层与平铺卷积层都和最大池化有一些有趣的关联：这些层的探测单元</p>

<p>都是由不同的过滤器驱动的。如果这些过滤器能够学会探测相同隐含特征的不同变</p>

<p>换形式，那么最大池化的单元对于学得的变换就具有不变性(如图9.9所示)。卷积</p>

<p>层对于平移具有内置的不变性。</p>

<p>实现卷积网络时，通常也需要除卷积以外的其他运算。为了实现学习，必须在 给定输出的梯度时能够计算核的梯度。在一些简单情况下，这种运算可以通过卷积 来实现，但在很多我们感兴趣的情况下，包括步幅大于 1的情况，并不具有这样的 性质。</p>

<p>回忆一下卷积是一种线性运算，所以可以表示成矩阵乘法的形式(如果我们首</p>

<p>先把输人张量变形为一个扁平的向量)。其中包含的矩阵是关于卷积核的函数。这个</p>

<p>矩阵是稀疏的并且核的每个元素都复制给矩阵的多个元素。这种观点能够帮助我们</p>

<p>导出实现一个卷积网络所需的很多其他运算。</p>

<p>通过卷积定义的矩阵转置的乘法就是这样一种运算。这种运算用于在卷积层反</p>

<p>向传播误差的导数，所以它在训练多于一个隐藏层的卷积网络时是必要的。如果我们</p>

<p>想要从隐藏层单元重构可视化单元时，同样的运算也是需要的(Simard et al., 1992)。</p>

<p>重构可视化单元是本书第三部分的模型广泛用到的一种运算，这些模型包括自编码</p>

<p>器、RBM和稀疏编码等等。构建这些模型的卷积化的版本都要用到转置化卷积。类 似核梯度运算，这种输人梯度运算在某些情况下可以用卷积来实现，但在一般情况 下需要用到第三种运算来实现。必须非常小心地来使这种转置运算和前向传播过程</p>

<p>相协调。转置运算返回的输出的大小取决于三个方面：零填充的策略、前向传播运</p>

<p>算的步幅以及前向传播的输出映射的大小。在一些情况下，不同大小的输入通过前</p>

<p>向传播过程能够得到相同大小的输出映射，所以必须明确地告知转置运算原始输入</p>

<p>的大小。</p>

<p>这三种运算——卷积、从输出到权重的反向传播和从输出到输入的反向传播 ——对于训练任意深度的前馈卷积网络，以及训练带有(基于卷积的转置的)重构 函数的卷积网络，这三种运算都足以计算它们所需的所有梯度。对于完全一般的多 维、多样例情况下的公式，完整的推导可以参考 Goodfellow (2010)。为了直观说明 这些公式是如何起作用的，我们这里给出一个二维单个样例的版本。</p>

<p>假设我们想要训练这样一个卷积网络，它包含步幅为 s 的步幅卷积，该卷积的</p>

<p>核为K，作用于多通道的图像V，定义为c(K, V,s)，就像式(9.8)中一样。假设我们 想要最小化某个损失函数J(V, K)。在前向传播过程中，我们需要用c本身来输出 Z，然后Z传递到网络的其余部分并且被用来计算损失函数J。在反向传播过程中， 我们会得到一个张量G满足Gi# = J(V, K)。</p>

<p>为了训练网络，我们需要对核中的权重求导。为了实现这个目的，我们可以使</p>

<p>用一个函数</p>

<p>g(G, V, s)i,j,k,l</p>

<p>d</p>

<p>dKi,j,k,l</p>

<p>J(V,K)= Gi,m,nVj</p>

<p>j,(m-1) xs + k,(n-1) xs+l -</p>

<p>(9.11)</p>

<p>如果这一层不是网络的底层，我们需要对V求梯度来使得误差进一步反向传播 我们可以使用如下的函数</p>

<p>h(K, G, s) i,j,k</p>

<p>J(V,K)</p>

<p>Kq,i,m,pGq,l,</p>

<p>(9.12)</p>

<p>(9.13)</p>

<p>l,m n,p</p>

<p>s.t. s.t.</p>

<p>(l-1)xs+m=j(n-1)xs+p=k</p>

<p>第十四章描述的自编码器网络，是一些被训练成把输入拷贝到输出的前馈网</p>

<p>络。一个简单的例子是PCA算法，将输人拷贝到一个近似的重构值r，通过函数 WTWx来实现。使用权重矩阵转置的乘法，就像PCA算法这种，在一般的自编码 器中是很常见的。为了使这些模型卷积化，我们可以用函数h来实现卷积运算的转 置。假定我们有和Z相同形式的隐藏单元H，并且我们定义一种重构运算</p>

<p>R=h(K,H,s).</p>

<p>(9.14)</p>

<p>为了训练自编码器，我们会得到关于R的梯度，表示为一个张量E。为了训练 解码器，我们需要获得对于 K 的梯度，这通过 g(H,E,s) 来得到。为了训练编码器 我们需要获得对于H的梯度，这通过c(K, E, s)来得到。通过用c和h对g求微分 也是可行的，但这些运算对于任何标准神经网络上的反向传播算法来说都是不需要 的。</p>

<p>一般来说，在卷积层从输人到输出的变换中我们不仅仅只用线性运算。我们一</p>

<p>般也会在进行非线性运算前，对每个输出加人一些偏置项。这样就产生了如何在偏</p>

<p>置项中共享参数的问题。对于局部连接层，很自然地对每个单元都给定它特有的偏</p>

<p>置，对于平铺卷积，也很自然地用与核一样的平铺模式来共享参数。对于卷积层来</p>

<p>说，通常的做法是在输出的每一个通道上都设置一个偏置，这个偏置在每个卷积映</p>

<p>射的所有位置上共享。然而，如果输人是已知的固定大小，也可以在输出映射的每个</p>

<p>位置学习一个单独的偏置。分离这些偏置可能会稍稍降低模型的统计效率，但同时</p>

<p>也允许模型来校正图像中不同位置的统计差异。例如，当使用隐含的零填充时，图</p>

<p>像边缘的探测单元接收到较少的输人，因此需要较大的偏置。</p>

<p>Seq/BUTPJOO0°</p>

<p>Spatial coordinates</p>

<p>图 9.15: 卷积网络的前两个输出通道只和前两个输人通道相连，随后的两个输出通道只和随后的 两个输人通道相连。</p>

<p>图 9.16: 局部连接层、平铺卷积和标准卷积的比较。当使用相同大小的核时，这三种方法在单元之 间具有相同的连接。此图是对使用两个像素宽的核的说明。这三种方法之间的区别在于它们如何 共享参数。（上） 局部连接层根本没有共享参数。我们对每个连接使用唯一的字母标记，来表明每</p>

<p>个连接都有它自身的权重。f中J平铺卷积有t个不同的核。这里我们说明t = 2的情况。其中一个 核具有标记为“a”和“b&rdquo;的边，而另一个具有标记为“c&rdquo;和“d&rdquo;的边。每当我们在输出中右移一</p>

<p>个像素后，我们使用一个不同的核。这意味着，与局部连接层类似，输出中的相邻单元具有不同的</p>

<p>参数。与局部连接层不同的是，在我们遍历所有可用的t个核之后，我们循环回到了第一个核。如 果两个输出单元间隔t个步长的倍数，则它们共享参数。f下」传统卷积等效于t = 1的平铺卷积。 它只有一个核，并且被应用到各个地方，我们在图中表示为在各处使用具有标记为 “a&rdquo; 和 “b&rdquo; 的 边的核</p>

<p>9.6 结构化输出
卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任务的类</p>

<p>标签或回归任务的实数值。通常这个对象只是一个张量，由标准卷积层产生。例如</p>

<p>模型可以产生张量S，其中Si,j,k是网络的输人像素(j,k)属于类i的概率。这允许</p>

<p>模型标记图像中的每个像素，并绘制沿着单个对象轮廓的精确掩模。</p>

<p>经常出现的一个问题是输出平面可能比输人平面要小，如图9.13所示。用于</p>

<p>对图像中单个对象分类的常用结构中，网络空间维数的最大减少来源于使用大步</p>

<p>幅的池化层。为了产生与输人大小相似的输出映射，我们可以避免把池化放在一起 (Jain et al., 2007)。另一种策略是单纯地产生一张低分辨率的标签网格 (Pinheiro and Collobert, 2014, 2015)。最后，原则上可以使用具有单位步幅的池化操作。</p>

<p>对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使用相邻</p>

<p>像素之间的交互来修正该原始猜测。重复这个修正步骤数次对应于在每一步使用相</p>

<p>同的卷积，该卷积在深层网络的最后几层之间共享权重(Jain et al., 2007)。这使得在 层之间共享参数的连续的卷积层所执行的一系列运算，形成了一种特殊的循环神经 网络 (Pinheiro and Collobert, 2014, 2015)。图9.17给出了这样一个循环卷积网络的</p>

<p>结构。</p>

<p>一旦对每个像素都进行了预测，我们就可以使用各种方法来进一步处理这些 预测，以便获得图像在区域上的分割 (Briggman et al., 2009; Turaga et al., 2010; Farabet et al., 2013)。一般的想法是假设大片相连的像素倾向于对应着相同的标签。 图模型可以描述相邻像素间的概率关系。或者，卷积网络可以被训练来最大化地近 似图模型的训练目标 (Ning et al., 2005; Thompson et al., 2014)。</p>

<p>图9.17:用于像素标记的循环卷积网络的示例。输人是图像张量X，它的轴对应图像的行、列和通 道(红，绿，蓝)。目标是输出标签张量Y＞，它遵循每个像素的标签的概率分布。该张量的轴对应 图像的行、列和不同类别。循环网络通过使用Y＞的先前估计作为创建新估计的输人，来迭代地改 善其估计，而不是单次输出Y＞，。每个更新的估计使用相同的参数，并且估计可以如我们所愿地被 改善任意多次。每一步使用的卷积核张量U，是用来计算给定输人图像的隐藏表示的。核张量V 用于产生给定隐藏值时标签的估计。除了第一步之外，核W都对Y＞进行卷积来提供隐藏层的输 人。在第一步中，此项由零代替。因为每一步使用相同的参数，所以这是一个循环网络的例子，如 第十章所述。</p>

<p>1</p>

<p>译者注：原文将K误写成了 k。</p>

<p>9.7 数据类型
卷积网络使用的数据通常包含多个通道，每个通道是时间上或空间中某一点的</p>

<p>不同观测量。参考表9.1来了解具有不同维数和通道数的数据类型的例子。</p>

<p>卷积网络用于视频的例子，可以参考Chen et al. (2010)。</p>

<p>到目前为止，我们仅讨论了训练和测试数据中的每个样例都有相同的空间维度</p>

<p>的情况。卷积网络的一个优点是它们还可以处理具有可变的空间尺度的输人。这些</p>

<p>类型的输人不能用传统的基于矩阵乘法的神经网络来表示。这为卷积网络的使用提</p>

<p>供了令人信服的理由，即使当计算开销和过拟合都不是主要问题时。</p>

<p>例如，考虑一组图像的集合，其中每个图像具有不同的高度和宽度。目前还不</p>

<p>清楚如何用固定大小的权重矩阵对这样的输人进行建模。卷积就可以很直接地应用</p>

<p>核依据输人的大小简单地被使用不同次，并且卷积运算的输出也相应地放缩。卷积</p>

<p>可以被视为矩阵乘法；相同的卷积核为每种大小的输人引人了一个不同大小的双重</p>

<p>分块循环矩阵。有时，网络的输出允许和输人一样具有可变的大小，例如如果我们</p>

<p>想要为输人的每个像素分配一个类标签。在这种情况下，不需要进一步的设计工作</p>

<p>在其他情况下，网络必须产生一些固定大小的输出，例如，如果我们想要为整个图</p>

<p>单通道</p>

<p>多通道</p>

<p>1维</p>

<p>音频波形：卷积的轴对应于时间。</p>

<p>我们将时间离散化并且在每个时</p>

<p>间点测量一次波形的振幅。</p>

<p>骨架动画 (skeleton animation) 数 据：计算机渲染的 3D 角色动画是 通过随时间调整 ‘‘骨架&rdquo; 的姿势 而生成的。在每个时间点，角色的 姿势通过骨架中的每个关节的角 度来描述。我们输人到卷积模型 的数据的每个通道，表示一个关 节关于一个轴的角度。</p>

<p>2维</p>

<p>已经使用傅立叶变换预处理过的</p>

<p>音频数据：我们可以将音频波形 变换成 2 维张量，不同的行对应 不同的频率，不同的列对应不同</p>

<p>的时间点。在时间轴上使用卷积 使模型等效于在时间上移动。在 频率轴上使用卷积使得模型等效 于在频率上移动， 这使得在不同 八度音阶中播放的相同旋律产生 相同的表示，但处于网络输出中 的不同高度。</p>

<p>彩色图像数据：其中一个通道包</p>

<p>含红色像素，另一个包含绿色像</p>

<p>素，最后一个包含蓝色像素。在图</p>

<p>像的水平轴和竖直轴上移动卷积</p>

<p>核，赋予了两个方向上平移等变</p>

<p>性。</p>

<p>3维</p>

<p>体积数据：这种数据一般来源于</p>

<p>医学成像技术，例如 CT 扫描等。</p>

<p>彩色视频数据：其中一个轴对应</p>

<p>着时间，另一个轴对应着视频帧</p>

<p>的高度， 最后一个对应着视频帧 的宽度。</p>

<p>表 9.1: 用于卷积网络的不同数据格式的示例</p>

<p>像指定单个类标签。在这种情况下，我们必须进行一些额外的设计步骤，例如插人</p>

<p>一个池化层，池化区域的大小要与输人的大小成比例，以便保持固定数量的池化输</p>

<p>出。这种策略的一些例子可以参考图9.11。</p>

<p>注意，使用卷积处理可变尺寸的输人，仅对输人是因为包含对同种事物的不同</p>

<p>量的观察(时间上不同长度的记录，空间上不同宽度的观察等) 而导致的尺寸变化这 种情况才有意义。如果输人是因为它可以选择性地包括不同种类的观察而具有可变 尺寸，使用卷积是不合理的。例如，如果我们正在处理大学申请，并且我们的特征 包括成绩等级和标准化测试分数，但不是每个申请人都进行了标准化测试，则使用 相同的权重来对成绩特征和测试分数特征进行卷积是没有意义的。</p>

<p>9.8 高效的卷积算法
现代卷积网络的应用通常需要包含超过百万个单元的网络。利用并行计算资源</p>

<p>的强大实现是很关键的，如第12.1节中所描述的。然而，在很多情况下，也可以通</p>

<p>过选择适当的卷积算法来加速卷积。</p>

<p>卷积等效于使用傅立叶变换将输人与核都转换到频域、执行两个信号的逐点相</p>

<p>乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散</p>

<p>卷积的朴素实现更快。</p>

<p>当一个 d 维的核可以表示成 d 个向量(每一维一个向量)的外积时，该核被称</p>

<p>为可分离的(separable )。当核可分离时，朴素的卷积是低效的。它等价于组合d个 一维卷积，每个卷积使用这些向量中的一个。组合方法显著快于使用它们的外积来</p>

<p>执行一个 d 维的卷积。并且核也只要更少的参数来表示成向量。如果核在每一维都</p>

<p>是 w 个元素宽，那么朴素的多维卷积需要 O(wd) 的运行时间和参数存储空间，而可 分离卷积只需要O(w x d)的运行时间和参数存储空间。当然，并不是每个卷积都可</p>

<p>以表示成这种形式。</p>

<p>设计更快的执行卷积或近似卷积，而不损害模型准确性的方法，是一个活跃的</p>

<p>研究领域。甚至仅提高前向传播效率的技术也是有用的，因为在商业环境中，通常</p>

<p>部署网络比训练网络还要耗资源。</p>

<p>9.9 随机或无监督的特征
通常，卷积网络训练中最昂贵的部分是学习特征。输出层的计算代价通常相对</p>

<p>不高，因为在通过若干层池化之后作为该层输人的特征的数量较少。当使用梯度下</p>

<p>降执行监督训练时，每步梯度计算需要完整地运行整个网络的前向传播和反向传播</p>

<p>减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。</p>

<p>有三种基本策略可以不通过监督训练而得到卷积核。其中一种是简单地随机初 始化它们。另一种是手动设计它们，例如设置每个核在一个特定的方向或尺度来检 测边缘。最后，可以使用无监督的标准来学习核。例如，Coates et al. (2011)将k均 值聚类算法应用于小图像块，然后使用每个学得的中心作为卷积核。第三部分描述 了更多的无监督学习方法。使用无监督的标准来学习特征，允许这些特征的确定与 位于网络结构顶层的分类层相分离。然后只需提取一次全部训练集的特征，构造用 于最后一层的新训练集。假设最后一层类似逻辑回归或者SVM，那么学习最后一层 通常是凸优化问题。</p>

<p>随机过滤器经常在卷积网络中表现得出乎意料得好 Jarrett et al. (2009b); Saxe et al. (2011); Pinto et al. (2011); Cox and Pinto (2011)。 Saxe et al. (2011) 说明，由</p>

<p>卷积和随后的池化组成的层，当赋予随机权重时，自然地变得具有频率选择性和平</p>

<p>移不变性。他们认为这提供了一种廉价的方法来选择卷积网络的结构：首先通过仅</p>

<p>训练最后一层来评估几个卷积网络结构的性能，然后选择最好的结构并使用更昂贵</p>

<p>的方法来训练整个网络。</p>

<p>一个中间方法是学习特征，但是使用那种不需要在每个梯度计算步骤中都进行</p>

<p>完整的前向和反向传播的方法。与多层感知机一样，我们使用贪心逐层预训练，单</p>

<p>独训练第一层，然后一次性地从第一层提取所有特征，之后用那些特征单独训练</p>

<p>第二层，以此类推。第八章描述了如何实现监督的贪心逐层预训练，第三部分将此</p>

<p>扩展到了无监督的范畴。卷积模型的贪心逐层预训练的经典模型是卷积深度信念网</p>

<p>络(Lee et al., 2009)。卷积网络为我们提供了相对于多层感知机更进一步采用预训 练策略的机会。并非一次训练整个卷积层，我们可以训练一小块模型，就像 Coates et al. (2011) 使用 k 均值做的那样。然后，我们可以用来自这个小块模型的参数来定 义卷积层的核。这意味着使用无监督学习来训练卷积网络并且在训练的过程中完全</p>

<p>不使用卷积是可能的。使用这种方法，我们可以训练非常大的模型，并且只在推断期</p>

<p>间产生高计算成本 (Ranzato et al., 2007c; Jarrett et al., 2009b; Kavukcuoglu et al., 2010; Coates et al., 2013)。这种方法大约在 2007 到 2013 年间流行，当时标记的数</p>

<p>据集很小，并且计算能力有限。如今，大多数卷积网络以纯粹监督的方式训练，在每</p>

<p>次训练迭代中使用通过整个网络的完整的前向和反向传播。</p>

<p>与其他无监督预训练的方法一样，使用这种方法的一些好处仍然难以说清。无</p>

<p>监督预训练可以提供一些相对于监督训练的正则化，或者它可以简单地允许我们训</p>

<p>练更大的结构，因为它的学习规则降低了计算成本。</p>

<p>9.10 卷积网络的神经科学基础
卷积网络也许是生物学启发人工智能的最为成功的案例。虽然卷积网络也经过</p>

<p>许多其他领域的指导，但是神经网络的一些关键设计原则来自于神经科学。</p>

<p>卷积网络的历史始于神经科学实验，远早于相关计算模型的发展。为了确定关 于哺乳动物视觉系统如何工作的许多最基本的事实，神经生理学家 David Hubel 和 Torsten Wiesel 合作多年 (Hubel and Wiesel, 1959, 1962, 1968)。他们的成就最终获 得了诺贝尔奖。他们的发现对当代深度学习模型有最大影响的是基于记录猫的单个 神经元的活动。他们观察了猫的脑内神经元如何响应投影在猫前面屏幕上精确位置 的图像。他们的伟大发现是，处于视觉系统较为前面的神经元对非常特定的光模式 (例如精确定向的条纹)反应最强烈，但对其他模式几乎完全没有反应。</p>

<p>他们的工作有助于表征大脑功能的许多方面，这些方面超出了本书的范围。从</p>

<p>深度学习的角度来看，我们可以专注于简化的、草图形式的大脑功能视图。</p>

<p>在这个简化的视图中，我们关注被称为 V1 的大脑的一部分，也称为初级视觉</p>

<p>皮层(primary visual cortex)。V1是大脑对视觉输人开始执行显著高级处理的第一</p>

<p>个区域。在该草图视图中，图像是由光到达眼睛并刺激视网膜(眼睛后部的光敏组</p>

<p>织)形成的。视网膜中的神经元对图像执行一些简单的预处理，但是基本不改变它</p>

<p>被表示的方式。然后图像通过视神经和称为外侧膝状核的脑部区域。这些解剖区域 的主要作用是仅仅将信号从眼睛传递到位于头后部的 V1。</p>

<p>卷积网络层被设计为描述 V1 的三个性质：</p>

<ol>
<li>V1 可以进行空间映射。它实际上具有二维结构来反映视网膜中的图像结构。例</li>
</ol>

<p>如，到达视网膜下半部的光仅影响V1相应的一半。卷积网络通过用二维映射</p>

<p>定义特征的方式来描述该特性。</p>

<ol>
<li>V1包含许多简单细胞(simple cell)。简单细胞的活动在某种程度上可以概括</li>
</ol>

<p>为在一个小的空间位置感受野内的图像的线性函数。卷积网络的检测器单元被</p>

<p>设计为模拟简单细胞的这些性质。</p>

<ol>
<li>V1还包括许多复杂细胞(complex cell)。这些细胞响应类似于由简单细胞检</li>
</ol>

<p>测的那些特征，但是复杂细胞对于特征的位置微小偏移具有不变性。这启发</p>

<p>了卷积网络的池化单元。复杂细胞对于照明中的一些变化也是不变的，不能简</p>

<p>单地通过在空间位置上池化来刻画。这些不变性激发了卷积网络中的一些跨通</p>

<p>道池化策略，例如 maxout 单元 (Goodfellow et al., 2013b)。</p>

<p>虽然我们最了解VI,但是一般认为相同的基本原理也适用于视觉系统的其他区</p>

<p>域。在我们视觉系统的草图视图中，当我们逐渐深人大脑时，遵循池化的基本探测</p>

<p>策略被反复执行。当我们穿过大脑的多个解剖层时，我们最终找到了响应一些特定</p>

<p>概念的细胞，并且这些细胞对输人的很多种变换都具有不变性。这些细胞被昵称为</p>

<p>‘‘祖母细胞&rdquo;——这个想法是一个人可能有一个神经元，当看到他祖母的照片时该神</p>

<p>经元被激活，无论祖母是出现在照片的左边或右边，无论照片是她的脸部的特写镜</p>

<p>头还是她的全身照，也无论她处在光亮还是黑暗中，等等。</p>

<p>这些祖母细胞已经被证明确实存在于人脑中，在一个被称为内侧颞叶的区域</p>

<p>(Quiroga et al., 2005)。研究人员测试了单个神经元是否会响应名人的照片。他们发 现了后来被称为 “Halle Berry 神经元&rdquo; 的神经元：由 Halle Berry 的概念激活的单 个神经元。当一个人看到 Halle Berry 的照片， Halle Berry 的图画，甚至包含单词 “Halle Berry&rdquo;的文本时，这个神经元会触发。当然，这与Halle Berry本人无关；其 他神经元会对 Bill Clinton， Jennifer Aniston 等的出现做出响应。</p>

<p>这些内侧颞叶神经元比现代卷积网络更通用一些，这些网络在读取名称时不会 自动联想到识别人或对象。与卷积网络的最后一层在特征上最接近的类比是称为颞 下皮质(IT)的脑区。当查看一个对象时，信息从视网膜经LGN流到VI,然后到 V2, V4,之后是IT。这发生在瞥见对象的前100ms内。如果允许一个人继续观察对 象更多的时间，那么信息将开始回流，因为大脑使用自上而下的反馈来更新较低级 脑区中的激活。然而，如果我们打断人的注视，并且只观察前100ms内的大多数前 向激活导致的放电率，那么IT被证明与卷积网络非常相似。卷积网络可以预测IT 放电率，并且在执行对象识别任务时与人类(时间有限的情况)非常类似 (DiCarlo, 2013)。</p>

<p>话虽如此，卷积网络和哺乳动物的视觉系统之间还是有许多区别。这些区别有</p>

<p>一些是计算神经科学家所熟知的，但超出了本书的范围。还有一些区别尚未知晓，因 为关于哺乳动物视觉系统如何工作的许多基本问题仍未得到回答。简要列表如下：</p>

<p>•人眼大部分是非常低的分辨率，除了一个被称为中央凹(fovea)的小块。中</p>

<p>央凹仅观察在手臂长度距离内一块拇指大小的区域。虽然我们觉得我们可以看</p>

<p>到高分辨率的整个场景，但这是由我们的大脑的潜意识部分创建的错觉，因为</p>

<p>它缝合了我们瞥见的若干个小区域。大多数卷积网络实际上接收大的全分辨率</p>

<p>的照片作为输人。人类大脑控制几次眼动，称为扫视(saccade)，以瞥见场景 中最显眼的或任务相关的部分。将类似的注意力机制融人深度学习模型是一</p>

<p>个活跃的研究方向。在深度学习的背景下，注意力机制对于自然语言处理是最</p>

<p>成功的，参考第12.4.5.1节。研究者已经研发了几种具有视觉机制的视觉模型</p>

<p>但到目前为止还没有成为主导方法 (Larochelle and Hinton, 2010; Denil et al.,</p>

<p>2012)。</p>

<p>• 人类视觉系统集成了许多其他感觉，例如听觉，以及像我们的心情和想法一样 的因素。卷积网络迄今为止纯粹是视觉的。</p>

<p>• 人类视觉系统不仅仅用于识别对象。它能够理解整个场景，包括许多对象和对 象之间的关系，以及处理我们的身体与世界交互所需的丰富的三维几何信息。</p>

<p>卷积网络已经应用于这些问题中的一些，但是这些应用还处于起步阶段。</p>

<p>• 即使像 V1 这样简单的大脑区域也受到来自较高级别的反馈的严重影响。反馈 已经在神经网络模型中被广泛地探索，但还没有被证明提供了引人注目的改进。</p>

<p>• 虽然前馈 IT 放电频率刻画了与卷积网络特征很多相同的信息，但是仍不清楚</p>

<p>中间计算的相似程度。大脑可能使用非常不同的激活和池化函数。单个神经元</p>

<p>的激活可能不能用单个线性过滤器的响应来很好地表征。最近的 V1 模型涉及</p>

<p>对每个神经元的多个二次过滤器 (Rust et al., 2005)。事实上，我们的 ‘‘简单细</p>

<p>胞&rdquo; 和 ‘‘复杂细胞&rdquo; 的草图图片可能并没有区别；简单细胞和复杂细胞可能是</p>

<p>相同种类的细胞，但是它们的‘‘参数&rdquo;使得它们能够实现从我们所说的‘‘简单&rdquo;</p>

<p>到 ‘‘复杂&rdquo; 的连续的行为。</p>

<p>还值得一提的是，神经科学很少告诉我们该如何训练卷积网络。具有跨多个空 间位置的参数共享的模型结构，可以追溯到早期关于视觉的联结主义模型 (Marr and Poggio, 1976)，但是这些模型没有使用现代的反向传播算法和梯度下降。例如</p>

<p>(Fukushima, 1980) 结合了现代卷积网络的大多数模型结构设计元素，但依赖于层次 化的无监督聚类算法。</p>

<p>Lang and Hinton (1988) 引人反向传播来训练 时延神经网络( time delay neural network, TDNN )。使用当代术语来说，TDNN是用于时间序列的一维卷积网络。用</p>

<p>于这些模型的反向传播不受任何神经科学观察的启发，并且被一些人认为是生物不 可信的。在基于使用反向传播训练的 TDNN 成功之后， LeCun et al. (1989) 通过将 相同的训练算法应用于图像的 2 维卷积来发展现代卷积网络。</p>

<p>到目前为止，我们已经描述了简单细胞对于某些特征是如何呈现粗略的线性和</p>

<p>选择性，复杂细胞是如何更加的非线性，并且对于这些简单细胞特征的某些变换具</p>

<p>有不变性，以及在选择性和不变性之间交替放置的层可以产生对非常特定现象的祖</p>

<p>母细胞。我们还没有精确描述这些单个细胞检测到了什么。在深度非线性网络中</p>

<p>可能难以理解单个细胞的功能。第一层中的简单细胞相对更容易分析，因为它们的</p>

<p>响应由线性函数驱动。在人工神经网络中，我们可以直接显示卷积核的图像，来查 看卷积层的相应通道是如何响应的。在生物神经网络中，我们不能访问权重本身。 相反，我们在神经元自身中放置一个电极，在动物视网膜前显示几个白噪声图像样 本，并记录这些样本中的每一个是如何导致神经元激活的。然后，我们可以对这些 响应拟合线性模型，以获得近似的神经元权重。这种方法被称为反向相关( reverse correlation)(Ringach and Shapley, 2004)。</p>

<p>反向相关向我们表明，大多数的 V1 细胞具有由 Gabor 函数( Gabor function) 所描述的权重。 Gabor 函数描述在图像中的 2 维点处的权重。我们可以认为图像是 2 维坐标 I(x,y) 的函数。类似地，我们可以认为简单细胞是在图像中的一组位置采</p>

<p>样，这组位置由一组x坐标X和一组y坐标Y来定义，并且使用的权重w(x,y)也 是位置的函数。从这个观点来看，简单细胞对于图像的响应由下式给出</p>

<p>s(I) = w(x, y)I(x, y). (9.15)</p>

<p>xex yeY</p>

<p>特别地，w(x,y)采用Gabor函数的形式：</p>

<p>w(x,y; a,冷x,卢y, f,沴，xo, yo,T) = a exp(_ArX’2 -卢&rdquo;y’2) cos(fx’ + 利， (9.16)</p>

<p>其中</p>

<p>x’ = (x - xo) cos(T) + (y - yo) sin(T) (9.17)</p>

<p>以及</p>

<p>y’ = -(x - xo) sin(T) + (y - yo) cos(t). (9.18)</p>

<p>这里a,Ax,凡fj&rsquo;^y^T都是控制Gabor函数性质的参数。图9.18给出 了Gabor函数在不同参数集上的一些例子。</p>

<p>DDDDDD11 •• □□□■■■■■ aa^ nDDnnD]□□□■■■■■</p>

<p>nnnminiD]» &gt;• □□□[!■■■■ a h n DDDDonm m □□□□ » • * •</p>

<p>h v \isQi// it ii DDDininm in □□□[] • • • • nnnnnmirii DDan i（ » «</p>

<p>DDnannm ii ii nnaimi（ &gt; i nnniii] 11 Haas nnn urno i（</p>

<p>图 9.18: 具有各种参数设置的 Gabor 函数。白色表示绝对值大的正权重，黑色表示绝对值大的负 权重，背景灰色对应于零权重。 （左） 控制坐标系的参数具有不同值的 Gabor 函数，这些参数包括</p>

<p>X0、yo和Y。在该网格中的每个Gabor函数被赋予和它在网格中的位置成比例的x。和yo的值， 并且t被选择为使得每个Gabor过滤器对从网格中心辐射出的方向非常敏感。对于其他两幅图, x。、y。和y固定为零。f中J具有不同高斯比例参数氏和氏的Gabor函数。当我们从左到右通 过网格时，Gabor函数被设置为增加宽度（减少氏）；当我们从上到下通过网格时，Gabor函数被 设置为为增加高度（减少凡）。对于其他两幅图，/3值固定为图像宽度的1.5倍。f右J具有不同的 正弦参数f和冷的Gabor函数。当我们从上到下移动时，f增加；当我们从左到右移动时，冷增 加。对于其他两幅图，冷固定为0, f固定为图像宽度的5倍。</p>

<p>参数xo, yo和t定义坐标系。我们平移和旋转x和y来得到xz和yz。具体地， 简单细胞会响应以点（xo,yo）为中心的图像特征，并且当我们沿着从水平方向旋转t 弧度的线移动时，简单细胞将响应亮度的变化。</p>

<p>作为xz和yz的函数，函数w会响应当我们沿着xz移动时的亮度变化。它有两 个重要的因子：一个是高斯函数，另一个是余弦函数。</p>

<p>高斯因子aexp（-dxx/2 - ^yy/2）可以被视为阈值项，用于保证简单细胞仅对接 近工&rsquo;和^都为零点处的值响应，换句话说，接近细胞接受域的中心。尺度因子a 调整简单细胞响应的总的量级，而礼和氏控制接受域消退的速度。</p>

<p>余弦因子cos（fxz + 0）控制简单细胞如何响应延x‘轴的亮度改变。参数f控制 余弦的频率，4控制它的相位偏移。</p>

<p>合在一起，简单细胞的这个草图视图意味着，简单细胞对在特定位置处、特定</p>

<p>方向上、特定空间频率的亮度进行响应。当图像中的光波与细胞的权重具有相同的</p>

<p>相位时，简单细胞是最兴奋的。这种情况发生在当图像亮时，它的权重为正，而图</p>

<p>像暗时，它的权重为负。当光波与权重完全异相时，简单细胞被抑制——当图像较</p>

<p>暗时，它的权重为正；较亮时，它的权重为负。</p>

<p>复杂细胞的草图视图是它计算包含两个简单细胞响应的2维向量的L2范数: c(I) = /so(I)2 + si(I)2。一个重要的特殊情况是当si和so具有除4以外都相同的 参数，并且被设置为使得si与so相位相差四分之一周期时。在这种情况下，so和 si形成象限对(quadrature pair )。当高斯重新加权的图像I(x，y) exp(-^x72-冷yy2) 包含具有频率f、在方向t上、接近(xo,yo)的高振幅正弦波时，用先前方法定义的 复杂细胞会响应，并且不管该波的相位偏移。换句话说，复杂细胞对于图像在方向T 上的微小变换或者翻转图像(用白色代替黑色，反之亦然)具有不变性。</p>

<p>神经科学和机器学习之间最显著的对应关系，是从视觉上比较机器学习模型学 得的特征与使用 V1 得到的特征。 Olshausen and Field (1996) 说明，一个简单的无 监督学习算法，稀疏编码，学习的特征具有与简单细胞类似的感受野。从那时起，我 们发现，当应用于自然图像时，极其多样的统计学习算法学习类Gabor函数的特征。 这包括大多数深度学习算法，它们在其第一层中学习这些特征。图9.19给出了一些 例子。因为如此众多不同的学习算法学习边缘检测器，所以很难仅基于学习算法学 得的特征，来断定哪一个特定的学习算法是‘‘正确&rdquo;的大脑模型(虽然，当应用于自 然图像时，如果一个算法不能学得某种检测器时，它能够作为一种否定标志)。这些 特征是自然图像的统计结构的重要部分，并且可以通过许多不同的统计建模方法来 重新获得。读者可以参考(Hyvarinen et al., 2009)来获得自然图像统计领域的综述。</p>

<p>图 9.19: 许多机器学习算法在应用于自然图像时，会学习那些用来检测边缘或边缘的特定颜色的特 征。这些特征检测器使人联想到已知存在于初级视觉皮层中的 Gabor 函数。 (左) 通过应用于小图</p>

<p>像块的无监督学习算法(尖峰和平板稀疏编码)学得的权重。f右」由完全监督的卷积maxout网</p>

<p>络的第一层学得的卷积核。相邻的一对过滤器驱动相同的 maxout 单元。</p>

<p>9.11 卷积网络与深度学习的历史
卷积网络在深度学习的历史中发挥了重要作用。它们是将研究大脑获得的深刻</p>

<p>理解成功用于机器学习应用的关键例子。它们也是首批表现良好的深度模型之一</p>

<p>远远早于任意深度模型被认为是可行之前。卷积网络也是第一个解决重要商业应用</p>

<p>的神经网络，并且仍然处于当今深度学习商业应用的前沿。例如，在20世纪90年 代，AT&amp;T的神经网络研究小组开发了一个用于读取支票的卷积网络(LeCun d &lt; 1998c)。到90年代末，NEC部署的这个系统已经被用于读取美国10%以上的支 票。后来，微软部署了若干个基于卷积网络的OCR和手写识别系统(Simard ef 2003)。关于卷积网络的这种应用和更现代应用的更多细节，参考第十二章。读者可 以参考(LeCun et al., 2010) 了解2010年之前的更为深人的卷积网络历史。</p>

<p>卷积网络也被用作在许多比赛中的取胜手段。当前对深度学习的商业兴趣的热 度始于Krizhevsky et al. (2012a)赢得了 ImageNet对象识别挑战，但是在那之前， 卷积网络也已经被用于赢得前些年影响较小的其他机器学习和计算机视觉竞赛了。</p>

<p>卷积网络是第一批能使用反向传播有效训练的深度网络之一。现在仍不完全清</p>

<p>楚为什么卷积网络在一般的反向传播网络被认为已经失败时反而成功了。这可能可</p>

<p>以简单地归结为卷积网络比全连接网络计算效率更高，因此使用它们运行多个实验 并调整它们的实现和超参数更容易。更大的网络也似乎更容易训练。利用现代硬件，</p>

<p>大型全连接的网络在许多任务上也表现得很合理，即使使用过去那些全连接网络被</p>

<p>认为不能工作得很好的数据集和当时流行的激活函数时，现在也能执行得很好。心</p>

<p>理可能是神经网络成功的主要阻碍（实践者没有期望神经网络有效，所以他们没有</p>

<p>认真努力地使用神经网络）。无论如何，幸运的是卷积网络在几十年前就表现良好。</p>

<p>在许多方面，它们为余下的深度学习传递火炬，并为一般的神经网络被接受铺平了</p>

<p>道路。</p>

<p>卷积网络提供了一种方法来特化神经网络，使其能够处理具有清楚的网格结构</p>

<p>拓扑的数据，以及将这样的模型扩展到非常大的规模。这种方法在二维图像拓扑上</p>

<p>是最成功的。为了处理一维序列数据，我们接下来转向神经网络框架的另一种强大</p>

<p>的特化：循环神经网络。</p>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18-%E7%9B%B4%E9%9D%A2%E9%85%8D%E5%88%86%E5%87%BD%E6%95%B0/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">18 直面配分函数</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%8B%E7%BC%A9/">
            <span class="next-text nav-default">DL 卷积神经网络的压缩</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
