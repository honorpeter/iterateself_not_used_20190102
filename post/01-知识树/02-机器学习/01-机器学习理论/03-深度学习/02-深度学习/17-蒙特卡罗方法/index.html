<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>17 蒙特卡罗方法 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="TODO aaa [TOC] 第十七章 蒙特卡罗方法 随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。Las Vegas算 法总是精确地返回一个正确答案（或" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="17 蒙特卡罗方法" />
<meta property="og:description" content="TODO aaa [TOC] 第十七章 蒙特卡罗方法 随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。Las Vegas算 法总是精确地返回一个正确答案（或" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/" /><meta property="article:published_time" content="2018-06-12T22:22:19&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-12T22:22:19&#43;00:00"/>
<meta itemprop="name" content="17 蒙特卡罗方法">
<meta itemprop="description" content="TODO aaa [TOC] 第十七章 蒙特卡罗方法 随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。Las Vegas算 法总是精确地返回一个正确答案（或">


<meta itemprop="datePublished" content="2018-06-12T22:22:19&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-12T22:22:19&#43;00:00" />
<meta itemprop="wordCount" content="10381">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="17 蒙特卡罗方法"/>
<meta name="twitter:description" content="TODO aaa [TOC] 第十七章 蒙特卡罗方法 随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。Las Vegas算 法总是精确地返回一个正确答案（或"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">17 蒙特卡罗方法</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-12 </span>
        
        <span class="more-meta"> 10381 words </span>
        <span class="more-meta"> 21 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#ref">REF</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>TODO</p>

<ul>
<li>aaa</li>
</ul>

<hr />

<p>[TOC]</p>

<p>第十七章 蒙特卡罗方法
随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。Las Vegas算 法总是精确地返回一个正确答案（或者返回算法失败了）。这类方法通常需要占用随 机量的计算资源（一般指内存或运行时间）。与此相对的，蒙特卡罗方法返回的答案</p>

<p>具有随机大小的错误。花费更多的计算资源（通常包括内存和运行时间）可以减少</p>

<p>这种错误。在任意固定的计算资源下，蒙特卡罗算法可以得到一个近似解。</p>

<p>对于机器学习中的许多问题来说，我们很难得到精确的答案。这类问题很难用 精确的确定性算法如Las Vegas算法解决。取而代之的是确定性的近似算法或蒙特卡 罗近似方法。这两种方法在机器学习中都非常普遍。本章主要关注蒙特卡罗方法。</p>

<p>17.1 采样和蒙特卡罗方法
机器学习中的许多重要工具都基于从某种分布中采样以及用这些样本对目标量</p>

<p>做一个蒙特卡罗估计。</p>

<p>17.1.1 为什么需要采样？
有许多原因使我们希望从某个分布中采样。当我们需要以较小的代价近似许多</p>

<p>项的和或某个积分时，采样是一种很灵活的选择。有时候，我们使用它加速一些很</p>

<p>费时却易于处理的求和估计，就像我们使用小批量对整个训练代价进行子采样一样。</p>

<p>在其他情况下，我们需要近似一个难以处理的求和或积分，例如估计一个无向模</p>

<p>型中配分函数对数的梯度时。在许多其他情况下，抽样实际上是我们的目标，例如</p>

<p>我们想训练一个可以从训练分布采样的模型。</p>

<p>502</p>

<p>17.1.2 蒙特卡罗采样的基础
当无法精确计算和或积分(例如，和具有指数数量个项，且无法被精确简化)</p>

<p>时，通常可以使用蒙特卡罗采样来近似它。这种想法把和或者积分视作某分布下的</p>

<p>期望，然后通过估计对应的平均值来近似这个期望。令</p>

<p>s = p(x)f(x) = Ep<a href="17-1">f(x)</a></p>

<p>或者</p>

<p>s = p(x)f(x)dx = Ep<a href="17.2">f(x)</a></p>

<p>为我们所需要估计的和或者积分，写成期望的形式，p是一个关于随机变量x的概</p>

<p>率分布(求和时)或者概率密度函数(求积分时)。</p>

<p>我们可以通过从p中抽取n个样本x(1),&hellip;, x(n)来近似s并得到一个经验平均</p>

<p>值</p>

<p>1n</p>

<p>Sn = nUf (x(i)).    (17.3)</p>

<p>i=1</p>

<p>下面几个性质表明了这种近似的合理性。首先很容易观察到S这个估计是无偏的， 由于</p>

<p>E[Sn]</p>

<p>E[f(x(i))]</p>

<p>i=1</p>

<p>=一〉S = S. n</p>

<p>i=1</p>

<p>(17.4)</p>

<p>此外，根据大数定理(Law of large number )，如果样本x(i)是独立同分布的，那么</p>

<p>其平均值几乎必然收敛到期望值，即</p>

<p>lim Sn = s,    (17.5)</p>

<p>n—</p>

<p>只需要满足各个单项的方差Var[f(x(i))]有界。详细地说，我们考虑当n增大时Sn 的方差。只要满足Var[f(x(i))] ＜⑻，方差Var[Sn]就会减小并收敛到0:</p>

<p>1n</p>

<p>Var[Sn] = ^ V&rdquo; Var<a href="17.6">f (x)</a></p>

<p>n2</p>

<p>i=1</p>

<p>=Var[f (x)]</p>

<p>(17.7)</p>

<p>这个简单有用的结果启迪我们如何估计蒙特卡罗均值中的不确定性，或者等价地说 是蒙特卡罗估计的期望误差。我们计算了的经验均值和方差、然后将估计的 方差除以样本数n来得到Var[Sn]的估计。中心极限定理(central limit theorem ) 告诉我们Sn的分布收敛到以s为均值以Var[n^为方差的正态分布。这使得我们可 以利用正态分布的累积函数来估计 sn 的置信区间。</p>

<p>以上的所有结论都依赖于我们可以从基准分布 p(x) 中轻易地采样，但是这个 假设并不是一直成立的。当我们无法从 p 中采样时，一个备选方案是用第17.2节讲 到的重要采样。一种更加通用的方式是构建一个收敛到目标分布的估计序列。这就 是马尔可夫链蒙特卡罗方法(见第17.3节)。</p>

<p>17.2 重要采样
如方程 (17.2)所示，在蒙特卡罗方法中，对积分(或者和)分解，确定积分中哪 一部分作为概率分布p^以及哪一部分作为被积的函数f» (我们感兴趣的是估 计 f(x) 在概率分布 p(x) 下的期望)是很关键的一步。 p(x)f(x) 不存在唯一的分解 因为它总是可以被写成</p>

<p>p(x)f(x) = q(x)</p>

<p>P(aQf(aQ</p>

<p>q(x)</p>

<p>(17.8)</p>

<p>在这里，我们从q分布中采样，然后估计#在此分布下的均值。许多情况中，我们 希望在给定 p 和 f 的情况下计算某个期望，这个问题既然是求期望，那么很自然地 p和f是一种分解选择。然而，如果考虑达到某给定精度所需要的样本数量，这个 问题最初的分解选择不是最优的选择。幸运的是，最优的选择q&rsquo;可以被简单地推导 出来。这种最优的采样函数q*对应所谓的最优重要采样。</p>

<p>从式(17.8)所示的关系中可以发现，任意蒙特卡罗估计</p>

<p>1n</p>

<p>(17.9)</p>

<p>sp = - T, f(x(i))</p>

<p>i=1,x(1&rsquo;^</p>

<p>可以被转化为一个重要采样的估计</p>

<p>E
= 1,x(i)〜q</p>

<p>p(x(i))f (x(i)) q(x(i))-</p>

<p>(17.10)</p>

<p>我们可以容易地发现估计的期望与q分布无关:</p>

<p>Eq[Sq] = Ep[Sp] = S.</p>

<p>(17.11)</p>

<p>(17.12)</p>

<p>然而，重要采样的方差可能对q的选择非常敏感。这个方差可以表示为</p>

<p>Var[Sq] = Var [p(xf(x)] /n.</p>

<p>q(x)</p>

<p>方差想要取到最小值， q 需要满足</p>

<p>q*(x) =    (17.13)</p>

<p>在这里Z表示归一化常数，选择适当的Z使得q*(x)之和或者积分为1。一个更好 的重要采样分布会把更多的权重放在被积函数较大的地方。事实上，当 f(x) 的正负 符号不变时，Var[Sq»] = 0，这意味着当使用最优的q分布时，只需要一个样本就足 够了。当然，这仅仅是因为计算q*时已经解决了原问题。所以在实践中这种只需要 采样一个样本的方法往往是无法实现的。</p>

<p>对于重要采样来说任意 q 分布都是可行的(从得到一个期望上正确的值的角度 来说)，q*指的是最优的q分布(从得到最小方差的角度上考虑)。从q*中采样往 往是不可行的，但是其他仍然能降低方差的 q 的选择还是可行的。</p>

<p>另一种方法是采用有偏重要采样(biased importance sampling )，这种方法有 一个优势，即不需要归一化的p或q分布。在处理离散变量时，有偏重要采样估计 可以表示为</p>

<p>.=E</p>

<p>^BIS =—</p>

<p>n</p>

<p>i=1</p>

<p>En p(x(i)) i=1 q(x(i)) n</p>

<p>i=1</p>

<p>En p(x(i)) i=1 q(x(i))</p>

<p>n=i §gf(x(i))</p>

<p>En p(x(i)) i=1 q(x(i))</p>

<p>(17.14)</p>

<p>(17.15)</p>

<p>(17.16)</p>

<p>其中 p 和 q 分别是分布 p 和 q 的未经归一化的形式， x(i) 是从分布 q 中抽取的样本。 这种估计是有偏的，因为E[sbis] = S，只有当n    且方程式(17.14)的分母收敛</p>

<p>到 1 时，等式才渐近地成立。所以这一估计也被称为渐近无偏的。</p>

<p>一个好的 q 分布的选择可以显著地提高蒙特卡罗估计的效率，而一个糟糕的 q 分布选择则会使效率更糟糕。我们回过头来看看方程 式(17.12)会发现，如果存在一</p>

<p>个q使得p(fx)很大，那么这个估计的方差也会很大。当q(x)很小，而f (x)和p(x) 都较大并且无法抵消q时，这种情况会非常明显。q分布经常会取一些简单常用的分 布使得我们能够从q分布中容易地采样。当X是高维数据时，q分布的简单性使得它 很难与p或者p|f |相匹配。当q(X⑴)》p(X⑴)|f (X⑴)|时，重要采样采到了很多无 用的样本(很小的数或零相加)。另一种相对少见的情况是q(X⑷)《p(X⑴)|f(X⑴)|， 相应的比值会非常大。正因为后一个事件是很少发生的，这种样本很难被采到，通 常使得对 s 的估计出现了典型的欠估计，很难被整体的过估计抵消。这样的不均匀 情况在高维数据屡见不鲜，因为在高维度分布中联合分布的动态域可能非常大。</p>

<p>尽管存在上述的风险，但是重要采样及其变种在机器学习的应用中仍然扮演着</p>

<p>重要的角色，包括深度学习算法。例如，重要采样被应用于加速训练具有大规模词</p>

<p>表的神经网络语言模型的过程中(见第12.4.3.3节)或者其他有着大量输出结点的神</p>

<p>经网络中。此外，还可以看到重要采样应用于估计配分函数(一个概率分布的归一 化常数)，详见第18.7节，以及在深度有向图模型比如变分自编码器中估计对数似然 (详见第20.10.3节)。采用随机梯度下降训练模型参数时重要采样可以用来改进对代 价函数梯度的估计，尤其是分类器这样的模型，其中代价函数的大部分代价来自于 少量错误分类的样本。在这种情况下，更加频繁地抽取这些困难的样本可以减小梯 度估计的方差 (Hinton et al., 2006a)。</p>

<p>17.3 马尔可夫链蒙特卡罗方法
在许多实例中，我们希望采用蒙特卡罗方法，然而往往又不存在一种简单的方法 可以直接从目标分布 pmodel(x) 中精确采样或者一个好的(方差较小的)重要采样分 布q(X)。在深度学习中，当分布Pmodel(X)表示成无向模型时，这种情况往往会发生。 在这种情况下，为了从分布 pmodel(x) 中近似采样，我们引入了一种称为马尔可夫 链(Markov Chain)的数学工具。利用马尔可夫链来进行蒙特卡罗估计的这一类算</p>

<p>法被称为马尔可夫链蒙特卡罗(Markov Chain Monte Carlo, MCMC )方法。Koller and Friedman (2009) 花了大量篇幅来描述马尔可夫链蒙特卡罗算法在机器学习中的 应用。 MCMC 技术最标准、最一般的理论保证只适用于那些各状态概率均不为零的 模型。因此，这些技术最方便的使用方法是用于从基于能量的模型( Energy-based model)即p(x) exp(-E(x))中采样，见第16.2.4节。在EBM的公式表述中，每</p>

<p>一个状态所对应的概率都不为零。事实上， MCMC 方法可以被广泛地应用在包含 0 概率状态的许多概率分布中。然而，在这种情况下，关于 MCMC 方法性能的理论保 证只能依据具体不同类型的分布具体分析证明。在深度学习中，我们通常依赖于那 些一般的理论保证，其在所有基于能量的模型都能自然成立。</p>

<p>为了解释从基于能量的模型中采样困难的原因，我们考虑一个包含两个变量 的EBM的例子，记p(a, b)为其分布。为了采a，我们必须先从p(a | b)中采样；为 了采b，我们又必须从p(b | a)中采样。这似乎成了棘手的先有鸡还是先有蛋的问题。 有向模型避免了这一问题因为它的图是有向无环的。为了完成原始采样( Ancestral Sampling)，在给定每个变量的所有父结点的条件下，我们根据拓扑顺序采样每一个 变量，这个变量是确定能够被采样的(详见第16.3节)。原始采样定义了一种高效 的、单遍的方法来抽取一个样本。</p>

<p>在 EBM 中，我们通过使用马尔可夫链来采样，从而避免了先有鸡还是先有蛋 的问题。马尔可夫链的核心思想是从某个可取任意值的状态x出发。随着时间的推 移，我们随机地反复更新状态x。最终x成为了一个从p(x)中抽出的(非常接近) 比较一般的样本。在正式的定义中，马尔可夫链由一个随机状态 x 和一个转移分布 T(X | x)定义而成，T(a/ | x)是一个概率分布，说明了给定状态x的情况下随机地 转移到xz的概率。运行一个马尔可夫链意味着根据转移分布T(xz | x)采出的值x7 来更新状态 x。</p>

<p>为了给出 MCMC 方法为何有效的一些理论解释，重参数化这个问题是很有用 的。首先我们关注一些简单的情况，其中随机变量x有可数个状态。我们将这种状 态简单地记作正整数x。不同的整数x的大小对应着原始问题中x的不同状态。</p>

<p>接下来我们考虑如果并行地运行无穷多个马尔可夫链的情况。不同马尔可夫 链的所有状态都采样自某一个分布q(t)(x)，在这里t表示消耗的时间数。开始时，对 每个马尔可夫链，我们采用一个分布q0来任意地初始化X。之后，q(t)与所有之前 运行的马尔可夫链有关。我们的目标是 q(t)(x) 收敛到 p(x)。</p>

<p>因为我们已经用正整数x重参数化了这个问题，我们可以用一个向量W来描述 这个概率分布 q，</p>

<p>q( x = i) = vi .</p>

<p>(17.17)</p>

<p>然后我们考虑更新单一的马尔可夫链，从状态x到新状态xz。单一状态转移到</p>

<p>xz的概率可以表示为</p>

<p>q(t+1)(x/) = ^^q(t)(x)T(xz | x).    (17.18)</p>

<p>x</p>

<p>根据状态为整数的参数化设定，我们可以将转移算子T表示成一个矩阵4。矩 阵 A 的定义如下：</p>

<p>4i,j=T(x/=i| x=j).    (17.19)</p>

<p>使用这一定义，我们可以改写式(17.18)。不同于之前使用q和T来理解单个状态的 更新，我们现在可以使用v和4来描述当我们更新时(并行运行的)不同马尔可夫 链上整个分布是如何变化的：</p>

<p>v(t) = 4v(t-1).    (17.20)</p>

<p>重复地使用马尔可夫链更新相当于重复地与矩阵 4 相乘。换言之，我们可以认为这 一过程就是关于 4 的幂乘：</p>

<p>v(t) = 4tv(0).    (17.21)</p>

<p>矩阵 4 有一种特殊的结构，因为它的每一列都代表一个概率分布。这样的矩阵 被称为随机矩阵(Stochastic Matrix )。如果对于任意状态x到任意其他状态x&rsquo;存在 一个 t 使得转移概率不为 0，那么 Perron-Frobenius 定理 (Perron, 1907; Frobenius, 1908) 可以保证这个矩阵的最大特征值是实数且大小为 1。我们可以看到所有的特征 值随着时间呈现指数变化：</p>

<p>v⑴=(Fdiag(A) y-1)tv(0) = Fdiag(A)t F-1v(0).    (17.22)</p>

<p>这个过程导致了所有不等于 1 的特征值都衰减到 0。在一些额外的较为宽松的假 设下，我们可以保证矩阵 4 只有一个对应特征值为 1 的特征向量。所以这个过程 收敛到平稳分布(Stationary Distribution )，有时也被称为均衡分布(Equilibrium Distribution)。收敛时，我们得到</p>

<p>v/ = 4v = v，    (17.23)</p>

<p>这个条件也适用于收敛之后的每一步。这就是特征向量方程。作为收敛的稳定点， v 一定是特征值为 1 所对应的特征向量。这个条件保证收敛到了平稳分布以后，再重 复转移采样过程不会改变所有不同马尔可夫链上状态的分布(尽管转移算子自然而</p>

<p>然地会改变每个单独的状态)。</p>

<p>如果我们正确地选择了转移算子T，那么最终的平稳分布q将会等于我们所希 望采样的分布p。我们会将第17.4节介绍如何选择T。</p>

<p>可数状态马尔可夫链的大多数性质可以被推广到连续状态的马尔可夫链中。在 这种情况下，一些研究者把这种马尔可夫链称为哈里斯链(Harris Chain)，但是我 们将这两种情况都称为马尔可夫链。通常在一些宽松的条件下，一个带有转移算子 T 的马尔可夫链都会收敛到一个不动点，这个不动点可以写成如下形式：</p>

<p>q/(x/) = Ex^qT(x; | x),    (17.24)</p>

<p>这个方程的离散版本就相当于重新改写方程式(17.23)。当 x 是离散值时，这个期 望对应着求和，而当 x 是连续值时，这个期望对应的是积分。</p>

<p>无论状态是连续的还是离散的，所有的马尔可夫链方法都包括了重复、随机地 更新直到最后状态开始从均衡分布中采样。运行马尔可夫链直到它达到均衡分布的 过程通常被称为马尔可夫链的磨合(Burning-in)过程。在马尔可夫链达到均衡分 布之后，我们可以从均衡分布中抽取一个无限多数量的样本序列。这些样本服从同 一分布，但是两个连续的样本之间会高度相关。所以一个有限的序列无法完全表 达均衡分布。一种解决这个问题的方法是每隔n个样本返回一个样本，从而使得我 们对于均衡分布的统计量的估计不会被MCMC方法的样本之间的相关性所干扰。所 以马尔可夫链的计算代价很高，主要源于达到均衡分布前需要磨合的时间以及在达 到均衡分布之后从一个样本转移到另一个足够无关的样本所需要的时间。如果我们 想要得到完全独立的样本，那么我们可以同时并行地运行多个马尔可夫链。这种方 法使用了额外的并行计算来减少时延。使用一条马尔可夫链来生成所有样本的策略 和(使用多条马尔可夫链)每条马尔可夫链只产生一个样本的策略是两种极端。深 度学习的从业者们通常选取的马尔可夫链的数目和小批量中的样本数相近，然后从 这些固定的马尔可夫链集合中抽取所需要的样本。马尔可夫链的数目通常选为 100。</p>

<p>另一个难点是我们无法预先知道马尔可夫链需要运行多少步才能到达均衡分布。 这段时间通常被称为混合时间(Mixing Time )。检测一个马尔可夫链是否达到平衡 是很困难的。我们并没有足够完善的理论来解决这个问题。理论只能保证马尔可夫 链会最终收敛，但是无法保证其他。如果我们从矩阵4作用在概率向量r上的角度 来分析马尔可夫链，那么我们可以发现当 At 除了单个1以外的特征值都趋于 0 时 马尔可夫链混合成功(收敛到了均衡分布)。这也意味着矩阵 4 的第二大特征值决 定了马尔可夫链的混合时间。然而，在实践中，我们通常不能真的将马尔可夫链表示 成矩阵的形式。我们的概率模型所能够达到的状态是变量数的指数级别，所以表达 ^，A或者A的特征值是不现实的。由于以上在内的诸多阻碍，我们通常无法知道马 尔可夫链是否已经混合成功。作为替代，我们只能运行一定量时间马尔可夫链直到 我们粗略估计这段时间是足够的，然后使用启发式的方法来判断马尔可夫链是否混 合成功。这些启发性的算法包括了手动检查样本或者衡量前后样本之间的相关性。</p>

<p>17.4 Gibbs 采样
目前为止我们已经了解了如何通过反复更新x &lt;- xz ~ T(xz | x)从一个分布 q(x)中采样。然而我们还没有介绍过如何确定q(x)是否是一个有效的分布。本书 中将会描述两种基本的方法。第一种方法是从已经学习到的分布 pmodel 中推导出 T，下文描述了如何从基于能量的模型中采样。第二种方法是直接用参数描述T，然 后学习这些参数，其平稳分布隐式地定义了我们所感兴趣的模型Pmodel。我们将在 第20.12节和第20.13节中讨论第二种方法的例子。</p>

<p>在深度学习中， 我们通常使用马尔可夫链从定义为基于能量的模型的分布 pmodel(x) 中采样。在这种情况下，我们希望马尔可夫链的 q(x) 分布就是 pmodel(x)。 为了得到所期望的q(x)分布，我们必须选取合适的T(xz I x)。</p>

<p>Gibbs采样(Gibbs Sampling )是一种概念简单而又有效的方法。它构造一个 从Pmodel(x)中采样的马尔可夫链，其中在基于能量的模型中从T(X&rsquo;| X)采样是通 过选择一个变量Xi，然后从pmodel中该点关于在无向图G (定义了基于能量的模 型结构)中邻接点的条件分布中采样。只要一些变量在给定相邻变量时是条件独立 的，那么这些变量就可以被同时采样。正如在第16.7.1节中看到的RBM示例一样， RBM 中所有的隐藏单元可以被同时采样，因为在给定所有可见单元的条件下它们相 互条件独立。同样地，所有的可见单元也可以被同时采样，因为在给定所有隐藏单 元的情况下它们相互条件独立。以这种方式同时更新许多变量的Gibbs采样通常被 称为 块吉布斯采样( block Gibbs Sampling)。</p>

<p>设计从 pmodel 中采样的马尔可夫链还存在其他备选方法。比如说， Metropolis-Hastings 算法在其他领域中广泛使用。不过在深度学习的无向模型中，我们主要使 用Gibbs采样，很少使用其他方法。改进采样技巧也是一个潜在的研究热点。</p>

<p>17.5 不同的峰值之间的混合挑战
使用MCMC方法的主要难点在于马尔可夫链的混合（Mixing）通常不理想。在 理想情况下，从设计好的马尔可夫链中采出的连续样本之间是完全独立的，而且在 x 空间中，马尔可夫链会按概率大小访问许多不同区域。</p>

<p>然而，MCMC方法采出的样本可能会具有很强的相关性，尤其是在高维的情况 下。我们把这种现象称为慢混合甚至混合失败。具有缓慢混合的MCMC方法可以被 视为对能量函数无意地执行类似于带噪声的梯度下降的操作，或者说等价于相对于 链的状态（被采样的随机变量）依据概率进行噪声爬坡。（在马尔可夫链的状态空 间中）从 x（t-1） 到 x（t） 该链倾向于选取很小的步长，其中能量 E（x（t）） 通常低于或 者近似等于能量E（x（t-1）），倾向于向较低能量的区域移动。当从可能性较小的状态 （比来自 p（x） 的典型样本拥有更高的能量）开始时，链趋向于逐渐减少状态的能量 并且仅仅偶尔移动到另一个峰值。一旦该链已经找到低能量的区域（例如，如果变量 是图像中的像素，则低能量的区域可以是同一对象所对应图像的一个连通的流形） 我们称之为峰值，链将倾向于围绕着这个峰值游走（按某一种形式随机游走）。它 时不时会走出该峰值，但是结果通常会返回该峰值或者（如果找到一条离开的路线） 移向另一个峰值。问题是对于很多有趣的分布来说成功的离开路线很少，所以马尔 可夫链将在一个峰值附近抽取远超过需求的样本。</p>

<p>当我们考虑Gibbs采样算法（见第17.4节）时，这种现象格外明显。在这种情 况下，我们考虑在一定步数内从一个峰值移动到一个临近峰值的概率。决定这个概 率的是两个峰值之间的 ‘‘能量障碍&rdquo; 的形状。隔着一个巨大 ‘‘能量障碍” （低概率 的区域）的两个峰值之间的转移概率是（随着能量障碍的高度）指数下降的，如 图17.1所示。当目标分布有多个高概率峰值并且被低概率区域所分割，尤其当Gibbs 采样的每一步都只是更新变量的一小部分而这一小部分变量又严重依赖其他的变量 时，就会产生问题。</p>

<p>举一个简单的例子，考虑两个变量 a， b 的基于能量的模型，这两个变量都是二 值的，取值+1或者-1。如果对某个较大的正数w，E（a，b） = -wab，那么这个模 型传达了一个强烈的信息，a和b有相同的符号。当a = 1时用Gibbs采样更新b。 给定b时的条件分布满足p（b = 1 | a = 1） = ct（w）。如果w的值很大，sigmoid函 数趋近于饱和，那么b也取到1的概率趋近于1。同理，如果a = -1,那么b取 到-1的概率也趋于1。根据模型Pmodel（a，b），两个变量取一样的符号的概率几乎相 等。根据Pmodei（a|b），两个变量应该有相同的符号。这也意味着Gibbs采样很难会</p>

<p>图 17.1: 对于三种分布使用 Gibbs 采样所产生的路径，所有的分布马尔可夫链初始值都设为峰 值。 （左） 一个带有两个独立变量的多维正态分布。由于变量之间是相互独立的， Gibbs 采样混合得 很好。 （中） 变量之间存在高度相关性的一个多维正态分布。变量之间的相关性使得马尔可夫链很 难混合。因为每一个变量的更新需要相对其他变量求条件分布，相关性减慢了马尔可夫链远离初 始点的速度。 （右）峰值之间间距很大且不在轴上对齐的混合高斯分布。 Gibbs 采样混合得很慢，因 为每次更新仅仅一个变量很难跨越不同的峰值。</p>

<p>改变这些变量的符号。</p>

<p>在更实际的问题中，这种挑战更加艰巨因为在实际问题中我们不能仅仅关注在</p>

<p>两个峰值之间的转移，更要关注在多个峰值之间的转移。如果由于峰值之间混合困</p>

<p>难，而导致某几个这样的转移难以完成，那么得到一些可靠的覆盖大部分峰值的样</p>

<p>本集合的计算代价是很高的，同时马尔可夫链收敛到它的平稳分布的过程也会非常</p>

<p>缓慢。</p>

<p>通过寻找一些高度依赖变量的组以及分块同时更新块（组）中的变量，这个问</p>

<p>题有时候是可以被解决的。然而不幸的是，当依赖关系很复杂时，从这些组中采样</p>

<p>的过程从计算角度上说是难以处理的。归根结底，马尔可夫链最初就是被提出来解</p>

<p>决这个问题，即从大量变量中采样的问题。</p>

<p>在定义了一个联合分布 pmodel（x,h） 的潜变量模型中，我们经常通过交替地从 pmodel（X |叫和pmodel（M X）中采样来达到抽X的目的。从快速混合的角度上说，我 们更希望pmodel（M X）有很大的熵。然而，从学习一个h的有用表示的角度上考虑， 我们还是希望h能够包含X的足够信息从而能够较完整地重构它，这意味h和X 要有非常高的互信息。这两个目标是相互矛盾的。我们经常学习到能够将X精确地</p>

<p>编码为 h 的生成模型，但是无法很好混合。这种情况在玻尔兹曼机中经常出现，一 个玻尔兹曼机学到的分布越尖锐，该分布的马尔可夫链采样越难混合得好。这个问 题在图 17.2中有所描述。</p>

<p>DBIDQBB^^BB BDBKHn BDnBH BQBQDBQSBO DBEaQBDBnQE] BDQQQBQDBQ EaDsBnmDsss BBnKMHHBHtBB</p>

<p>□□BBnESBsHQ BDnBESnQESlis</p>

<p>B333HH3B3E5</p>

<p>图 17.2: 深度概率模型中一个混合缓慢问题的例证。每张图都是按照从左到右从上到下的顺序的 （左） Gibbs 采样从 MNIST 数据集训练成的深度玻尔兹曼机中采出的连续样本。这些连续的样本</p>

<p>之间非常相似。由于Gibbs采样作用于一个深度图模型，相似度更多地是基于语义而非原始视觉</p>

<p>特征。但是对于吉布斯链来说从分布的一个峰值转移到另一个仍然是很困难的，比如说改变数字 （右） 从生成式对抗网络中抽出的连续原始样本。因为原始采样生成的样本之间互相独立，所以不 存在混合问题。</p>

<p>当感兴趣的分布对于每个类具有单独的流形结构时， 所有这些问题都 使MCMC方法变得不那么有用：分布集中在许多峰值周围，并且这些峰值由大量高 能量区域分割。我们在许多分类问题中遇到的是这种类型的分布，由于峰值之间混 合缓慢，它将使得 MCMC 方法非常缓慢地收敛。</p>

<p>17.5.1 不同峰值之间通过回火来混合
当一个分布有一些陡峭的峰并且被低概率区域包围时，很难在分布的不同峰</p>

<p>值之间混合。一些加速混合的方法是基于构造一个概率分布替代目标分布，这个概</p>

<p>率分布的峰值没有那么高，峰值周围的低谷也没有那么低。基于能量的模型为这个</p>

<p>想法提供一种简单的做法。目前为止，我们一直将基于能量的模型描述为定义一个</p>

<p>概率分布：</p>

<p>基于能量的模型可以通过添加一个额外的控制峰值尖锐程度的参数来加强：</p>

<p>p^(x) x exp(-^E(x)).    (17.26)</p>

<p>冷参数可以被理解为温度(temperature )的倒数，反映了基于能量的模型的统计物 理学起源。当温度趋近于0时，趋近于无穷大，此时的基于能量的模型是确定性 的。当温度趋近于无穷大时，趋近于零，基于能量的模型(对离散的x)成了均匀 分布。</p>

<p>通常情况下，在^ = 1时训练一个模型。但我们也可以利用其他温度，尤其是 卢＜ 1的情况。回火(tempering )作为一种通用的策略，它通过从卢＜ 1模型中采 样来实现在 p1 的不同峰值之间快速混合。</p>

<p>基于 回火转移( tempered transition) (Neal, 1994) 的马尔可夫链临时从高温 度的分布中采样使其在不同峰值之间混合，然后继续从单位温度的分布中采样。这 些技巧被应用在一些模型比如RBM中(Salakhutdinov, 2010)。另一种方法是利用并 行回火( parallel tempering) (Iba, 2001)。其中马尔可夫链并行地模拟许多不同温 度的不同状态。最高温度的状态混合较慢，相比之下最低温度的状态，即温度为 1 时，采出了精确的样本。转移算子包括了两个温度之间的随机跳转，所以一个高温 度状态分布槽中的样本有足够大的概率跳转到低温度分布的槽中。这个方法也被应 用到了 RBM中(Desjardins et al., 2010; Cho et al., 2010a)。尽管回火这种方法前景 可期，现今它仍然无法让我们在采样复杂的基于能量的模型中更进一步。一个可能的 原因是在临界温度(critical temperatures )时温度转移算子必须设置得非常慢(因 为温度需要逐渐下降)来确保回火的有效性。</p>

<p>17.5.2 深度也许会有助于混合
当我们从潜变量模型p(仏x)中采样时，我们可以发现如果p(h | x)将x编码 得非常好，那么从p(x|叫中采样时，并不会太大地改变x，那么混合结果会很糟 糕。解决这个问题的一种方法是使得成为一种将x编码为h的深度表示，从而使 得马尔可夫链在h空间中更容易混合。在许多表示学习算法如自编码器和RBM中， h的边缘分布相比于x上的原始数据分布，通常表现为更加均匀、更趋近于单峰值。 或许可以说，这是因为利用了所有可用的表示空间并尽量减小重构误差。因为当训 练集上的不同样本之间在 h 空间能够被非常容易地区分时，我们也会很容易地最 小化重构误差。 Bengio et al. (2013a) 观察到这样的现象，堆叠越深的正则化自编码 器或者RBM，顶端h空间的边缘分布越趋向于均匀和发散，而且不同峰值（比如 说实验中的类别）所对应区域之间的间距也会越小。在高层空间中训练RBM会使 得 Gibbs 采样在峰值间混合得更快。然而，如何利用这种观察到的现象来辅助训练 深度生成模型或者从中采样仍然有待探索。</p>

<p>尽管存在混合的难点，蒙特卡罗技术仍然是一个有用的工具，通常也是最好的</p>

<p>可用工具。事实上，在遇到难以处理的无向模型中的配分函数时，蒙特卡罗方法仍</p>

<p>然是最主要的工具，这将在下一章详细阐述。</p>

<h1 id="ref">REF</h1>

<ol>
<li>《深度学习》Ian Goodfellow</li>
</ol>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/01-%E5%BE%AE%E7%A7%AF%E5%88%86/%E7%A7%AF%E5%88%86%E4%B8%8E%E5%BE%AE%E7%A7%AF%E5%88%86%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">积分与微积分基本定理</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/">
            <span class="next-text nav-default">14 自编码器</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
